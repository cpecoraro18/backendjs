<head><title>Backendjs Documentation</title><link rel="shortcut icon" href="img/logo.png" type="image/png" /><link rel="icon" href="img/logo.png" type="image/png" /><script src="js/jquery.js"></script><link href="css/bootstrap.css" rel="stylesheet"><script src="js/bootstrap.js"></script><link rel="stylesheet" href="css/doc.css"><link href="css/font-awesome.css" rel="stylesheet"></head><body>
<div class="container"><nav class="navbar navbar-expand-lg navbar-light"><div class="navbar-brand"><a href="/"><img class="logo" src="img/logo.png"></a><span>Backend library for Node.js</span></div><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div id="navbar" class="navbar-collapse collapse"><ul class="navbar-nav"><li><a class="nav-link" href="doc.html"><i class="fa fa-gears fa-fw"></i> Documentation</a></li><li><a class="nav-link" href="http://github.com/vseryakov/backendjs"><i class="fa fa-github fa-fw"></i> Github</a><li></ul></div></nav><h1 id="backendjs-documentation">Backendjs Documentation</h1>
<p>##Table of contents</p>
<ul>
<li><a href="#backend-library-for-node-js"> Backend library for Node.js</a></li>
<li><a href="#installation"> Installation</a></li>
<li><a href="#dependencies"> Dependencies</a></li>
<li><a href="#quick-start-and-introduction"> Quick start and introduction</a></li>
<li><a href="#to-run-an-example"> To run an example</a></li>
<li><a href="#configuration"> Configuration</a></li>
<li><a href="#backend-runtime"> Backend runtime</a></li>
<li><a href="#application-structure"> Application structure</a></li>
<li><a href="#modules"> Modules</a></li>
<li><a href="#npm-packages-as-modules"> NPM packages as modules</a></li>
<li><a href="#database-schema-definition"> Database schema definition</a></li>
<li><a href="#api-requests-handling"> API requests handling</a></li>
<li><a href="#example-of-todo-application"> Example of TODO application</a></li>
<li><a href="#backend-directory-structure"> Backend directory structure</a></li>
<li><a href="#cache-configurations"> Cache configurations</a></li>
<li><a href="#local"> Local</a></li>
<li><a href="#redis"> Redis</a></li>
<li><a href="#pub-sub-or-queue-configurations"> PUB/SUB or Queue configurations</a></li>
<li><a href="#redis-system-bus"> Redis system bus</a></li>
<li><a href="#redis-queue"> Redis Queue</a></li>
<li><a href="#sqs"> SQS</a></li>
<li><a href="#local"> Local</a></li>
<li><a href="#nats"> NATS</a></li>
<li><a href="#rabbitmq"> RabbitMQ</a></li>
<li><a href="#security-configurations"> Security configurations</a></li>
<li><a href="#api-only"> API only</a></li>
<li><a href="#secure-web-site-client-verification"> Secure Web site, client verification</a></li>
<li><a href="#secure-web-site-backend-verification"> Secure Web site, backend verification</a></li>
<li><a href="#websockets-connections"> WebSockets connections</a></li>
<li><a href="#versioning"> Versioning</a></li>
<li><a href="#the-backend-provisioning-utility-bkjs"> The backend provisioning utility: bkjs</a></li>
<li><a href="#web-development-notes"> Web development notes</a></li>
<li><a href="#deployment-use-cases"> Deployment use cases</a></li>
<li><a href="#aws-instance-setup-with-node-and-backendjs"> AWS instance setup with node and backendjs</a></li>
<li><a href="#aws-provisioning-examples"> AWS Provisioning examples</a></li>
<li><a href="#make-an-ami"> Make an AMI</a></li>
<li><a href="#launch-instances-when-not-using-autoscaling-groups"> Launch instances when not using AutoScaling Groups</a></li>
<li><a href="#copy-autoscaling-launch-templates-after-new-ami-is-created"> Copy Autoscaling launch templates after new AMI is created</a></li>
<li><a href="#update-route53-with-all-ips-from-running-instances"> Update Route53 with all IPs from running instances</a></li>
<li><a href="#proxy-mode"> Proxy mode</a></li>
<li><a href="#configure-http-port"> Configure HTTP port</a></li>
<li><a href="#backend-library-development-mac-os-x-developers-"> Backend library development (Mac OS X, developers)</a></li>
<li><a href="#design-considerations"> Design considerations</a></li>
<li><a href="#api-endpoints-provided-by-the-backend"> API endpoints provided by the backend</a></li>
<li><a href="#authentication-and-sessions"> Authentication and sessions</a></li>
<li><a href="#signature"> Signature</a></li>
<li><a href="#authentication-api"> Authentication API</a></li>
<li><a href="#accounts"> Accounts</a></li>
<li><a href="#health-enquiry"> Health enquiry</a></li>
<li><a href="#public-images-endpoint"> Public Images endpoint</a></li>
<li><a href="#data"> Data</a></li>
<li><a href="#system-api"> System API</a></li>
<li><a href="#author"> Author</a></li>
<li><a href="#configuration-parameters">Configuration parameters</a></li>
<li>Javascript API functions<ul>
<li><a href="#module-api">api</a></li>
<li><a href="#module-app">app</a></li>
<li><a href="#module-auth">auth</a></li>
<li><a href="#module-aws">aws</a></li>
<li><a href="#module-core">core</a></li>
<li><a href="#module-db">db</a></li>
<li><a href="#module-events">events</a></li>
<li><a href="#module-http_get">http_get</a></li>
<li><a href="#module-index">index</a></li>
<li><a href="#module-ipc">ipc</a></li>
<li><a href="#module-jobs">jobs</a></li>
<li><a href="#module-lib">lib</a></li>
<li><a href="#module-logger">logger</a></li>
<li><a href="#module-metrics">metrics</a></li>
<li><a href="#module-msg">msg</a></li>
<li><a href="#module-pool">pool</a></li>
<li><a href="#module-server">server</a></li>
<li><a href="#module-bk_data">bk_data</a></li>
<li><a href="#module-bk_system">bk_system</a></li>
<li><a href="#module-bk_user">bk_user</a></li>
</ul>
</li>
</ul>
<h1 id="backend-library-for-nodejs">Backend library for Node.js</h1>
<p>General purpose backend library. The primary goal is to have a scalable platform for running and managing Node.js
servers for Web services implementation.</p>
<p>This project only covers the lower portion of the Web services ecosystem:
Node.js processes, HTTP servers, basic API functionality, database access, caching, messaging between processes,
metrics and monitoring, a library of tools for developing Node.js servers.</p>
<p>For the UI and presentation layer there are no restrictions what to use as long as it can run on top of the Express server.</p>
<p>Features:</p>
<ul>
<li>Exposes a set of Web service APIs over HTTP(S) using Express framework.</li>
<li>Database API supports SQLite, PostreSQL, DynamoDB, ElasticSearch with all basic operations behaving the
same way allowing you to switch databases without changing the code.</li>
<li>Database operations (Get, Put, Del, Update, Select) for all supported databases using the same DB API.</li>
<li>Experimental database drivers for MySQL, Cassandra, Riak, CouchDB</li>
<li>Experimental DynamoDB Streams processing in background worker processes</li>
<li>Easily extensible to support any kind of database, provides a database driver on top of Redis with all supported methods as an example.</li>
<li>Provides accounts, connections, locations, messaging and icons APIs with basic functionality for a quick start.</li>
<li>Supports crontab and queue job processing by separate worker processes.</li>
<li>Authentication is based on signed requests using API key and secret, similar to Amazon AWS signing requests.</li>
<li>Runs web server as separate processes to utilize multiple CPU cores.</li>
<li>Supports WebSockets connections and process them with the same Express routes as HTTP requests</li>
<li>Supports several cache modes(Redis, Memcache, Hazelcast, LRU) for the database operations, multiple hosts support
in the clients for failover.</li>
<li>Supports several PUB/SUB modes of operations using Redis, NATS, RabbitMQ, Hazelcast.</li>
<li>Supports async jobs processing using several work queue implementations on top of SQS, Redis, NATS, DB, RabbitMQ, Hazelcast.</li>
<li>REPL (command line) interface for debugging and looking into server internals.</li>
<li>Supports push notifications via Webpush, APN and FCM.</li>
<li>Supports HTTP(S) reverse proxy mode where multiple Web workers are load-balanced by the proxy
server running in the master process instead of relying on the OS scheduling between processes listening on the same port.</li>
<li>Can be used with any MVC, MVVC or other types of frameworks that work on top of, or with, the Express server.</li>
<li>AWS support is very well integrated including EC2, S3, DynamoDB, SQS and more.</li>
<li>Includes simple log watcher to monitor the log files including system errors.</li>
<li>Supports i18n hooks for request/response objects, easily overriden with any real i18n implementation.</li>
<li>Integrated very light unit testing facility which can be used to test modules and API requests</li>
<li>Support runtime metrics about the timing on database, requests, cache, memory and request rate limit control</li>
<li>Full implementation of SRP6a protocol in the server and client</li>
<li>Hosted on <a href="https://github.com/vseryakov/backendjs">github</a>, BSD licensed.</li>
</ul>
<p>Check out the <a href="http://bkjs.io">Documentation</a> for more details.</p>
<h1 id="installation">Installation</h1>
<p>To install the module with all optional dependencies if they are available in the system</p>
<pre><code>npm install backendjs
</code></pre>
<p>To install from the git</p>
<pre><code> npm install git+https://github.com/vseryakov/backendjs.git
</code></pre>
<p>or simply</p>
<pre><code> npm install vseryakov/backendjs
</code></pre>
<h1 id="dependencies">Dependencies</h1>
<p>Only core required dependencies are installed but there are many modules which require a module to work correctly.</p>
<p>All optional dependencies are listed in the package.json under &quot;modDependencies&quot; so npm cannot use it, only manual install of required modules is supported or
it is possible to install all optional dependencies for development purposes.</p>
<p>Here is the list of modules required for each internal feature:</p>
<ul>
<li><code>pg</code> - PostgreSQL database access</li>
<li><code>argon2</code> or <code>bcrypt</code> - for user password hashing</li>
<li><code>mmmagic</code> - file detection in uploads, only used when <code>allow</code> is passed to the <code>api.putFile</code></li>
<li><code>consolidate</code> - for API templating, disabled by default</li>
<li><code>redis</code> - for Redis queue and cache driver</li>
<li><code>unix-dgram</code> - for syslog on Linux to use local syslog</li>
<li><code>bkjs-sqlite</code> - to use SQLite database driver</li>
<li><code>web-push</code> - for Web push notifications</li>
<li><code>@parse/node-apn</code> - for Apple push notifications</li>
<li><code>bkjs-wand</code> - for scaling images in uploads using ImageMagick module</li>
<li><code>sharp</code> - scaling images in uploads using VPS imaging</li>
<li><code>nats</code> - NATS driver for queue and events</li>
<li><code>amqplib</code> - RabbitMQ driver for queue and events (alpha)</li>
</ul>
<p>The command below will show all core and optional dependencies</p>
<pre><code> bkjs deps -dry-run -mods
</code></pre>
<h1 id="quick-start-and-introduction">Quick start and introduction</h1>
<ul>
<li><p>Simplest way of using the backendjs, it will start the server listening on port 8000</p>
<pre><code>  $ node
  &gt; const bkjs = require(&#39;backendjs&#39;)
  &gt; bkjs.server.start()
</code></pre>
</li>
<li><p>Access is allowed only with valid signature except urls that are explicitly allowed without it (see <code>api-allow</code> config parameter below)</p>
</li>
<li><p>Same but using the helper tool, by default no database driver are enablked so here we use embedded SQLite database and listen on port 8000.</p>
<pre><code>  bkjs web -db-pool sqlite -db-sqlite-pool default
</code></pre>
</li>
<li><p>or to the PostgreSQL server, database backend (if not running local server can be started with <code>bkjs init-pgsql</code> if postgresql is installed)</p>
<pre><code>  bkjs web -db-pool pg -db-pg-pool postgresql://postgres@localhost/backend
</code></pre>
</li>
<li><p>If running on EC2 instance with IAM profile no need to specify AWS credentials:</p>
<pre><code>  bkjs web -db-pool dynamodb -db-dynamodb-pool default
</code></pre>
</li>
<li><p>To start the server and connect to the DynamoDB (command line parameters can be saved in the <code>etc/config file</code>, see below about config files)</p>
<pre><code>  bkjs web -db-pool dynamodb -db-dynamodb-pool default -aws-key XXXX -aws-secret XXXX
</code></pre>
</li>
<li><p>or to the ElasticSearch server, database backend</p>
<pre><code>  bkjs web -db-pool elasticsearch -db-elasticsearch-pool http://127.0.0.1:9200
</code></pre>
</li>
<li><p>All commands above will behave exactly the same</p>
</li>
<li><p><strong>Tables are not created by default</strong>, in order to initialize the database, run the server or the shell with <code>-db-create-tables</code> flag,
it is called only inside a master process, a worker never creates tables on start</p>
<ul>
<li><p>prepare the tables in the shell</p>
<pre><code>bksh -db-pool dynamodb -db-dynamodb-pool default -db-create-tables
</code></pre>
</li>
<li><p>run the server and create tables on start, run Elasticsearch locally first on the local machine</p>
<pre><code>bkjs get-elasticsearch
bkjs run-elasticsearch

bkjs web -db-pool elasticsearch -db-elasticsearch-pool http://127.0.0.1:9200 -db-create-tables
</code></pre>
</li>
</ul>
</li>
<li><p>While the local backendjs is runnning, the documentation is always available at <a href="http://localhost:8000/doc.html">http://localhost:8000/doc.html</a> (or whatever port is the server using)</p>
</li>
<li><p>To add users from the command line</p>
<pre><code>  bksh -user-add login test secret test name TestUser email test@test.com -scramble 1
</code></pre>
</li>
<li><p>To start Node.js shell with backendjs loaded and initialized, all command line parameters apply to the shell as well</p>
<pre><code>  bkjs shell
</code></pre>
</li>
<li><p>To access the database while in the shell</p>
<pre><code>  &gt; db.select(&quot;bk_user&quot;, {}, console.log);
  &gt; db.select(&quot;bk_user&quot;, {}, lib.log);
  &gt; db.add(&quot;bk_user&quot;, { id: &#39;test2&#39;, login: &#39;test2&#39;, secret: &#39;test2&#39;, name&#39; Test 2 name&#39; }, lib.log);
  &gt; db.select(&quot;bk_user&quot;, { id: &#39;test2&#39; }, lib.log);
  &gt; db.select(&quot;bk_user&quot;, { id: [&#39;test1&#39;,&#39;test2&#39;] }, { ops: { id: &quot;in&quot; } }, lib.log);
</code></pre>
</li>
<li><p>To search using Elasticsearch (assuming it runs on EC2 and it is synced with DynamoDB using streams)</p>
<pre><code>  &gt; db.select(&quot;bk_user&quot;, { q: &#39;test&#39; }, { pool: &quot;elasticsearch&quot; }, lib.log);
</code></pre>
</li>
</ul>
<h2 id="to-run-an-example">To run an example</h2>
<ul>
<li><p>The library is packaged with copies of Bootstrap, jQuery, Knockout.js for quick Web development
in web/js and web/css directories, all scripts are available from the browser with /js or /css paths. To use all at once as a bundle
run the following command:</p>
<pre><code>  npm run devbuild
</code></pre>
</li>
</ul>
<ul>
<li><p>Go to <code>examples/api</code> directory:</p>
</li>
<li><p>Run the application, it will start the Web server on port 8000:</p>
<pre><code>  ./app.sh
</code></pre>
</li>
<li><p>Now log in with the new account,</p>
</li>
<li><p>Go to <a href="http://localhost:8000/api.html">http://localhost:8000/api.html</a> and click on <em>Login</em> at the top-right corner, then enter &#39;test&#39; as login and &#39;test&#39; as secret in the login popup dialog.</p>
</li>
<li><p>To see your account details run the command in the console <code>/account/get</code></p>
</li>
<li><p>To see current metrics run the command in the console <code>/system/stats/get</code></p>
</li>
<li><p>When the web server is started with <code>-watch</code> parameter or as <code>bkjs watch</code> then any change in the source files will make the server restart automatically
letting you focus on the source code and not server management, this mode is only enabled by default in development mode,
check <code>app.sh</code> for parameters before running it in production.</p>
</li>
</ul>
<h1 id="configuration">Configuration</h1>
<p>Almost everything in the backend is configurable using config files, a config database or DNS.
The whole principle behind it is that once deployed in production, even quick restarts are impossible to do so
there should be a way to push config changes to the processes without restarting.</p>
<p>Every module defines a set of config parameters that defines the behavior of the code, due to the single threaded
nature of the Node.js. It is simple to update any config parameter to a new value so the code can operate differently.
To achieve this the code must be written in a special way, like driven by configuration which can be changed at
any time.</p>
<p>All configuration goes through the configuration process that checks all inputs and produces valid output which
is applied to the module variables. Config file or database table with configuration can be loaded on demand or
periodically, for example all local config files are watched for modification and reloaded automatically, the
config database is loaded periodically which is defined by another config parameter.</p>
<h1 id="backend-runtime">Backend runtime</h1>
<p>When the backendjs server starts it spawns several processes that perform different tasks.</p>
<p>There are 2 major tasks of the backend that can be run at the same time or in any combination:</p>
<ul>
<li>a Web server (server) with Web workers (web)</li>
<li>a job scheduler (master)</li>
</ul>
<p>These features can be run standalone or under the guard of the monitor which tracks all running processes and restarted any failed ones.</p>
<p>This is the typical output from the ps command on Linux server:</p>
<pre><code>ec2-user    891  0.0  0.6 1071632 49504 ?  Ssl  14:33   0:01 bkjs: monitor
ec2-user    899  0.0  0.6 1073844 52892 ?  Sl   14:33   0:01 bkjs: master
ec2-user    908  0.0  0.8 1081020 68780 ?  Sl   14:33   0:02 bkjs: server
ec2-user    917  0.0  0.7 1072820 59008 ?  Sl   14:33   0:01 bkjs: web
ec2-user    919  0.0  0.7 1072820 60792 ?  Sl   14:33   0:02 bkjs: web
ec2-user    921  0.0  0.7 1072120 40721 ?  Sl   14:33   0:02 bkjs: worker
</code></pre>
<p>To enable any task a command line parameter must be provided, it cannot be specified in the config file. The <code>bkjs</code> utility supports several
commands that simplify running the backend in different modes.</p>
<ul>
<li><code>bkjs watch</code> - runs the master and Web server in wather mode checking all source files for changes, this is the common command to be used
 in development, it passes the command line switches: <code>-watch -master</code></li>
<li><code>bkjs start</code> - this command is supposed to be run at the server startup as a service, it runs in the background and the monitors all tasks,
 the env variable <code>BKJS_SERVER</code> can be set in the profile to one of the <code>master or monitor</code> to define which run mode to use, default mode is monitor</li>
<li><code>bkjs monitor</code> - this command is supposed to be run at the server startup, it runs in the background and the monitors all processes,
 the command line parameters are: <code>-daemon -monitor -master -syslog</code></li>
<li><code>bkjs master</code> - this command is supposed to be run at the server startup, it runs in the background and the monitors all processes,
 the command line parameters are: <code>-daemon -monitor -master -syslog</code>, web server and workers are started by default</li>
<li><code>bkjs web</code> - this command runs just web server process with child processes as web workers</li>
<li><code>bkjs run</code> - this command runs without other parameters, all additional parameters can be added in the command line, this command
 is a barebone helper to be used with any other custom settings.</li>
<li><code>bkjs run -api</code> - this command runs a single process as web server, sutable for Docker</li>
<li><code>bkjs run -worker</code> - this command runs a single process worker, suatable for Docker</li>
<li><code>bkjs shell</code> or <code>bksh</code> - start backendjs shell, no API or Web server is initialized, only the database pools</li>
</ul>
<h1 id="application-structure">Application structure</h1>
<p>The main purpose of the backendjs is to provide API to access the data, the data can be stored in the database or some other way
but the access to that data will be over HTTP and returned back as JSON. This is default functionality but any custom application
may return data in whatever format is required.</p>
<p>Basically the backendjs is a Web server with ability to perform data processing using local or remote jobs which can be scheduled similar to Unix cron.</p>
<p>The principle behind the system is that nowadays the API services just return data which Web apps or mobiles apps can render to
the user without the backend involved. It does not mean this is simple gateway between the database, in many cases it is but if special
processing of the data is needed before sending it to the user, it is possible to do and backendjs provides many convenient helpers and tools for it.</p>
<p>When the API layer is initialized, the api module contains <code>app</code> object which is an Express server.</p>
<p>Special module/namespace <code>app</code> is designated to be used for application development/extension. This module is available in the same way as <code>api</code> and <code>core</code>
which makes it easy to refer and extend with additional methods and structures.</p>
<p>The typical structure of a single file backendjs application is the following:</p>
<pre><code class="language-javascript">    const bkjs = require(&#39;backendjs&#39;);
    const api = bkjs.api;
    const app = bkjs.app;
    const db = bkjs.db;

    app.listArg = [];

    // Define the module config parameters
    core.describeArgs(&#39;app&#39;, [
        { name: &quot;list-arg&quot;, array: 1, type: &quot;list&quot;, descr: &quot;List of words&quot; },
        { name: &quot;int-arg&quot;, type: &quot;int&quot;, descr: &quot;An integer parameter&quot; },
     ]);

    // Describe the tables or data models, all DB pools will use it, the master or shell
    // process only creates new tables, workers just use the existing tables
    db.describeTables({
         ...
    });

     // Optionally customize the Express environment, setup MVC routes or else, `api.app` is the Express server
    app.configureMiddleware = function(options, callback)
    {
       ...
       callback()
    }

    // Register API endpoints, i.e. url callbacks
    app.configureWeb = function(options, callback)
    {
        api.app.get(&#39;/some/api/endpoint&#39;, (req, res) =&gt; {
          // to return an error, the message will be translated with internal i18n module if locales
          // are loaded and the request requires it
          api.sendReply(res, err);
          // or with custom status and message, explicitely translated
          api.sendReply(res, 404, res.__({ phrase: &quot;not found&quot;, locale: &quot;fr&quot; }));

          // with config check
          if (app.intArg &gt; 5) ...
          if (app.listArg.indexOf(req.query.name) &gt; -1) ...

          // to send data back with optional postprocessing hooks
          api.sendJSON(req, err, data);
          // or simply
          res.json(data);
        });
        ...
        callback();
    }

    // Optionally register post processing of the returned data from the default calls
    api.registerPostProcess(&#39;&#39;, /^\/account\/([a-z\/]+)$/, function(req, res, rows) { ... });
     ...

    // Optionally register access permissions callbacks
    api.registerAccessCheck(&#39;&#39;, /^\/test\/list$/, function(req, status, callback) { ...  });
    api.registerPreProcess(&#39;&#39;, /^\/test\/list$/, function(req, status, callback) { ...  });
     ...
    bkjs.server.start();
</code></pre>
<p>Another probably easier way to create single file apps is to use your namespace instead of <code>app</code>:</p>
<pre><code class="language-javascript">    const bkjs = require(&quot;backendjs&quot;);
    const api = bkjs.api;
    const db = bkjs.db;

    const mymod = {
        name: &quot;mymod&quot;,
        args: [
            { name: &quot;types&quot;, type: &quot;list&quot;, descr: &quot;Types allowed&quot; },
            { name: &quot;size&quot;, type: &quot;int&quot;, descr: &quot;Records in one page&quot; },
        ],
        tables: {
            mytable: {
                id: { type: &quot;int&quot;, primary: 1 },
                name: { primary: 2 },
                type: { type: &quot;list&quot; },
                descr: {}
            }
        }
    };
    exports.module = mymod;
    bkjs.core.addModule(mymod);

    mymod.configureWeb = function(options, callback)
    {
        api.app.all(&quot;/mymod&quot;, function(req, res) {
            if (!req.query.id) return api.sendReply(res, 400, &quot;id is required&quot;);
            req.query.type = mod.types;

            db.select(&quot;mymod&quot;, req.query, { ops: { type: &quot;in&quot; }, count: mod.size }, (err, rows) =&gt; {
               api.sendJSON(req, err, rows);
            });
        });
    }

    bkjs.server.start();
</code></pre>
<p>Except the <code>app.configureWeb</code> and <code>server.start()</code> all other functions are optional, they are here for the sake of completeness of the example. Also
because running the backend involves more than just running web server many things can be setup using the configuration options like common access permissions,
configuration of the cron jobs so the amount of code to be written to have fully functioning production API server is not that much, basically only
request endpoint callbacks must be provided in the application.</p>
<p>As with any Node.js application, node modules are the way to build and extend the functionality, backendjs does not restrict how
the application is structured.</p>
<h2 id="modules">Modules</h2>
<p><em>By default no system modules are loaded except <code>bk_user</code>, it must be configured by the <code>-preload-modules</code> config parameter to
preload modules from the backendjs/modules/.</em></p>
<p>Another way to add functionality to the backend is via external modules specific to the backend, these modules are loaded on startup from the backend
home subdirectory <code>modules/</code>. The format is the same as for regular Node.js modules and only top level .js files are loaded on the backend startup.</p>
<p>Once loaded they have the same access to the backend as the rest of the code, the only difference is that they reside in the backend home and
can be shipped regardless of the npm, node modules and other env setup. These modules are exposed in the <code>core.modules</code> the same way as all other core submodules
methods.</p>
<p>Let&#39;s assume the <code>modules/</code> contains file facebook.js which implements custom FB logic:</p>
<pre><code class="language-javascript">    const bkjs = require(&quot;backendjs&quot;);
    const core = bkjs.core;
    const mod = {
        name: &quot;facebook&quot;,
        args: [
            { name: &quot;token&quot;, descr: &quot;API token&quot; },
        ]
    }
    module.exports = mod;

    mod.configureWeb = function(options, callback) {
       ...
    }

    mod.makeRequest = function(options, callback) {
         core.sendRequest({ url: options.path, query: { access_token: fb.token } }, callback);
    }
</code></pre>
<p>This is the main app code:</p>
<pre><code class="language-javascript">    const bkjs = require(&quot;backendjs&quot;);
    const core = bkjs.core;

    // Using facebook module in the main app
    api.app.get(&quot;/me&quot;, (req, res) =&gt; {

       core.modules.facebook.makeRequest({ path: &quot;/me&quot; }, (err, data) =&gt; {
          bkjs.api.sendJSON(req, err, data);
       });
    });

    bkj.server.start();
</code></pre>
<h2 id="npm-packages-as-modules">NPM packages as modules</h2>
<p>In case different modules is better keep separately for maintenance or development purposes they can be split into
separate NPM packages, the structure is the same, modules must be in the modules/ folder and the package must be loadable
via require as usual. In most cases just empty index.js is enough. Such modules will not be loaded via require though but
by the backendjs <code>core.loadModule</code> machinery, the NPM packages are just keep different module directories separate from each other.</p>
<p>The config parameter <code>allow-packages</code> can be used to specify NPM package names to be loaded separated by comma, as with the default
application structure all subfolders inside each NPM package will be added to the core:</p>
<ul>
<li>modules will be loaded from the modules/ folder</li>
<li>locales from the locales/ folder</li>
<li>files in the web/ folder will be added to the static search path</li>
<li>all templates from views/ folder will be used for rendering</li>
</ul>
<p>If there is a config file present as <code>etc/config</code> it will be loaded as well, this way each package can maintain its default config parameters if necessary
without touching other or global configuration. Although such config files will not be reloaded on changes, when NPM installs or updates packages it
moves files around so watching the old config is no point because the updated config file will be different.</p>
<h1 id="database-schema-definition">Database schema definition</h1>
<p>The backend support multiple databases and provides the same db layer for access. Common operations are supported and all other specific usage can be achieved by
using SQL directly or other query language supported by any particular database.
The database operations supported in the unified way provide simple actions like <code>db.get, db.put, db.update, db.del, db.select</code>. The <code>db.query</code> method provides generic
access to the database driver and executes given query directly by the db driver, it can be SQL or other driver specific query request.</p>
<p>Before the tables can be queried the schema must be defined and created, the backend db layer provides simple functions to do it:</p>
<ul>
<li>first the table needs to be described, this is achieved by creating a JavaScript object with properties describing each column, multiple tables can be described
at the same time, for example lets define album table and make sure it exists when we run our application:</li>
</ul>
<pre><code class="language-javascript">        db.describeTables({
           album: {
               id: { primary: 1 },                         // Primary key for an album
               name: { pub: 1 },                           // Album name, public column
               mtime: { type: &quot;now&quot; },                     // Modification timestamp
           },
           photo: {
               album_id: { primary: 1 },                   // Combined primary key
               id: { primary: 1 },                         // consisting of album and photo id
               name: { pub: 1, index: 1 },                 // Photo name or description, public column with the index for faster search
               mtime: { type: &quot;now&quot; }
           }
        });
</code></pre>
<ul>
<li>the system will automatically create the album and photos tables, this definition must remain in the app source code
and be called on every app startup. This allows 1) to see the db schema while working with the app and 2) easily maintain it by adding new columns if
necessary, all new columns will be detected and the database tables updated accordingly. And it is all JavaScript, no need to learn one more language or syntax
to maintain database tables.</li>
</ul>
<p>Each database may restrict how the schema is defined and used, the db layer does not provide an artificial layer hiding all specifics, it just provides the same
API and syntax, for example, DynamoDB tables must have only hash primary key or combined hash and range key, so when creating table to be used with DynamoDB, only
one or two columns can be marked with primary property while for SQL databases the composite primary key can consist of more than 2 columns.</p>
<p>The backendjs always creates several tables in the configured database pools by default, these tables are required to support default API functionality and some
are required for backend operations. Refer below for the JavaScript modules documentation that described which tables are created by default. In the custom applications
the <code>db.describeTables</code> method can modify columns in the default table and add more columns if needed.</p>
<p>For example, to make age and some other columns in the accounts table public and visible by other users with additional columns the following can be
done in the <code>api.initApplication</code> method. It will extend the bk_user table and the application can use new columns the same way as the already existing columns.
Using the birthday column we make &#39;age&#39; property automatically calculated and visible in the result, this is done by the internal method <code>api.processAccountRow</code> which
is registered as post process callback for the bk_user table. The computed property <code>age</code> will be returned because it is not present in the table definition
and all properties not defined and configured are passed as is.</p>
<p>The cleanup of the public columns is done by the <code>api.sendJSON</code> which is used by all API routes when ready to send data back to the client. If any post-process
hooks are registered and return data itself then it is the hook responsibility to cleanup non-public columns.</p>
<pre><code class="language-javascript">    db.describeTables({
        bk_user: {
            birthday: {},
            ssn: {},
            salary: { type: &quot;int&quot; },
            occupation: {},
            home_phone: {},
            work_phone: {},
        });

    app.configureWeb = function(options, callback)
    {
       db.setProcessRow(&quot;post&quot;, &quot;bk_user&quot;, this.processAccountRow);
       ...
       callback();
    }
    app.processAccountRow = function(req, row, options)
    {
       if (row.birthday) row.age = Math.floor((Date.now() - core.toDate(row.birthday))/(86400000*365));
    }
</code></pre>
<p>To define tables inside a module just provide a <code>tables</code> property in the module object, it will be picked up by database initialization automatically.</p>
<pre><code class="language-javascript">    const mod = {
        name: &quot;billing&quot;,
        tables: {
            invoices: {
                id: { type: &quot;int&quot;, primary: 1 },
                name: {},
                price: { type: &quot;real&quot; },
                mtime: { type: &quot;now&quot; }
            }
        }
    }
    module.exports = mod;

    // Run db setup once all the DB pools are configured, for example produce dynamic icon property
    // for each record retrieved
    mod.configureModule = function(options, callback)
    {
        db.setProcessRows(&quot;post&quot;, &quot;invoices&quot;, function(req, row, opts) {
         if (row.id) row.icon = &quot;/images/&quot; + row.id + &quot;.png&quot;;
     });
        callback();
    }
</code></pre>
<h1 id="api-requests-handling">API requests handling</h1>
<p>All methods will put input parameters in the <code>req.query</code>, GET or POST.</p>
<p>One way to verify input values is to use <code>lib.toParams</code>, only specified parameters will be returned and converted according to
the type or ignored.</p>
<p>Example:</p>
<pre><code class="language-javascript">   var params = {
      test1: { id: { type: &quot;text&quot; },
               count: { type: &quot;int&quot; },
               email: { regexp: /^[^@]+@[^@]+$/ }
      }
   };

   api.app.all(&quot;/endpoint/test1&quot;, function(req, res) {
      const query = lib.toParams(req.query, params.test1);
      if (typeof query == &quot;string&quot;) return api.sendReply(res, 400, query);
      ...
   });
</code></pre>
<h1 id="example-of-todo-application">Example of TODO application</h1>
<p>Here is an example how to create simple TODO application using any database supported by the backend. It supports basic
operations like add/update/delete a record, show all records.</p>
<p>Create a file named <code>app.js</code> with the code below.</p>
<pre><code class="language-javascript">    const bkjs = require(&#39;backendjs&#39;);
    const api = bkjs.api;
    const lib = bkjs.lib;
    const app = bkjs.app;
    const db = bkjs.db;

    // Describe the table to store todo records
    db.describeTables({
       todo: {
           id: { type: &quot;uuid&quot;, primary: 1 },  // Store unique task id
           due: {},                           // Due date
           name: {},                          // Short task name
           descr: {},                         // Full description
           mtime: { type: &quot;now&quot; }             // Last update time in ms
       }
    });

    // API routes
    app.configureWeb = function(options, callback)
    {
        api.app.get(/^\/todo\/([a-z]+)$/, function(req, res) {
           var options = api.getOptions(req);
           switch (req.params[0]) {
             case &quot;get&quot;:
                if (!req.query.id) return api.sendReply(res, 400, &quot;id is required&quot;);
                db.get(&quot;todo&quot;, { id: req.query.id }, options, (err, rows) =&gt; { api.sendJSON(req, err, rows); });
                break;

             case &quot;select&quot;:
                options.noscan = 0; // Allow empty scan of the whole table if no query is given, disabled by default
                db.select(&quot;todo&quot;, req.query, options, (err, rows) =&gt; { api.sendJSON(req, err, rows); });
                break;

            case &quot;add&quot;:
                if (!req.query.name) return api.sendReply(res, 400, &quot;name is required&quot;);
                // By default due date is tomorrow
                if (req.query.due) req.query.due = lib.toDate(req.query.due, Date.now() + 86400000).toISOString();
                db.add(&quot;todo&quot;, req.query, options, (err, rows) =&gt; { api.sendJSON(req, err, rows); });
                break;

            case &quot;update&quot;:
                if (!req.query.id) return api.sendReply(res, 400, &quot;id is required&quot;);
                db.update(&quot;todo&quot;, req.query, options, (err, rows) =&gt; { api.sendJSON(req, err, rows); });
                break;

            case &quot;del&quot;:
                if (!req.query.id) return api.sendReply(res, 400, &quot;id is required&quot;);
                db.del(&quot;todo&quot;, { id: req.query.id }, options, (err, rows) =&gt; { api.sendJSON(req, err, rows); });
                break;
            }
        });
        callback();
     }
     bkjs.server.start();
</code></pre>
<p>Now run it with an option to allow API access without an account:</p>
<pre><code>node app.js -log debug -web -api-allow-path /todo -db-create-tables
</code></pre>
<p>To use a different database, for example PostgresSQL(running localy) or DynamoDB(assuming EC2 instance),
all config parametetrs can be stored in the etc/config as well</p>
<pre><code>node app.js -log debug -web -api-allow-path /todo -db-pool dynamodb -db-dynamodb-pool default -db-create-tables
node app.js -log debug -web -api-allow-path /todo -db-pool pg -db-pg-pool default -db-create-tables
</code></pre>
<p>API commands can be executed in the browser or using <code>curl</code>:</p>
<pre><code>curl &#39;http://localhost:8000/todo?name=TestTask1&amp;descr=Descr1&amp;due=2015-01-01`
curl &#39;http://localhost:8000/todo/select&#39;
</code></pre>
<h1 id="backend-directory-structure">Backend directory structure</h1>
<p>When the backend server starts and no -home argument passed in the command line the backend makes its home environment in the <code>~/.bkjs</code> directory.
It is also possible to set the default home using BKJS_HOME environment variable.</p>
<p>The backend directory structure is the following:</p>
<ul>
<li><p><code>etc</code> - configuration directory, all config files are there</p>
<ul>
<li><p><code>etc/profile</code> - shell script loaded by the bkjs utility to customize env variables</p>
</li>
<li><p><code>etc/config</code> - config parameters, same as specified in the command line but without leading -, each config parameter per line:</p>
<p>  Example:</p>
<pre><code>  debug=1
  db-pool=dynamodb
  db-dynamodb-pool=http://localhost:9000
  db-pg-pool=postgresql://postgres@127.0.0.1/backend

  To specify other config file: bkjs shell -config-file file
</code></pre>
</li>
<li><p>etc/config.local - same as the config but for the cases when local environment is different than the production or for dev specific parameters</p>
</li>
<li><p>some config parameters can be configured in DNS as TXT records, the backend on startup will try to resolve such records and use the value if not empty.
All params that  marked with DNS TXT can be configured in the DNS server for the domain where the backend is running, the config parameter name is
concatenated with the domain and queried for the TXT record, for example: <code>cache-host</code> parameter will be queried for cache-host.domain.name for TXT record type.</p>
</li>
<li><p><code>etc/crontab</code> - jobs to be run with intervals, JSON file with a list of cron jobs objects:</p>
<p>  Example:</p>
<ol>
<li><p>Create file in ~/.backend/etc/crontab with the following contents:</p>
<pre><code> [ { &quot;cron&quot;: &quot;0 1 1 * * 1,3&quot;, &quot;job&quot;: { &quot;app.cleanSessions&quot;: { &quot;interval&quot;: 3600000 } } } ]
</code></pre>
</li>
<li><p>Define the function that the cron will call with the options specified, callback must be called at the end, create this app.js file</p>
<pre><code> var bkjs = require(&quot;backendjs&quot;);
 bkjs.app.cleanSessions = function(options, callback) {
      bkjs.db.delAll(&quot;session&quot;, { mtime: options.interval + Date.now() }, { ops: &quot;le&quot; }, callback);
 }
 bkjs.server.start()
</code></pre>
</li>
<li><p>Start the jobs queue and the web server at once</p>
<pre><code> bkjs master -jobs-workers 1 -jobs-cron
</code></pre>
</li>
</ol>
</li>
<li><p>etc/crontab.local - additional local crontab that is read after the main one, for local or dev environment</p>
</li>
</ul>
</li>
<li><p><code>modules</code> - loadable modules with specific functionality</p>
</li>
<li><p><code>images</code> - all images to be served by the API server, every subfolder represent naming space with lots of subfolders for images</p>
</li>
<li><p><code>var</code> - database files created by the server</p>
</li>
<li><p><code>tmp</code> - temporary files</p>
</li>
<li><p><code>web</code> - Web pages served by the static Express middleware</p>
</li>
</ul>
<h1 id="cache-configurations">Cache configurations</h1>
<p>Database layer support caching of the responses using <code>db.getCached</code> call, it retrieves exactly one record from the configured cache, if no record exists it
will pull it from the database and on success will store it in the cache before returning to the client. When dealing with cached records, there is a special option
that must be passed to all put/update/del database methods in order to clear local cache, so next time the record will be retrieved with new changes from the database
and refresh the cache, that is <code>{ cached: true }</code> can be passed in the options parameter for the db methods that may modify records with cached contents. In any case
it is required to clear cache manually there is <code>db.clearCache</code> method for that.</p>
<p>Also there is a configuration option <code>-db-caching</code> to make any table automatically cached for all requests.</p>
<h2 id="local">Local</h2>
<p>If no cache is configured the local driver is used, it keeps the cache on the master process in the LRU pool and any worker or Web process
communicate with it via internal messaging provided by the <code>cluster</code> module. This works only for a single server.</p>
<h2 id="redis">Redis</h2>
<p>Set <code>ipc-client=redis://HOST[:PORT]</code> that points to the server running Redis server.</p>
<p>The config option <code>max_attempts</code> defines maximum number of times to reconnect before giving up. Any other <code>node-redis</code> module parameter can be passed as well in
the options or url, the system supports special parameters that start with <code>bk-</code>, it will extract them into options automatically.</p>
<p>For example:</p>
<pre><code>ipc-client=redis://host1?bk-max_attempts=3
ipc-client-backup=redis://host2
ipc-client-backup-options-max_attempts=3
</code></pre>
<h1 id="pubsub-or-queue-configurations">PUB/SUB or Queue configurations</h1>
<h2 id="redis-system-bus">Redis system bus</h2>
<p>If configured all processes subscribe to it and listen for system messages, it must support PUB/SUB and does not need to be reliable. Websockets
in the API server also use the system bus to send broadcasts between multiple api instances.</p>
<pre><code>ipc-client-system=redis://
ipc-system-queue=system
</code></pre>
<h2 id="redis-queue">Redis Queue</h2>
<p>To configure the backend to use Redis for job processing set <code>ipc-queue=redis://HOST</code> where HOST is IP address or hostname of the single Redis server.
This driver implements reliable Redis queue, with <code>visibilityTimeout</code> config option works similar to AWS SQS.</p>
<p>Once configured, then all calls to <code>jobs.submitJob</code> will push jobs to be executed to the Redis queue, starting somewhere a backend master
process with <code>-jobs-workers 2</code> will launch 2 worker processes which will start pulling jobs from the queue and execute.</p>
<p>The naming convention is that any function defined as <code>function(options, callback)</code> can be used as a job to be executed in one of the worker processes.</p>
<p>An example of how to perform jobs in the API routes:</p>
<pre><code class="language-javascript">
    core.describeArgs(&#39;app&#39;, [
        { name: &quot;queue&quot;, descr: &quot;Queue for jobs&quot; },
    ]);
    app.queue = &quot;somequeue&quot;;

    app.processAccounts = function(options, callback) {
        db.select(&quot;bk_user&quot;, { type: options.type || &quot;user&quot; }, (err, rows) =&gt; {
          ...
          callback();
        });
    }

    api.all(&quot;/process/accounts&quot;, function(req, res) {
        jobs.submitJob({ job: { &quot;app.processAccounts&quot;: { type: req.query.type } } }, { queueName: app.queue }, (err) =&gt; {
            api.sendReply(res, err);
        });
    });
</code></pre>
<h2 id="sqs">SQS</h2>
<p>To use AWS SQS for job processing set <code>ipc-queue=https://sqs.amazonaws.com....</code>, this queue system will poll SQS for new messages on a worker
and after successful execution will delete the message. For long running jobs it will automatically extend visibility timeout if it is configured.</p>
<h2 id="local-1">Local</h2>
<p>The local queue is implemented on the master process as a list, communication is done via local sockets between the master and workers.
This is intended for a single server development purposes only.</p>
<h2 id="nats">NATS</h2>
<p>To use NATS (<a href="https://nats.io">https://nats.io</a>) configure a queue like ipc-queue-nats=nats://HOST:PORT, it supports broadcasts and job queues only, visibility timeout is
supported as well.</p>
<h2 id="rabbitmq">RabbitMQ</h2>
<p>To configure the backend to use RabbitMQ for messaging set <code>ipc-queue=amqp://HOST</code> and optionally <code>amqp-options=JSON</code> with options to the amqp module.
Additional objects from the config JSON are used for specific AMQP functions: { queueParams: {}, subscribeParams: {}, publishParams: {} }. These
will be passed to the corresponding AMQP methods: <code>amqp.queue, amqp.queue.subcribe, amqp.publish</code>. See AMQP Node.js module for more info.</p>
<h1 id="security-configurations">Security configurations</h1>
<h2 id="api-only">API only</h2>
<p>This is default setup of the backend when all API requests except must provide valid signature and all HTML, JavaScript, CSS and image files
are available to everyone. This mode assumes that Web development will be based on &#39;single-page&#39; design when only data is requested from the Web server and all
rendering is done using JavaScript. This is how the <code>examples/api/api.html</code> developers console is implemented, using JQuery-UI and Knockout.js.</p>
<p>To see current default config parameters run any of the following commands:</p>
<pre><code>    bkjs bkhelp | grep api-allow

    node -e &#39;require(&quot;backendjs&quot;).core.showHelp()&#39;
</code></pre>
<h2 id="secure-web-site-client-verification">Secure Web site, client verification</h2>
<p>This is a mode when the whole Web site is secure by default, even access to the HTML files must be authenticated. In this mode the pages must defined &#39;Backend.session = true&#39;
during the initialization on every html page, it will enable Web sessions for the site and then no need to sign every API request.</p>
<p>The typical client JavaScript verification for the html page may look like this, it will redirect to login page if needed,
this assumes the default path &#39;/public&#39; still allowed without the signature:</p>
<pre><code class="language-javascript">   &lt;link href=&quot;/css/bkjs.bundle.css&quot; rel=&quot;stylesheet&quot;&gt;
   &lt;script src=&quot;/js/bkjs.bundle.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
   &lt;script&gt;
    $(function () {
       Bkjs.session = true;
       $(Bkjs).on(&quot;bkjs.nologin&quot;, function() { window.location=&#39;/public/index.html&#39;; });
       Bkjs.koInit();
   });
   &lt;/script&gt;
</code></pre>
<h2 id="secure-web-site-backend-verification">Secure Web site, backend verification</h2>
<p>On the backend side in your application app.js it needs more secure settings defined i.e. no html except /public will be accessible and
in case of error will be redirected to the login page by the server. Note, in the login page <code>Bkjs.session</code> must be set to true for all
html pages to work after login without singing every API request.</p>
<ol>
<li>We disable all allowed paths to the html and registration:</li>
</ol>
<pre><code class="language-javascript">   app.configureMiddleware = function(options, callback) {
       this.allow.splice(this.allow.indexOf(&#39;^/$&#39;), 1);
       this.allow.splice(this.allow.indexOf(&#39;\\.html$&#39;), 1);
       callback();
   }
</code></pre>
<ol start="2">
<li>We define an auth callback in the app and redirect to login if the request has no valid signature, we check all html pages, all allowed html pages from the /public
will never end up in this callback because it is called after the signature check but allowed pages are served before that:</li>
</ol>
<pre><code class="language-javascript">   api.registerPreProcess(&#39;&#39;, /^\/$|\.html$/, function(req, status, callback) {
       if (status.status != 200) {
           status.status = 302;
           status.url = &#39;/public/index.html&#39;;
       }
       callback(status);
   });
</code></pre>
<h1 id="websockets-connections">WebSockets connections</h1>
<p>The simplest way is to configure <code>ws-port</code> to the same value as the HTTP port. This will run WebSockets server along the regular Web server.</p>
<p>In the browser the connection config is stored in the <code>bkjs.wsconf</code> and by default it connects to the local server on port 8000.</p>
<p>There are two ways to send messages via Websockets to the server from a browser:</p>
<ul>
<li><p>as urls, eg. <code>bkjs.wsSend(&#39;/project/update?id=1&amp;name=Test2&#39;)</code></p>
<p>In this case the url will be parsed and checked for access and authorization before letting it pass via Express routes. This method allows to
share the same route handlers between HTTP and Websockets requests, the handlers will use the same code and all responses will be sent back,
only in the Websockets case the response will arrived in the message listener (see an example below)</p>
</li>
</ul>
<pre><code class="language-javascript">    bkjs.wsConnect({ path: &quot;/project/ws?id=1&quot; });

    $(bkjs).on(&quot;bkjs.ws.message&quot;, (msg) =&gt; {
        switch (msg.op) {
        case &quot;/account/update&quot;:
            bkjs.wsSend(&quot;/account/ws/account&quot;);
            break;

        case &quot;/project/update&quot;:
            for (const p in msg.project) app.project[p] = msg.project[p];
            break;

        case &quot;/message/new&quot;:
            bkjs.showAlert(&quot;info&quot;, `New message: ${msg.msg}`);
            break;
        }
    });
</code></pre>
<ul>
<li><p>as JSON objects, eg. <code>bkjs.wsSend({ op: &quot;/project/update&quot;, project: { id: 1, name: &quot;Test2&quot; } })</code></p>
<p>  In this case the server still have to check for access so it treats all JSON messages as coming from the path which was used during the connect,
  i.e. the one stored in the <code>bkjs.wsconf.path</code>. The Express route handler for this path will receive all messages from Websocket clients, the response will be
  received in the event listener the same way as for the first use case.</p>
</li>
</ul>
<pre><code class="language-javascript">    // Notify all clients who is using the project being updated
    api.app.all(&quot;/project/ws&quot;, (req, res) =&gt; {
        switch (req.query.op) {
        case &quot;/project/update&quot;:
            ....
           api.wsNotify({ query: { id: req.query.project.id }, { op: &quot;/project/update&quot;, project: req.query.project });
           break;
       }
       res.send(&quot;&quot;);
   });
</code></pre>
<p>In any case all Websocket messages sent from the server will arrive in the event handler and must be formatted properly in order to distinguish what is what, this is
the application logic. If the server needs to send a message to all or some specific clients for example due to some updates in the DB, it must use the
<code>api.wsNotify</code> function.</p>
<pre><code class="language-javascript">    // Received a new message for a user from external API service, notify all websocket clients by account id
    api.app.post(&quot;/api/message&quot;, (req, res) =&gt; {
        ....
        ... processing logic
        ....
        api.wsNotify({ account_id: req.query.uid }, { op: &quot;/message/new&quot;, msg: req.query.msg });
    });
</code></pre>
<h1 id="versioning">Versioning</h1>
<p>There is no ready to use support for different versions of API because there is no just one solution that satisfies all applications. But there are
tools ready to use that will allow to implement such versioning system in the backend. Some examples are provided below:</p>
<ul>
<li><p>Fixed versions
This is similar to AWS version system when versions are fixed and changed not very often. For such cases the backend exposes <code>core.bkVersion</code> which is
supposed to be a core backend version. This version is returned with every backend response in the Server: header. A client also can specify the core version
using <code>bk-version</code> header. When a request is parsed and the version is provided it will be set in the request options object as <code>apiVersion</code>.</p>
<p>All API routes are defined using Express middleware and one of the possible ways of dealing with different versions can look like this, by
appending version to the command it is very simple to call only changed API code.</p>
</li>
</ul>
<pre><code class="language-javascript">    api.all(/\/domain\/(get|put|del)/, function(req, res) {
        var options = api.getOptions(req);
        var cmd = req.params[0];
        if (options.apiVersion) cmd += &quot;/&quot; + options.apiVersion;
        switch (cmd) {
        case &quot;get&quot;:
            break;

        case &quot;get/2015-01-01&quot;:
            break;

        case &quot;put&quot;:
            break;

        case &quot;put/2015-02-01&quot;:
            break;

        case &quot;del&quot;
            break;
        }
    });
</code></pre>
<ul>
<li><p>Application semver support
For cases when applications support Semver kind of versioning and it may be too many releases the method above still can be used while the number of versions is
small, once too many different versions with different minor/patch numbers, it is easier to support greater/less comparisons.</p>
<p>The application version <code>bk-app</code> can be supplied in the query or as a header or in the user-agent HTTP header which is the easiest case for mobile apps.
In the middlware, the code can look like this:</p>
</li>
</ul>
<pre><code class="language-javascript">    var options = api.getOptions(req);
    var version = lib.toVersion(options.appVersion);
    switch (req.params[0]) {
    case &quot;get&quot;:
        if (version &lt; lib.toVersion(&quot;1.2.5&quot;)) {
            res.json({ id: 1, name: &quot;name&quot;, description: &quot;descr&quot; });
            break;
        }
        if (version &lt; lib.toVersion(&quot;1.1&quot;)) {
            res.json([id, name]);
            break;
        }
        res.json({ id: 1, name: &quot;name&quot;, descr: &quot;descr&quot; });
        break;
    }
</code></pre>
<p>The actual implementation can be modularized, split into functions, controllers.... there are no restrictions how to build the working backend code,
the backend just provides all necessary information for the middleware modules.</p>
<h1 id="the-backend-provisioning-utility-bkjs">The backend provisioning utility: bkjs</h1>
<p>The purpose of the <code>bkjs</code> shell script is to act as a helper tool in configuring and managing the backend environment
and as well to be used in operations on production systems. It is not required for the backend operations and provided as a convenience tool
which is used in the backend development and can be useful for others running or testing the backend.</p>
<p>Running without arguments will bring help screen with description of all available commands.</p>
<p>The tool is multi-command utility where the first argument is the command to be executed with optional additional arguments if needed.
On Linux, when started the bkjs tries to load and source the following config files:</p>
<pre><code>    /etc/default/bkjs
    /etc/sysconfig/bkjs
    $BKJS_HOME/etc/profile
</code></pre>
<p>Any of the following config files can redefine any environment variable thus pointing to the correct backend environment directory or
customize the running environment, these should be regular shell scripts using bash syntax.</p>
<p>Most common used commands are:</p>
<ul>
<li><p>bkjs watch - run the backend or the app for development purposes, uses local app.js if exists otherwise runs generic server</p>
</li>
<li><p>bkjs shell - start REPL shell with the backend module loaded and available for use, all submodules are available in the shell as well like core, db, api</p>
</li>
<li><p>bkjs sync [-path path] [-host host] [-user user] - sync sources of the app with the remote site, this is for development version of the backend only</p>
</li>
<li><p>bkjs init-server [-home path] [-user user] [-host name] [-domain name] - initialize Linux instance(Amazon) for backend use,
optional -home can be specified where the backend home will be instead of ~/.bkjs,   optional -user tells to use
existing user instead of the current user and not root.</p>
<p><strong>This command will create <code>/etc/sysconfig/bkjs</code> file with BKJS_HOME set to the home of the
backendjs app which was passed in the command line. This makes the bkjs or bksh run globally regardless of the current directory.</strong></p>
</li>
</ul>
<h1 id="web-development-notes">Web development notes</h1>
<p>Then run the dev build script to produce web/js/bkjs.bundle.js and web/css/bkjs.bundle.css</p>
<pre><code>    npm run devbuild
</code></pre>
<p>Now instead of including a bunch of .js or css files in the html pages it only needs /js/bkjs.bundle.js and /css/bkjs.bundle.css. The configuration is in the
package.json file.</p>
<p>The list of files to be used in bundles is in the package.json under <code>config.bundles</code>.</p>
<p>To enable auto bundler in your project just add to the local config <code>~/.bkjs/etc/config.local</code> a list of directories to be
watched for changes. For example adding these lines to the local config will enable the watcher and bundle support</p>
<pre><code>    watch-web=web/js,web/css,$HOME/src/js,$HOME/src/css
    watch-ignore=.bundle.(js|css)$
    build-web=bkjs web-bundle -dev
</code></pre>
<p>The simple script below allows to build the bundle and refresh Chrome tab automatically, saves several clicks:</p>
<pre><code>    #!/bin/bash
    bkjs web-bundle -dev -file $2
    [ &quot;$?&quot; != &quot;0&quot; ] &amp;&amp; exit
    osascript -e &quot;tell application \&quot;Google Chrome\&quot; to reload (tabs of window 1 whose URL contains \&quot;$1\&quot;)&quot;
</code></pre>
<p>To use it call this script instead in the config.local:</p>
<pre><code>    build-web=web-bundle.sh /website
</code></pre>
<p>NOTE: Because the rebuild happens while the watcher is running there are cases like the server is restarting or pulling a large update from the
repository when the bundle build may not be called or called too early. To force rebuild run the command:</p>
<pre><code>    bkjs web-bundle -dev -all -force
</code></pre>
<h1 id="deployment-use-cases">Deployment use cases</h1>
<h2 id="aws-instance-setup-with-node-and-backendjs">AWS instance setup with node and backendjs</h2>
<ul>
<li><p>start new AWS instance via AWS console, use Amazon Linux</p>
</li>
<li><p>login as <code>ec2-user</code></p>
</li>
<li><p>install commands</p>
<pre><code>  git clone https://github.com/vseryakov/backendjs.git
  sudo backendjs/bkjs install-ec2 -tools $(pwd)/backendjs/tools
</code></pre>
</li>
<li><p>run <code>ps agx</code>, it should show several backend processes running after monit started the service</p>
</li>
<li><p>try to access the instance via HTTP port for the API console or documentation</p>
</li>
</ul>
<p>NOTE: if running behind a Load balancer and actual IP address is needed set Express option in the command line <code>-api-express-options {&quot;trust%20proxy&quot;:1}</code>. In the config file
replacing spaces with %20 is not required.</p>
<h2 id="aws-provisioning-examples">AWS Provisioning examples</h2>
<p>Note: on OS X laptop the <code>-aws-sdk-profile uc</code> when AWS credentials are in the ~/.aws/credentials.</p>
<h3 id="make-an-ami">Make an AMI</h3>
<p>On the running machine which will be used for an image:</p>
<pre><code>    bksh -aws-create-image -no-reboot
</code></pre>
<p>Use an instance by tag for an image:</p>
<pre><code>    bksh -aws-create-image -no-reboot -instance-id `bkjs ec2-show -tag api -fmt id | head -1`
</code></pre>
<h3 id="launch-instances-when-not-using-autoscaling-groups">Launch instances when not using AutoScaling Groups</h3>
<p>When launching from an EC2 instance no need to specify any AWS credentials.</p>
<ul>
<li><p>admin (EC2)</p>
<pre><code> bksh -aws-sdk-profile uc -aws-launch-instances -aws-instance-type t2.small -subnet-name api -name admin -elb-name Admin -alarm-name alarms -public-ip 1 -dry-run
</code></pre>
</li>
<li><p>api (EC2)</p>
<pre><code> bksh -aws-sdk-profile uc -aws-launch-instances -aws-instance-type m3.large -subnet-name api -name api -elb-name api -alarm-name alarms -public-ip 1 -dry-run
</code></pre>
</li>
<li><p>jobs (EC2)</p>
<pre><code> bksh -aws-sdk-profile uc -aws-launch-instances -aws-instance-type t2.small -subnet-name internal -name sync -alarm-name alarms -dry-run
 bksh -aws-sdk-profile uc -aws-launch-instances -aws-instance-type t2.small -subnet-name internal -name sync -zone 1c -alarm-name alarms -dry-run
</code></pre>
</li>
<li><p>Elasticsearch</p>
<pre><code> bksh -aws-sdk-profile uc -aws-launch-instances -aws-instance-type m3.large -subnet-name internal -name elasticsearch -bkjs-cmd stop-service -bkjs-cmd &quot;init-elasticsearch-service -memsize 50&quot; -alarm-name alarms -public-ip 1 -dry-run
</code></pre>
</li>
<li><p>Redis</p>
<pre><code> bksh -aws-sdk-profile uc -aws-launch-instances -aws-instance-type m3.large -subnet-name internal -name redis -bkjs-cmd stop-service -bkjs-cmd &quot;init-redis-service -memsize 70&quot; -alarm-name alarms  -public-ip 1 -dry-run
</code></pre>
</li>
</ul>
<h3 id="copy-autoscaling-launch-templates-after-new-ami-is-created">Copy Autoscaling launch templates after new AMI is created</h3>
<pre><code>bksh -aws-create-launch-template-version -name jobs -aws-sdk-profile uc -dry-run
bksh -aws-create-launch-template-version -name api -aws-sdk-profile uc -dry-run
</code></pre>
<h3 id="update-route53-with-all-ips-from-running-instances">Update Route53 with all IPs from running instances</h3>
<pre><code>bksh -aws-set-route53 -name elasticsearch.ec-internal -filter elasticsearch
</code></pre>
<h2 id="proxy-mode">Proxy mode</h2>
<p>By default the Web proceses spawned by the server are load balanced using default cluster module which relies on the OS to do scheduling. On Linux with node 0.10
this is proven not to work properly due to the kernel keeping the context switches to a minimum thus resulting in one process to be very busy while the others
idle. Node versions 4 and above perform round-robin by default.</p>
<p>For such case the Backendjs implements the proxy mode by setting <code>proxy-port</code> config parameter to any number above 1000, this will be the initial
port for the web processes to listen for incoming requests, for example if use <code>-proxy-port 3000</code> and launch 2 web processes they will listen on ports
3000 and 3001. The main server process will start internal HTTP proxy and will perform round-robin load balancing the incoming requests between the web processes by forwarding
them to the web processes over TCP and then returning the responses back to the clients.</p>
<h2 id="configure-http-port">Configure HTTP port</h2>
<p>The first thing when deploying the backend into production is to change API HTTP port, by default is is 8000, but we would want port 80 so regardless
how the environment is setup it is ultimately 2 ways to specify the port for HTTP server to use:</p>
<ul>
<li><p>config file</p>
<p>The config file is always located in the etc/ folder in the backend home directory, how the home is specified depends on the system but basically it can be
defined via command line arguments as <code>-home</code> or via environment variables when using bkjs. See bkjs documentation but on AWS instances created with bkjs
<code>init-server</code> command, for non-standard home use <code>/etc/sysconfig/bkjs</code> profile, specify <code>BKJS_HOME=/home/backend</code> there and the rest will be taken care of</p>
</li>
<li><p>command line arguments</p>
<p>When running node scripts which use the backend, just specify <code>-home</code> command line argument with the directory where your backend should be and the backend will use it</p>
<p>Example:</p>
<pre><code>  node app.js -home $HOME -port 80
</code></pre>
</li>
<li><p>config database</p>
<p>If <code>-db-config</code> is specified in the command line or <code>db-config=</code> in the local config file, this will trigger loading additional
config parameters from the specified database pool, it will load all records from the <code>bk_config</code> table on that db pool. Using the database to store
configuration make it easier to maintain dynamic environment for example in case of auto scaling or launching on demand, this way
a new instance will query current config from the database and this eliminates supporting text files and distributing them to all instances.</p>
<p>The config database is refreshed from time to time acording to the <code>db-config-interval</code> parameter, also all records with <code>ttl</code> property in the bk_config
will be pulled every ttl interval and updated in place.</p>
</li>
<li><p>DNS records
Some config options may be kept in the DNS TXT records and every time a instance is started it will query the local DNS for such parameters. Only a small subset of
all config parameters support DNS store. To see which parameters can be stored in the DNS run <code>bkjs show-help</code> and look for &#39;DNS TXT configurable&#39;.</p>
</li>
</ul>
<h1 id="backend-library-development-mac-os-x-developers">Backend library development (Mac OS X, developers)</h1>
<ul>
<li><p><code>git clone https://github.com/vseryakov/backendjs.git</code> or <code>git clone git@github.com:vseryakov/backendjs.git</code></p>
</li>
<li><p>cd backendjs</p>
</li>
<li><p>if Node.js is already installed skip to the next section</p>
<ul>
<li><p>to install binary release run the command, it will install it into /opt/local on Darwin</p>
<p>  bkjs install-node</p>
<h1 id="to-install-into-different-path">To install into different path</h1>
<p>  bkjs install-node -prefix /usr/local/node</p>
</li>
<li><p><strong>Important</strong>: Add NODE_PATH=$BKJS_PREFIX/lib/node_modules to your environment in .profile or .bash_profile so
node can find global modules, replace $BKJS_PREFIX with the actual path unless this variable is also set in the .profile</p>
</li>
</ul>
</li>
<li><p>to install all dependencies and make backendjs module and bkjs globally available:</p>
<pre><code> npm link backendjs
</code></pre>
</li>
<li><p>to run local server on port 8000 run command:</p>
<pre><code>  bkjs web
</code></pre>
</li>
<li><p>to start the backend in command line mode, the backend environment is prepared and initialized including all database pools.
 This command line access allows you to test and run all functions from all modules of the backend without running full server
 similar to Node.js REPL functionality. All modules are accessible from the command line.</p>
<pre><code>  $ ./bkjs shell
  &gt; core.version
  &#39;0.70.0&#39;
  &gt; logger.setLevel(&#39;info&#39;)
</code></pre>
</li>
</ul>
<h1 id="design-considerations">Design considerations</h1>
<p>While creating Backendjs there were many questions and issues to be considered, some I was able to implement, some still not. Below are the thoughts that
might be useful when designing, developing or choosing the API platform:</p>
<ul>
<li>purpose of the API:<ul>
<li>to expose some parts of the existing system to external apps, users...</li>
<li>to make it the only way to access services</li>
<li>to complement another system</li>
</ul>
</li>
<li>scalability considerations:<ul>
<li>unlimited/uncontrolled access like mobile, web, more users the better</li>
<li>enterprise level, controlled growth</li>
<li>not to be horizontally scalable, just vertically</li>
</ul>
</li>
<li>security:<ul>
<li>support authentication, users, accounts, profiles...</li>
<li>just for robots, limited by api key only</li>
<li>signed requests only</li>
<li>support all access, web, mobile, desktop</li>
<li>user access controls, how to distinguish users, grant access to only parts of the API</li>
<li>ability to run custom/specific filters during processing API requests, independently and ability to extend the app without rewriting/rebuilding the whole system</li>
<li>third party authentication, OAUTH, user mapping</li>
</ul>
</li>
<li>platform/framework:<ul>
<li>one for all, same language/SDK/framework to cover all aspects</li>
<li>multiple languages/frameworks for different tasks, then how to integrate, how to communicate, share code</li>
<li>availability of the third party modules, libraries</li>
<li>support, forums, docs, how easy to learn for new developers</li>
<li>modularity, ability to develop by multiple developers, teams</li>
<li>flexibility in extending, how simple/easy to add custom stuff</li>
<li>maintenance, support,how easy to scale, change, replace parts</li>
</ul>
</li>
<li>database layer:<ul>
<li>one central database for everything</li>
<li>multiple database for different parts of the system according to scalability/other requirements</li>
<li>switch databases behind the scene in order to scale, adding to features, easier to maintain</li>
<li>caching, needs to be independent from other parts and easily enabled/disabled for different components preferably via config</li>
<li>to have or not ORM</li>
</ul>
</li>
<li>process management, easy to deploy, monitor</li>
<li>logging, metrics, profiling</li>
<li>agnostic to the frontends or to be included with some kind of MVC/server based tools</li>
<li>ability to support simple Web development for simple web pages without installing/supporting general purpose tools like Apache/PHP/nginx</li>
</ul>
<h1 id="api-endpoints-provided-by-the-backend">API endpoints provided by the backend</h1>
<p>All API endpoints are optional and can be disabled or replaced easily. By default the naming convention is:</p>
<pre><code> /namespace/command[/subname[/subcommand]]
</code></pre>
<p>Any HTTP methods can be used because its the command in the URL that defines the operation. The payload can be url-encoded query
parameters or JSON or any other format supported by any particular endpoint. This makes the backend universal and usable with any
environment, not just a Web browser. Request signature can be passed in the query so it does not require HTTP headers at all.</p>
<h2 id="authentication-and-sessions">Authentication and sessions</h2>
<h3 id="signature">Signature</h3>
<p>All requests to the API server must be signed with account login/secret pair.</p>
<ul>
<li>The algorithm how to sign HTTP requests (Version 1, 2):<ul>
<li>Split url to path and query parameters with &quot;?&quot;</li>
<li>Split query parameters with &quot;&amp;&quot;</li>
<li>&#39;&#39;&#39;ignore parameters with empty names&#39;&#39;&#39;</li>
<li>&#39;&#39;&#39;Sort&#39;&#39;&#39; list of parameters alphabetically</li>
<li>Join sorted list of parameters with &quot;&amp;&quot;<ul>
<li>Make sure all + are encoded as %2B</li>
</ul>
</li>
<li>Form canonical string to be signed as the following:<ul>
<li>Line1: The signature version</li>
<li>Line2: The application tag or other opaque data</li>
<li>Line3: The login name</li>
<li>Line4: The HTTP method(GET), followed by a newline.</li>
<li>Line5: The host name, lowercase, followed by a newline.</li>
<li>Line6: The request URI (/), followed by a newline.</li>
<li>Line7: The sorted and joined query parameters as one string, followed by a newline.</li>
<li>Line8: The expiration value in milliseconds, required, followed by a newline</li>
<li>Line9: The Content-Type HTTP header, lowercase, optional, followed by a newline</li>
<li>Line10: The SHA1 checksum of the body content, optional, for JSON and other forms of requests not supported by query parameters</li>
</ul>
</li>
<li>Computed HMAC-SHA1 digest from the canonical string and encode it as BASE64 string, preserve trailing = if any</li>
<li>Form the signature HTTP header as the following:<ul>
<li>The header string consist of multiple fields separated by pipe |<ul>
<li>Field1: Signature version:<ul>
<li>version 1, obsolete, do not use first 3 lines in the canonical string</li>
<li>version 2,3 to be used in session cookies only</li>
<li>version 4</li>
</ul>
</li>
<li>Field2: Application tag or other app specific data</li>
<li>Field3: account login or whatever it might be in the login column</li>
<li>Field4: HMAC-SHA digest from the canonical string, version 1 uses SHA1, other SHA256</li>
<li>Field5: expiration value in milliseconds, same as in the canonical string</li>
<li>Field6: SHA1 checksum of the body content, optional, for JSON and other forms of requests not supported by query parameters</li>
<li>Field7: empty, reserved for future use</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>The resulting signature is sent as HTTP header <code>bk-signature</code> or in the header specified by the <code>api-signature-name</code> config parameter.</p>
<p>For JSON content type, the method must be POST and no query parameters specified, instead everything should be inside the JSON object
which is placed in the body of the request. For additional safety, SHA1 checksum of the JSON payload can be calculated and passed in the signature,
this is the only way to ensure the body is not modified when not using query parameters.</p>
<p>See <a href="https://github.com/vseryakov/backendjs/blob/master/web/js/bkjs.js">web/js/bkjs.js</a> function <code>Bkjs.createSignature</code> or
<a href="https://github.com/vseryakov/backendjs/blob/master/api.js">api.js</a> function <code>api.createSignature</code> for the JavaScript implementations.</p>
<p>There is also native iOS implementation <a href="https://raw.githubusercontent.com/vseryakov/backendjs-ios/master/BKjs.m">Bkjs.m</a>.</p>
<h3 id="authentication-api">Authentication API</h3>
<ul>
<li><p><code>/auth</code></p>
<p> This API request returns the current user record from the <code>bk_user</code> table if the request is verified and the signature provided
 is valid. If no signature or it is invalid the result will be an error with the corresponding error code and message.</p>
<p> By default this endpoint is secured, i.e. requires a valid signature.</p>
<p> Parameters:</p>
<ul>
<li><p><code>_session=1</code> - if the call is authenticated a cookie with the session signature is returned, from now on
 all requests with such cookie will be authenticated, the primary use for this is Web apps</p>
</li>
<li><p><code>_accesstoken=1</code> - returns new access token to be used for subsequent requests without a signature for the current account,
 the token is short lived with expiration date returned as well. This access token can be used instead of a signature and
 is passed in the query as <code>bk-access-token=TOKEN</code>.</p>
<p> Example:</p>
<p>   /auth?_accesstoken=1</p>
<blockquote>
<p>{ id: &quot;XXXX...&quot;, name: &quot;Test User&quot;, &quot;bk-access-token&quot;: &quot;XXXXX....&quot;, &quot;bk-access-token-age&quot;: 604800000 }
   /account/get?bk-access-token=XXXXXX...
{ id: &quot;XXXX...&quot;, name: &quot;Test User&quot;, ... }</p>
</blockquote>
</li>
</ul>
</li>
<li><p><code>/login</code></p>
<p> Same as the /auth but it uses secret for user authentication, this request does not need a signature, just simple
 login and secret query parameters to be sent to the backend. This must be sent over SSL.</p>
<p> The intended usage is for Web sessions which use sessions cookies when sent with <code>_session=1</code> or to be used with access tokens when
 sent with <code>_accesstoken=1</code>.</p>
<p> Parameters:</p>
<ul>
<li><code>login</code> - account login</li>
<li><code>secret</code> - account secret</li>
<li><code>_session=1</code> - same as in /auth request</li>
<li><code>_accesstoken=1</code> - same as in /auth reuest</li>
</ul>
<p> On successful login, the result contains full account record including the secret, this is the only time when the secret is returned back</p>
<p> Example:</p>
</li>
</ul>
<pre><code class="language-javascript">    $.ajax({ url: &quot;/login?login=test123&amp;secret=test123&amp;_session=1&quot;,
        success: function(json, status, xhr) { console.log(json) }
    });

    &gt; { id: &quot;XXXX...&quot;, name: &quot;Test User&quot;, login: &quot;test123&quot;, ...}
</code></pre>
<ul>
<li><p><code>/logout</code></p>
<p> Logout the current user, clear session cookies if exist. For pure API access with the signature this will not do anything on the backend side.</p>
</li>
</ul>
<h2 id="accounts">Accounts</h2>
<p>The accounts API manages accounts and authentication, it provides basic user account features with common fields like email, name, address.</p>
<ul>
<li><p><code>/account/get</code></p>
<p>Returns information about the current account, all account columns are returned except the secret and other table columns with the property <code>priv</code></p>
<p>Response:</p>
<pre><code>      { &quot;id&quot;: &quot;57d07a4e28fc4f33bdca9f6c8e04d6c3&quot;,
      &quot;name&quot;: &quot;Test User&quot;,
      &quot;mtime&quot;: 1391824028,
      &quot;login&quot;: &quot;testuser&quot;,
      &quot;type&quot;: [&quot;user&quot;],
      }
</code></pre>
<p>How to make an account as admin</p>
<pre><code>      # Run backend shell
      bkjs shell

      # Update record by login
      &gt; db.update(&quot;bk_user&quot;, { login: &#39;login@name&#39;, type: &#39;admin&#39; });
</code></pre>
</li>
<li><p><code>/account/del</code></p>
<p>Delete current account, after this call no more requests will be authenticated with the current credentials</p>
</li>
<li><p><code>/account/update</code></p>
<p>Update current account with new values, the parameters are columns of the table <code>bk_user</code>, only columns with non empty values will be updated.</p>
<p>Example:</p>
<pre><code>      /account/update?name=New%2BName
</code></pre>
</li>
</ul>
<h3 id="health-enquiry">Health enquiry</h3>
<p>When running with AWS load balancer there should be a url that a load balancer polls all the time and this must be very quick and lightweight request. For this
purpose there is an API endpoint <code>/ping</code> that just responds with status 200. It is open by default in the default <code>api-allow-path</code> config parameter.</p>
<h2 id="public-images-endpoint">Public Images endpoint</h2>
<p>This endpoint can server any icon uploaded to the server for any account, it is supposed to be a non-secure method, i.e. no authentication will be performed and no signature
will be needed once it is configured which prefix can be public using <code>api-allow</code> or <code>api-allow-path</code> config parameters.</p>
<p>The format of the endpoint is:</p>
<ul>
<li><p><code>/image/prefix/id/type[.png|.jpg]</code></p>
<p>  Example:</p>
<pre><code>  # Configure accounts icons to be public in the etc/config
  api-allow-path=/image/account/

  # Or pass in the command line
  ./app.sh -api-allow-path /image/account/

  # Make requests
  /image/account/12345/0
  /image/account/12345/1
  /image/account/12345/1.jpg

  #Return icons for account 12345 for types 0 and 1
</code></pre>
</li>
</ul>
<h2 id="data">Data</h2>
<p>The data API is a generic way to access any table in the database with common operations, as oppose to the any specific APIs above this API only deals with
one table and one record without maintaining any other features like auto counters, cache...</p>
<p><em>Because it exposes the whole database to anybody who has a login it is a good idea to disable this endpoint in the production or provide access callback that verifies
who can access it.</em></p>
<ul>
<li><p>To disable this endpoint completely in the config: <code>deny-modules=bk_data</code></p>
</li>
<li><p>To allow admins to access it only in the config: <code>api-allow-admin=^/data</code></p>
</li>
<li><p>To allow admins to access it only:</p>
<pre><code>api.registerPreProcess(&#39;GET&#39;, &#39;/data&#39;, function(req, status, cb) { if (req.account.type != &quot;admin&quot;) return cb({ status: 401, message: &#39;access denied&#39; }; cb(status)); });
</code></pre>
</li>
</ul>
<p>This is implemented by the <code>data</code> module from the core.</p>
<ul>
<li><p><code>/data/columns</code></p>
</li>
<li><p><code>/data/columns/TABLE</code>
Return columns for all tables or the specific TABLE</p>
</li>
<li><p><code>/data/keys/TABLE</code>
Return primary keys for the given TABLE</p>
</li>
<li><p><code>/data/(select|search|list|get|add|put|update|del|incr|replace)/TABLE</code>
Perform database operation on the given TABLE, all options for the <code>db</code> functiobns are passed as query parametrrs prepended with underscore,
regular parameters are the table columns.</p>
<p>By default the API does not allow table scans without a condition to avoid expensive and long queries, to enable a scan pass <code>_noscan=0</code>.
For this to work the Data API must be configured as unsecure in the config file using the parameter <code>api-unsecure=data</code>.</p>
<p>Some tables like messages and connections perform data convertion before returning the results, mostly splitting combined columns like type into
separate fields. To return raw data pass the parameter <code>_noprocessrows=1</code>.</p>
<p>Example:</p>
<pre><code>  /data/get/bk_user?login=12345
  /data/update/bk_user?login=12345&amp;name=Admin
  /data/select/bk_user?name=john&amp;_ops=name,gt&amp;_select=name,email
  /data/select/bk_user?_noscan=0&amp;_noprocessrows=1
</code></pre>
</li>
</ul>
<h2 id="system-api">System API</h2>
<p>The system API returns information about the backend statistics, allows provisioning and configuration commands and other internal maintenance functions. By
default is is open for access to all users but same security considerations apply here as for the Data API.</p>
<p>This is implemented by the <code>system</code> module from the core. To enable this functionality specify <code>-preload-modules=bk_system</code>.</p>
<ul>
<li><p><code>/system/restart</code>
  Perform restart of the Web processes, this will be done gracefully, only one Web worker process will be restarting while the other processes will keep
  serving requests. The intention is to allow code updates on live systems without service interruption.</p>
</li>
<li><p><code>/system/cache/(init|stats|keys|get|set|put|incr|del|clear)</code>
  Access to the caching functions</p>
</li>
<li><p><code>/system/config/(init)</code>
  Access to the config functions</p>
</li>
<li><p><code>/system/msg/(init|send)</code>
  Access to the messaging functions</p>
</li>
<li><p><code>/system/jobs/(send)</code>
  Access to the jobs functions</p>
</li>
<li><p><code>/system/queue/(init|publish)</code>
  Access to the queue functions</p>
</li>
<li><p><code>/system/params/get</code>
  Return all config parameters applied from the config file(s) or remote database.</p>
</li>
<li><p><code>/system/stats/get</code>
Database pool statistics and other diagnostics</p>
<ul>
<li>latency - how long a pending request waits in queue at this moment</li>
<li>busy - how many busy error responses have been returned so far</li>
<li>pool - database metrics<ul>
<li>response - stats about how long it takes between issuing the db request and till the final moment all records are ready to be sent to the client</li>
<li>queue - stats about db requests at any given moment queued for the execution</li>
<li>cache - db cache response time and metrics</li>
</ul>
</li>
<li>api - Web requests metrics, same structure as for the db pool metrics</li>
<li>url - metrics per url endpoints</li>
</ul>
<p>Individual sub-objects:</p>
<ul>
<li>meter - Things that are measured as events / interval.<ul>
<li>rmean: The average rate since the meter was started.</li>
<li>rcnt: The total of all values added to the meter.</li>
<li>rate: The rate of the meter since the last toJSON() call.</li>
<li>r1m: The rate of the meter biased towards the last 1 minute.</li>
<li>r5m: The rate of the meter biased towards the last 5 minutes.</li>
<li>r15m: The rate of the meter biased towards the last 15 minutes.</li>
</ul>
</li>
<li>queue or histogram - Keeps a reservoir of statistically relevant values biased towards the last 5 minutes to explore their distribution<ul>
<li>hmin: The lowest observed value.</li>
<li>mmax: The highest observed value.</li>
<li>hsum: The sum of all observed values.</li>
<li>hvar: The variance of all observed values.</li>
<li>hmean: The average of all observed values.</li>
<li>hdev: The standard deviation of all observed values.</li>
<li>hcnt: The number of observed values.</li>
<li>hmed: median, 50% of all values in the reservoir are at or below this value.</li>
<li>hp75: See median, 75% percentile.</li>
<li>hp95: See median, 95% percentile.</li>
<li>hp99: See median, 99% percentile.</li>
<li>hp999: See median, 99.9% percentile.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="author">Author</h1>
<p>  Vlad Seryakov</p>
<p>Check out the <a href="http://bkjs.io">Documentation</a> for more details.</p>
<h2 id="configuration-parameters">Configuration parameters</h2>
<ul>
<li><code>help</code> - Print help and exit</li>
<li><code>log</code> - Set debugging level to any of test,dev,debug,info,notice,log,warn,error,none</li>
<li><code>log-filter</code> - Enable debug filters, format is: label,... to enable, and !label,... to disable. Only first argument is used for label in logger.debug</li>
<li><code>log-file</code> - Log to a file, if not specified used default logfile, disables syslog</li>
<li><code>log-ignore</code> - Regexp with property names which must not be exposed in the log when using custom logger inspector</li>
<li><code>log-inspect</code> - Install custom secure logger inspection instead of util.inspect</li>
<li><code>syslog</code> - Log messages to syslog, pass 0 to disable, 1 or url (tcp|udp|unix):[//host:port]/path[?facility]</li>
<li><code>console</code> - All logging goes to the console resetting all previous log related settings, this is used in the development mode mostly</li>
<li><code>home</code> - Specify home directory for the server, the server will try to chdir there or exit if it is not possible, the directory must exist</li>
<li><code>conf-file</code> - Name of the config file to be loaded instead of the default etc/config, can be relative or absolute path</li>
<li><code>err-file</code> - Path to the error log file where daemon will put app errors and crash stacks</li>
<li><code>etc-dir</code> - Path where to keep config files</li>
<li><code>tmp-dir</code> - Path where to keep temp files</li>
<li><code>spool-dir</code> - Path where to keep modifiable files</li>
<li><code>log-dir</code> - Path where to keep other log files, log-file and err-file are not affected by this</li>
<li><code>files-dir</code> - Path where to keep uploaded files</li>
<li><code>images-dir</code> - Path where to keep images</li>
<li><code>web-path</code> - Path where to keep web pages and other static files to be served by the web servers</li>
<li><code>views-path</code> - Path where to keep virtual hosts web pages, every subdirectory name is a host name to match with Host: header, www. is always stripped before matching vhost directory</li>
<li><code>modules-path</code> - Directory from where to load modules, these are the backendjs modules but in the same format and same conventions as regular node.js modules, the format of the files is NAME_{web,worker,shell}.js. The modules can load any other files or directories, this is just an entry point</li>
<li><code>locales-path</code> - Path where to keep locale translations</li>
<li><code>role</code> - Override servers roles, this may have very strange side effects and should only be used for testing purposes</li>
<li><code>umask</code> - Permissions mask for new files, calls system umask on startup, if not specified the current umask is used</li>
<li><code>force-uid</code> - Drop privileges if running as root by all processes as early as possibly, this reqiures uid being set to non-root user. A convenient switch to start the backend without using any other tools like su or sudo.</li>
<li><code>port</code> - port to listen for the HTTP server, this is global default</li>
<li><code>bind</code> - Bind to this address only, if not specified listen on all interfaces</li>
<li><code>backlog</code> - The maximum length of the queue of pending connections, used by HTTP server in listen.</li>
<li><code>ws-port</code> - Port to listen for WebSocket server, it can be the same as HTTP/S ports to co-exist on existing web servers</li>
<li><code>ws-bind</code> - Bind to this address only for WebSocket, if not specified listen on all interfaces, only when the port is different from existing web ports</li>
<li><code>ws-ping</code> - How often to ping Websocket connections</li>
<li><code>ws-path</code> - Websockets will be accepted only if request path maches the pattern</li>
<li><code>ws-queue</code> - A queue where to publish messages for websockets, API process will listen for messages and proxy it to all macthing connected websockets </li>
<li><code>ssl-port</code> - port to listen for HTTPS server, this is global default, be advised that proxy-port takes precedence</li>
<li><code>ssl-bind</code> - Bind to this address only for HTTPS server, if not specified listen on all interfaces</li>
<li><code>ssl-key</code> - Path to SSL prvate key</li>
<li><code>ssl-cert</code> - Path to SSL certificate</li>
<li><code>ssl-pfx</code> - A string or Buffer containing the private key, certificate and CA certs of the server in PFX or PKCS12 format. (Mutually exclusive with the key, cert and ca options.)</li>
<li><code>ssl-ca</code> - An array of strings or Buffers of trusted certificates in PEM format. If this is omitted several well known root CAs will be used, like VeriSign. These are used to authorize connections.</li>
<li><code>ssl-passphrase</code> - A string of passphrase for the private key or pfx</li>
<li><code>ssl-crl</code> - Either a string or list of strings of PEM encoded CRLs (Certificate Revocation List)</li>
<li><code>ssl-ciphers</code> - A string describing the ciphers to use or exclude. Consult <a href="http://www.openssl.org/docs/apps/ciphers.html#CIPHER_LIST_FORMAT">http://www.openssl.org/docs/apps/ciphers.html#CIPHER_LIST_FORMAT</a> for details on the format</li>
<li><code>ssl-request-cert</code> - If true the server will request a certificate from clients that connect and attempt to verify that certificate. </li>
<li><code>concurrency</code> - How many simultaneous tasks to run at the same time inside one process, this is used by async module only to perform several tasks at once, this is not multithreading but and only makes sense for I/O related tasks</li>
<li><code>timeout</code> - HTTP request idle timeout for servers in ms, how long to keep the connection socket open, this does not affect Long Poll requests</li>
<li><code>keep-alive-timeout</code> - Number of milliseconds to keep the HTTP conection alive</li>
<li><code>request-timeout</code> - Number of milliseconds to receive the entire request from the client</li>
<li><code>max-requests-per-socket</code> - The maximum number of requests a socket can handle before closing keep alive connection</li>
<li><code>daemon</code> - Daemonize the process, go to the background, can be specified only in the command line</li>
<li><code>shell</code> - Run command line shell, load the backend into the memory and prompt for the commands, can be specified only in the command line</li>
<li><code>monitor</code> - For production use, monitors the master and Web server processes and restarts if crashed or exited, can be specified only in the command line</li>
<li><code>master</code> - Start the master server, can be specified only in the command line, this process handles job schedules and starts Web server, keeps track of failed processes and restarts them</li>
<li><code>web</code> - Start Web server processes, spawn workers that listen on the same port, for use without master process which starts Web servers automatically</li>
<li><code>proxy-port</code> - Start the HTTP reverse proxy server, all Web workers will listen on different ports and will be load-balanced by the proxy, the proxy server will listen on global HTTP port and all workers will listen on ports starting with the proxy-port</li>
<li><code>proxy-ssl</code> - Start HTTPS reverse proxy to accept incoming SSL requests, ssl-key/cert must be defined</li>
<li><code>salt</code> - Set random or specific salt value to be used for consistent suuid generation</li>
<li><code>app-name</code> - Set appName and version explicitely an skip reading it from package.json, it can be just a name or name-version</li>
<li><code>app-package</code> - NPM package containing the application package.json, it will be added to the list of package.json files for app name and version discovery. The package must be included in the -allow-packages list.</li>
<li><code>instance-(.+)</code> - Set instance properties explicitly: tag, region, zone</li>
<li><code>run-mode</code> - Running mode for the app, used to separate different running environment and configurations. DNS TXT configurable.</li>
<li><code>no-monitor</code> - Disable monitor process, for cases when the master will be monitored by other tool like monit...</li>
<li><code>no-master</code> - Do not start the master process</li>
<li><code>no-watch</code> - Disable source code watcher</li>
<li><code>no-web</code> - Disable Web server processes, without this flag Web servers start by default</li>
<li><code>no-db</code> - Do not initialize DB drivers</li>
<li><code>no-db-config</code> - Do not retrieve config from the DB</li>
<li><code>no-dns</code> - Do not use DNS configuration during the initialization</li>
<li><code>no-modules</code> - Do not load any external modules</li>
<li><code>no-packages</code> - Do not load any NPM packages</li>
<li><code>no-configure</code> - Do not run configure hooks during the initialization</li>
<li><code>repl-port-([a-z]+)$</code> - Base REPL port for process role (server, master, web, worker), if specified it initializes REPL in the processes, for workers the port is computed by adding a worker id to the base port, for example if specified <code>-repl-port-web 2090</code> then a web worker will use any available 2091,2092...</li>
<li><code>repl-bind</code> - Listen only on specified address for REPL server in the master process</li>
<li><code>repl-file</code> - User specified file for REPL history</li>
<li><code>repl-size</code> - Max size to read on start from the end of the history file</li>
<li><code>worker</code> - Set this process as a worker even it is actually a master, this skips some initializations</li>
<li><code>allow-packages</code> - NPM packages to load on startup, the modules, locales, viewes, web subfolders from the package will be added automatically to the system paths, modules will be loaded if present, the config file in etc subfolder will be parsed if present</li>
<li><code>preload-modules</code> - Modules to preload first from any modules/ folders including the system folder, this can be used to preload default bkjs system modules</li>
<li><code>user-agent</code> - Add HTTP user-agent header to be used in HTTP requests, for scrapers or other HTTP requests that need to be pretended coming from Web browsers</li>
<li><code>backend-host</code> - Host of the master backend, can be used for backend nodes communications using core.sendRequest function calls with relative URLs, also used in tests.</li>
<li><code>backend-login</code> - Credentials login for the master backend access when using core.sendRequest</li>
<li><code>backend-secret</code> - Credentials secret for the master backend access when using core.sendRequest</li>
<li><code>host-name</code> - Hostname/domain to use for communications, default is current domain of the host machine</li>
<li><code>config-domain</code> - Domain to query for configuration TXT records, must be specified to enable DNS configuration</li>
<li><code>watch</code> - Watch sources directory for file changes to restart the server, for development only, the backend module files will be added to the watch list automatically, so only app specific directores should be added. In the production -monitor must be used.</li>
<li><code>watch-ignore</code> - Files to be ignored by the watcher</li>
<li><code>watch-match</code> - Files to be watched, .js and .css is the default</li>
<li><code>watch-web</code> - List of directories to be watched for file modifications and execute a <code>buildWeb</code> command to produce bundles, apps, etc... Relative paths will be applied to all packages, example: web/js,web/css</li>
<li><code>build-web</code> - Command to run on web files modifications, to be used with tools like minify/uglify</li>
<li><code>locales</code> - A list of locales to load from the locales/ directory, only language name must be specified, example: en,es. It enables internal support for <code>res.__</code> and <code>req.__</code> methods that can be used for translations, for each request the internal language header will be honored forst, then HTTP Accept-Language</li>
<li><code>no-locales</code> - Do not load locales on start</li>
<li><code>email-from</code> - Email address to be used when sending emails from the backend</li>
<li><code>email-transport</code> - Send emails via supported transports: ses:, sendgrid://?key=SG, if not set default SMTP settings are used</li>
<li><code>smtp-(.+)</code> - SMTP server parameters, user, password, host, ssl, tls...see emailjs for details</li>
<li><code>tmp-watcher-(.+)</code> - How long to keep files per subdirectory in seconds</li>
<li><code>stop-on-error</code> - Exit the process on any error when loading modules, for dev purposes</li>
<li><code>logwatcher-pool</code> - DB pool to keep track of positions for log files, default is local</li>
<li><code>logwatcher-mod</code> - Alternative module to be used for watching logs, it must have the <code>watchLogs</code> method, if no such method defined there the logwatcher does not run at all</li>
<li><code>logwatcher-from</code> - Email address to send logwatcher notifications from, for cases with strict mail servers accepting only from known addresses</li>
<li><code>logwatcher-subject</code> - Email subject template, all placeholders have access to the core module only</li>
<li><code>logwatcher-interval</code> - How often to check for errors in the log files in seconds</li>
<li><code>logwatcher-any-range</code> - Number of lines for matched channel <code>any</code> to be attached to the previous matched channel, if more than this number use the channel <code>any</code> on its own</li>
<li><code>logwatcher-match-[a-z]+</code> - Regexp patterns that match conditions for logwatcher notifications, this is in addition to default backend logger patterns, suffix defines the log channel to use, like error, warning.... Special channel <code>any</code> is reserved to send matched lines to the previously matched channel if within configured range. Example: <code>-logwatcher-match-error=^failed:</code> <code>-logwatcher-match-any=line:[0-9]+</code></li>
<li><code>logwatcher-send-[a-z]+</code> - Email address or other supported transport for the logwatcher notifications, the monitor process scans system and backend log files for errors and sends them to this email address, if not specified no log watching will happen, each channel must define a transport separately, one of error, warning, info, all. Supported transports: table://TABLE, <a href="http://URL">http://URL</a>, sns://ARN, ses://EMAIL, email@addr. Example: <code>-logwatcher-send-error=help@error.com</code></li>
<li><code>logwatcher-ignore-[a-z]+</code> - Regexp with patterns that need to be ignored by the logwatcher process, it is added to the list of existing patterns for each specified channel separately</li>
<li><code>logwatcher-once-[a-z0-9]+</code> - Regexp with patterns that need to be included only once by the logwatcher process, it is added to the list of existng patterns by tag to keep track each pattern separately, example: -logwatcher-once-restart &#39;restarting.+&#39; -logwatcher-once-recon &#39;reconnecting:.+&#39;</li>
<li><code>logwatcher-file(-[a-z]+)?</code> - Add a file to be watched by the logwatcher, it will use all configured match patterns</li>
<li><code>bkjs-help</code> - Print help and exit</li>
<li><code>bkjs-log</code> - Set debugging level to any of test,dev,debug,info,notice,log,warn,error,none</li>
<li><code>bkjs-log-filter</code> - Enable debug filters, format is: label,... to enable, and !label,... to disable. Only first argument is used for label in logger.debug</li>
<li><code>bkjs-log-file</code> - Log to a file, if not specified used default logfile, disables syslog</li>
<li><code>bkjs-log-ignore</code> - Regexp with property names which must not be exposed in the log when using custom logger inspector</li>
<li><code>bkjs-log-inspect</code> - Install custom secure logger inspection instead of util.inspect</li>
<li><code>bkjs-syslog</code> - Log messages to syslog, pass 0 to disable, 1 or url (tcp|udp|unix):[//host:port]/path[?facility]</li>
<li><code>bkjs-console</code> - All logging goes to the console resetting all previous log related settings, this is used in the development mode mostly</li>
<li><code>bkjs-home</code> - Specify home directory for the server, the server will try to chdir there or exit if it is not possible, the directory must exist</li>
<li><code>bkjs-conf-file</code> - Name of the config file to be loaded instead of the default etc/config, can be relative or absolute path</li>
<li><code>bkjs-err-file</code> - Path to the error log file where daemon will put app errors and crash stacks</li>
<li><code>bkjs-etc-dir</code> - Path where to keep config files</li>
<li><code>bkjs-tmp-dir</code> - Path where to keep temp files</li>
<li><code>bkjs-spool-dir</code> - Path where to keep modifiable files</li>
<li><code>bkjs-log-dir</code> - Path where to keep other log files, log-file and err-file are not affected by this</li>
<li><code>bkjs-files-dir</code> - Path where to keep uploaded files</li>
<li><code>bkjs-images-dir</code> - Path where to keep images</li>
<li><code>bkjs-web-path</code> - Path where to keep web pages and other static files to be served by the web servers</li>
<li><code>bkjs-views-path</code> - Path where to keep virtual hosts web pages, every subdirectory name is a host name to match with Host: header, www. is always stripped before matching vhost directory</li>
<li><code>bkjs-modules-path</code> - Directory from where to load modules, these are the backendjs modules but in the same format and same conventions as regular node.js modules, the format of the files is NAME_{web,worker,shell}.js. The modules can load any other files or directories, this is just an entry point</li>
<li><code>bkjs-locales-path</code> - Path where to keep locale translations</li>
<li><code>bkjs-role</code> - Override servers roles, this may have very strange side effects and should only be used for testing purposes</li>
<li><code>bkjs-umask</code> - Permissions mask for new files, calls system umask on startup, if not specified the current umask is used</li>
<li><code>bkjs-force-uid</code> - Drop privileges if running as root by all processes as early as possibly, this reqiures uid being set to non-root user. A convenient switch to start the backend without using any other tools like su or sudo.</li>
<li><code>bkjs-port</code> - port to listen for the HTTP server, this is global default</li>
<li><code>bkjs-bind</code> - Bind to this address only, if not specified listen on all interfaces</li>
<li><code>bkjs-backlog</code> - The maximum length of the queue of pending connections, used by HTTP server in listen.</li>
<li><code>bkjs-ws-port</code> - Port to listen for WebSocket server, it can be the same as HTTP/S ports to co-exist on existing web servers</li>
<li><code>bkjs-ws-bind</code> - Bind to this address only for WebSocket, if not specified listen on all interfaces, only when the port is different from existing web ports</li>
<li><code>bkjs-ws-ping</code> - How often to ping Websocket connections</li>
<li><code>bkjs-ws-path</code> - Websockets will be accepted only if request path maches the pattern</li>
<li><code>bkjs-ws-queue</code> - A queue where to publish messages for websockets, API process will listen for messages and proxy it to all macthing connected websockets </li>
<li><code>bkjs-ssl-port</code> - port to listen for HTTPS server, this is global default, be advised that proxy-port takes precedence</li>
<li><code>bkjs-ssl-bind</code> - Bind to this address only for HTTPS server, if not specified listen on all interfaces</li>
<li><code>bkjs-ssl-key</code> - Path to SSL prvate key</li>
<li><code>bkjs-ssl-cert</code> - Path to SSL certificate</li>
<li><code>bkjs-ssl-pfx</code> - A string or Buffer containing the private key, certificate and CA certs of the server in PFX or PKCS12 format. (Mutually exclusive with the key, cert and ca options.)</li>
<li><code>bkjs-ssl-ca</code> - An array of strings or Buffers of trusted certificates in PEM format. If this is omitted several well known root CAs will be used, like VeriSign. These are used to authorize connections.</li>
<li><code>bkjs-ssl-passphrase</code> - A string of passphrase for the private key or pfx</li>
<li><code>bkjs-ssl-crl</code> - Either a string or list of strings of PEM encoded CRLs (Certificate Revocation List)</li>
<li><code>bkjs-ssl-ciphers</code> - A string describing the ciphers to use or exclude. Consult <a href="http://www.openssl.org/docs/apps/ciphers.html#CIPHER_LIST_FORMAT">http://www.openssl.org/docs/apps/ciphers.html#CIPHER_LIST_FORMAT</a> for details on the format</li>
<li><code>bkjs-ssl-request-cert</code> - If true the server will request a certificate from clients that connect and attempt to verify that certificate. </li>
<li><code>bkjs-concurrency</code> - How many simultaneous tasks to run at the same time inside one process, this is used by async module only to perform several tasks at once, this is not multithreading but and only makes sense for I/O related tasks</li>
<li><code>bkjs-timeout</code> - HTTP request idle timeout for servers in ms, how long to keep the connection socket open, this does not affect Long Poll requests</li>
<li><code>bkjs-keep-alive-timeout</code> - Number of milliseconds to keep the HTTP conection alive</li>
<li><code>bkjs-request-timeout</code> - Number of milliseconds to receive the entire request from the client</li>
<li><code>bkjs-max-requests-per-socket</code> - The maximum number of requests a socket can handle before closing keep alive connection</li>
<li><code>bkjs-daemon</code> - Daemonize the process, go to the background, can be specified only in the command line</li>
<li><code>bkjs-shell</code> - Run command line shell, load the backend into the memory and prompt for the commands, can be specified only in the command line</li>
<li><code>bkjs-monitor</code> - For production use, monitors the master and Web server processes and restarts if crashed or exited, can be specified only in the command line</li>
<li><code>bkjs-master</code> - Start the master server, can be specified only in the command line, this process handles job schedules and starts Web server, keeps track of failed processes and restarts them</li>
<li><code>bkjs-web</code> - Start Web server processes, spawn workers that listen on the same port, for use without master process which starts Web servers automatically</li>
<li><code>bkjs-proxy-port</code> - Start the HTTP reverse proxy server, all Web workers will listen on different ports and will be load-balanced by the proxy, the proxy server will listen on global HTTP port and all workers will listen on ports starting with the proxy-port</li>
<li><code>bkjs-proxy-ssl</code> - Start HTTPS reverse proxy to accept incoming SSL requests, ssl-key/cert must be defined</li>
<li><code>bkjs-salt</code> - Set random or specific salt value to be used for consistent suuid generation</li>
<li><code>bkjs-app-name</code> - Set appName and version explicitely an skip reading it from package.json, it can be just a name or name-version</li>
<li><code>bkjs-app-package</code> - NPM package containing the application package.json, it will be added to the list of package.json files for app name and version discovery. The package must be included in the -allow-packages list.</li>
<li><code>bkjs-instance-(.+)</code> - Set instance properties explicitly: tag, region, zone</li>
<li><code>bkjs-run-mode</code> - Running mode for the app, used to separate different running environment and configurations. DNS TXT configurable.</li>
<li><code>bkjs-no-monitor</code> - Disable monitor process, for cases when the master will be monitored by other tool like monit...</li>
<li><code>bkjs-no-master</code> - Do not start the master process</li>
<li><code>bkjs-no-watch</code> - Disable source code watcher</li>
<li><code>bkjs-no-web</code> - Disable Web server processes, without this flag Web servers start by default</li>
<li><code>bkjs-no-db</code> - Do not initialize DB drivers</li>
<li><code>bkjs-no-db-config</code> - Do not retrieve config from the DB</li>
<li><code>bkjs-no-dns</code> - Do not use DNS configuration during the initialization</li>
<li><code>bkjs-no-modules</code> - Do not load any external modules</li>
<li><code>bkjs-no-packages</code> - Do not load any NPM packages</li>
<li><code>bkjs-no-configure</code> - Do not run configure hooks during the initialization</li>
<li><code>bkjs-repl-port-([a-z]+)$</code> - Base REPL port for process role (server, master, web, worker), if specified it initializes REPL in the processes, for workers the port is computed by adding a worker id to the base port, for example if specified <code>-repl-port-web 2090</code> then a web worker will use any available 2091,2092...</li>
<li><code>bkjs-repl-bind</code> - Listen only on specified address for REPL server in the master process</li>
<li><code>bkjs-repl-file</code> - User specified file for REPL history</li>
<li><code>bkjs-repl-size</code> - Max size to read on start from the end of the history file</li>
<li><code>bkjs-worker</code> - Set this process as a worker even it is actually a master, this skips some initializations</li>
<li><code>bkjs-allow-packages</code> - NPM packages to load on startup, the modules, locales, viewes, web subfolders from the package will be added automatically to the system paths, modules will be loaded if present, the config file in etc subfolder will be parsed if present</li>
<li><code>bkjs-preload-modules</code> - Modules to preload first from any modules/ folders including the system folder, this can be used to preload default bkjs system modules</li>
<li><code>bkjs-user-agent</code> - Add HTTP user-agent header to be used in HTTP requests, for scrapers or other HTTP requests that need to be pretended coming from Web browsers</li>
<li><code>bkjs-backend-host</code> - Host of the master backend, can be used for backend nodes communications using core.sendRequest function calls with relative URLs, also used in tests.</li>
<li><code>bkjs-backend-login</code> - Credentials login for the master backend access when using core.sendRequest</li>
<li><code>bkjs-backend-secret</code> - Credentials secret for the master backend access when using core.sendRequest</li>
<li><code>bkjs-host-name</code> - Hostname/domain to use for communications, default is current domain of the host machine</li>
<li><code>bkjs-config-domain</code> - Domain to query for configuration TXT records, must be specified to enable DNS configuration</li>
<li><code>bkjs-watch</code> - Watch sources directory for file changes to restart the server, for development only, the backend module files will be added to the watch list automatically, so only app specific directores should be added. In the production -monitor must be used.</li>
<li><code>bkjs-watch-ignore</code> - Files to be ignored by the watcher</li>
<li><code>bkjs-watch-match</code> - Files to be watched, .js and .css is the default</li>
<li><code>bkjs-watch-web</code> - List of directories to be watched for file modifications and execute a <code>buildWeb</code> command to produce bundles, apps, etc... Relative paths will be applied to all packages, example: web/js,web/css</li>
<li><code>bkjs-build-web</code> - Command to run on web files modifications, to be used with tools like minify/uglify</li>
<li><code>bkjs-locales</code> - A list of locales to load from the locales/ directory, only language name must be specified, example: en,es. It enables internal support for <code>res.__</code> and <code>req.__</code> methods that can be used for translations, for each request the internal language header will be honored forst, then HTTP Accept-Language</li>
<li><code>bkjs-no-locales</code> - Do not load locales on start</li>
<li><code>bkjs-email-from</code> - Email address to be used when sending emails from the backend</li>
<li><code>bkjs-email-transport</code> - Send emails via supported transports: ses:, sendgrid://?key=SG, if not set default SMTP settings are used</li>
<li><code>bkjs-smtp-(.+)</code> - SMTP server parameters, user, password, host, ssl, tls...see emailjs for details</li>
<li><code>bkjs-tmp-watcher-(.+)</code> - How long to keep files per subdirectory in seconds</li>
<li><code>bkjs-stop-on-error</code> - Exit the process on any error when loading modules, for dev purposes</li>
<li><code>bkjs-logwatcher-pool</code> - DB pool to keep track of positions for log files, default is local</li>
<li><code>bkjs-logwatcher-mod</code> - Alternative module to be used for watching logs, it must have the <code>watchLogs</code> method, if no such method defined there the logwatcher does not run at all</li>
<li><code>bkjs-logwatcher-from</code> - Email address to send logwatcher notifications from, for cases with strict mail servers accepting only from known addresses</li>
<li><code>bkjs-logwatcher-subject</code> - Email subject template, all placeholders have access to the core module only</li>
<li><code>bkjs-logwatcher-interval</code> - How often to check for errors in the log files in seconds</li>
<li><code>bkjs-logwatcher-any-range</code> - Number of lines for matched channel <code>any</code> to be attached to the previous matched channel, if more than this number use the channel <code>any</code> on its own</li>
<li><code>bkjs-logwatcher-match-[a-z]+</code> - Regexp patterns that match conditions for logwatcher notifications, this is in addition to default backend logger patterns, suffix defines the log channel to use, like error, warning.... Special channel <code>any</code> is reserved to send matched lines to the previously matched channel if within configured range. Example: <code>-logwatcher-match-error=^failed:</code> <code>-logwatcher-match-any=line:[0-9]+</code></li>
<li><code>bkjs-logwatcher-send-[a-z]+</code> - Email address or other supported transport for the logwatcher notifications, the monitor process scans system and backend log files for errors and sends them to this email address, if not specified no log watching will happen, each channel must define a transport separately, one of error, warning, info, all. Supported transports: table://TABLE, <a href="http://URL">http://URL</a>, sns://ARN, ses://EMAIL, email@addr. Example: <code>-logwatcher-send-error=help@error.com</code></li>
<li><code>bkjs-logwatcher-ignore-[a-z]+</code> - Regexp with patterns that need to be ignored by the logwatcher process, it is added to the list of existing patterns for each specified channel separately</li>
<li><code>bkjs-logwatcher-once-[a-z0-9]+</code> - Regexp with patterns that need to be included only once by the logwatcher process, it is added to the list of existng patterns by tag to keep track each pattern separately, example: -logwatcher-once-restart &#39;restarting.+&#39; -logwatcher-once-recon &#39;reconnecting:.+&#39;</li>
<li><code>bkjs-logwatcher-file(-[a-z]+)?</code> - Add a file to be watched by the logwatcher, it will use all configured match patterns</li>
<li><code>ipc-none</code> - disable all IPC subsystems</li>
<li><code>ipc-ping-interval</code> - Interval for a worker keep-alive pings, if not received within this period it will be killed</li>
<li><code>ipc-lru-max</code> - Max number of items in the limiter LRU cache, this cache is managed by the master Web server process and available to all Web processes maintaining only one copy per machine</li>
<li><code>ipc-system-queue</code> - System queue name to send broadcast control messages, this is a PUB/SUB queue to process system messages like restart, re-init config,...</li>
<li><code>ipc-(cache|queue)-?([a-z0-9]+)?</code> - An URL that points to a cache/queue server in the format <code>PROTO://HOST[:PORT]?PARAMS</code>, multiple clients can be defined with unique names, all params starting with <code>bk-</code> will be copied into the options without the prefix and removed from the url, the rest of params will be left in the url, ex: -ipc-client-redis redis://localhost?bk-count=3&amp;bk-ttl=3000</li>
<li><code>ipc-(cache|queue)-([a-z0-9]+)?-?options</code> - Additional parameters for clients, specific to each implementation, ex: <code>-ipc-client-options {&quot;ttl&quot;:3000}</code></li>
<li><code>ipc-(cache|queue)(-([a-z0-9]+)?-?options)-(.+)$</code> - Additional parameters for clients, specific to each implementation, ex: <code>-ipc-client-options-ttl 3000</code></li>
<li><code>aws-key</code> - AWS access key</li>
<li><code>aws-secret</code> - AWS access secret</li>
<li><code>aws-token</code> - AWS security token</li>
<li><code>aws-region</code> - AWS region</li>
<li><code>aws-zone</code> - AWS availability zone</li>
<li><code>aws-meta</code> - Retrieve instance metadata, 0 to disable</li>
<li><code>aws-sdk-profile</code> - AWS SDK profile to use when reading credentials file</li>
<li><code>aws-sns-app-arn</code> - SNS Platform application ARN to be used for push notifications</li>
<li><code>aws-key-name</code> - AWS instance keypair name for remote job instances or other AWS commands</li>
<li><code>aws-elb-name</code> - AWS ELB name to be registered with on start up or other AWS commands</li>
<li><code>aws-target-group</code> - AWS ELB target group to be registered with on start up or other AWS commands</li>
<li><code>aws-elastic-ip</code> - AWS Elastic IP to be associated on start</li>
<li><code>aws-host-name</code> - List of hosts to update in Route54 zone with the current private IP address, hosts must be in FQDN format, supports @..@ core.instance placeholders</li>
<li><code>aws-iam-profile</code> - IAM instance profile name for instances or commands</li>
<li><code>aws-image-id</code> - AWS image id to be used for instances or commands</li>
<li><code>aws-subnet-id</code> - AWS subnet id to be used for instances or commands</li>
<li><code>aws-vpc-id</code> - AWS VPC id to be used for instances or commands</li>
<li><code>aws-group-id</code> - AWS security group(s) to be used for instances or commands</li>
<li><code>aws-instance-type</code> - AWS instance type to launch on demand</li>
<li><code>aws-account-id</code> - AWS account id if not running on an instance</li>
<li><code>aws-eni-id</code> - AWS Elastic Network Interfaces to attach on start, format is: eni[:index],eni...</li>
<li><code>aws-config-parameters</code> - Prefix for AWS Config Parameters Store to load and parse as config before initializing the database pools, example: /bkjs/config/</li>
<li><code>aws-set-parameters</code> - AWS Config Parameters Store to set on start, supports @..@ core.instance placeholders: format is: path:value,....</li>
<li><code>aws-conf-file</code> - S3 url for config file to download on start</li>
<li><code>aws-logwatcher-groups</code> - List of AWS Cloudwatch Logs groups to watch for errors, format is: name:type,...</li>
<li><code>aws-logwatcher-filters-(.+)</code> - AWS Cloudwatch Logs filter pattern by group, overrides the global filter</li>
<li><code>aws-logwatcher-filter</code> - AWS Cloudwatch Logs filter pattern, only matched events will be returned and analyzed the the core logwatcher regexps</li>
<li><code>aws-logwatcher-match-(.+)</code> - Logwatcher line regexp patterns by group, overrides default regexp patterns</li>
<li><code>aws-ddb-endpoint</code> - Default endpoint to use, for LocalDynamoDB use</li>
<li><code>aws-ddb-read-capacity</code> - Default DynamoDB read capacity for all tables</li>
<li><code>aws-ddb-write-capacity</code> - Default DynamoDB write capacity for all tables</li>
<li><code>aws-ddb-retry-count</code> - Default DynamoDB number of retries in case of throttling event</li>
<li><code>aws-ddb-retry-timeout</code> - Default DynamoDB min timeout for retry backoff in case of throttling event</li>
<li><code>aws-ddb-retry-status</code> - Default DynamoDB HTTP statuses to retry in case of throttling event or an error</li>
<li><code>db-pool</code> - Default pool to be used for db access without explicit pool specified. DNS TXT configurable.</li>
<li><code>db-name</code> - Default database name to be used for default connections in cases when no db is specified in the connection url</li>
<li><code>db-create-tables</code> - Create tables in the database or perform table upgrades for new columns in all pools, only shell or server process can perform this operation</li>
<li><code>db-create-tables-roles</code> - Only processes with these roles can create tables</li>
<li><code>db-cache-tables</code> - List of tables that can be cached: bk_user, bk_counter. This list defines which DB calls will cache data with currently configured cache. This is global for all db pools.</li>
<li><code>db-cache-pools</code> - List of pools which trigger cache flushes on update.</li>
<li><code>db-cache-sync</code> - List of tables that perform synchronized cache updates before returning from a DB call, by default cache updates are done in the background</li>
<li><code>db-cache-keys-([a-z0-9_]+)-(.+)</code> - List of columns to be used for the table cache, all update operations will flush the cache if the cache key can be created from the record columns. This is for ad-hoc and caches to be used for custom selects which specified the cache key.</li>
<li><code>db-describe-tables</code> - A JSON object with table descriptions to be merged with the existing definitions</li>
<li><code>db-cache-ttl</code> - Default global TTL for cached tables</li>
<li><code>db-cache-ttl-(.+)</code> - TTL in milliseconds for each individual table being cached</li>
<li><code>db-cache-name-(.+)</code> - Cache client name to use for cache reading and writing for each table instead of the default in order to split cache usage for different tables, it can be just a table name or <code>pool.table</code></li>
<li><code>db-cache-update-(.+)</code> - Cache client name to use for updating only for each table instead of the default in order to split cache usage for different tables, it can be just a table name or <code>pool.table</code>. This cache takes precedence for updating cache over <code>cache-name</code> parameter</li>
<li><code>db-cache2-max</code> - Max number of items to keep in the LRU Level 2 cache</li>
<li><code>db-cache2-(.+)</code> - Tables with TTL for level2 cache, i.e. in the local process LRU memory. It works before the primary cache and keeps records in the local LRU cache for the given amount of time, the TTL is in ms and must be greater than zero for level 2 cache to work</li>
<li><code>db-custom-column-([a-z0-9_]+)-(.+)</code> - A column that is allowed to be used in any table, the name is a regexp with the value to be a type</li>
<li><code>db-local</code> - Local database pool for properties, cookies and other local instance only specific stuff</li>
<li><code>db-config</code> - Configuration database pool to be used to retrieve config parameters from the database, must be defined to use remote db for config parameters, set to <code>default</code> to use current default pool</li>
<li><code>db-config-interval</code> - Interval between loading configuration from the database configured with -db-config, in minutes, 0 disables refreshing config from the db</li>
<li><code>db-config-count</code> - Max number of records to read fron the config table</li>
<li><code>db-config-roles</code> - Roles to assume when pulling config parameters from the config table</li>
<li><code>db-local-tables</code> - Only enable local, default and config pools</li>
<li><code>db-no-cache-columns</code> - Do not read/cache table columns</li>
<li><code>db-cache-columns-interval</code> - How often in minutes to refresh tables columns from the database, it calls cacheColumns for each pool which supports it</li>
<li><code>db-match-tables-(.+)</code> - Table columns to be returned by matching the regexp, for ad-hoc tables</li>
<li><code>db-skip-drop</code> - A pattern of table names which will skipped in db.drop operations to prevent accidental table deletion</li>
<li><code>db-aliases-(.+)</code> - Table aliases to be used instead of the requested table name</li>
<li><code>db-([a-z0-9]+)-pool$</code> - A database pool name, depending on the driver it can be an URL, name or pathname, examples of db pools: <code>-db-pg-pool, -db-dynamodb-pool</code>, url format: <code>protocol://[user:password@]hostname[:port]/dbname</code> or <code>default</code></li>
<li><code>db-([a-z0-9]+)-pool-(disabled)$</code> - Disable the specified pool but keep the configuration</li>
<li><code>db-([a-z0-9]+)-pool-(max)$</code> - Max number of open connections for a pool, default is Infinity</li>
<li><code>db-([a-z0-9]+)-pool-(min)$</code> - Min number of open connections for a pool</li>
<li><code>db-([a-z0-9]+)-pool-(idle)$</code> - Number of ms for a db pool connection to be idle before being destroyed</li>
<li><code>db-([a-z0-9]+)-pool-(tables)$</code> - Tables to be created only in this pool, to prevent creating all tables in every pool</li>
<li><code>db-([a-z0-9]+)-pool-exclusive-tables$</code> - A DB pool tables, pattern of tables that belong to this pool only</li>
<li><code>db-([a-z0-9]+)-pool-connect$</code> - Connect options for a DB pool driver for new connection, driver specific</li>
<li><code>db-([a-z0-9]+)-pool-options-([a-zA-Z0-9_.-]+)$</code> - General options for a DB pool</li>
<li><code>db-([a-z0-9]+)-pool-(cache-columns)$</code> - Enable caching table columns for this pool if it supports it</li>
<li><code>db-([a-z0-9]+)-pool-(create-tables)$</code> - Create tables for this pool on startup</li>
<li><code>db-([a-z0-9]+)-pool-cache2-(.+)</code> - Level 2 cache TTL for the specified pool and table, data is JSON strings in the LRU cache</li>
<li><code>db-([a-z0-9]+)-pool-alias</code> - Pool alias to refer by an alternative name</li>
<li><code>msg-([^-]+)-key(@.+)?</code> - API private key for FCM/Webpush or similar services, if the suffix is specified in the config parameter will be used as the app name, without the suffix it is global</li>
<li><code>msg-([^-]+)-pubkey(@.+)?</code> - API public key for Webpush or similar services, if the suffix is specified in the config parameter will be used as the app name, without the suffix it is global</li>
<li><code>msg-([^-]+)-authkey-([^-]+)-(.+)</code> - A auth key for APN in p8 format, can be a file name with .p8 extension or a string with the key contents encoded with base64, the format is: -msg-apn-authkey-TEAMID-KEYID KEYDATA</li>
<li><code>msg-([^-]+)-sandbox(@.+)?</code> - Enable sandbox for a service, default is production mode</li>
<li><code>msg-([^-]+)-options-([^@]+)(@.+)?</code> - A config property to the specified agent, driver specific</li>
<li><code>msg-shutdown-timeout</code> - How long to wait for messages draining out in ms on shutdown before exiting</li>
<li><code>msg-app-default</code> - Default app id(app bundle id) to be used when no app_id is specified</li>
<li><code>msg-app-dependency@(.+)</code> - List of other apps that are considered in the same app family, sending to the primary app will also send to all dependent apps</li>
<li><code>msg-app-team-(.+)</code> - Regexp that identifies all app bundles for a team</li>
<li><code>server-workers</code> - Max number of processes to launch for Web servers, 0 means <code>NumberOfCPUs-1</code>, &lt; 0 means <code>NumberOfCPUs*abs(N)</code></li>
<li><code>server-crash-delay</code> - Delay between respawing the crashed process</li>
<li><code>server-restart-delay</code> - Delay between respawning the server after changes</li>
<li><code>server-no-restart</code> - Do not restart any processes terminated, for debugging crashes only</li>
<li><code>server-log-errors</code> - If true, log crash errors from child processes by the logger, otherwise write to the daemon err-file. The reason for this is that the logger puts everything into one line thus breaking formatting for stack traces.</li>
<li><code>server-process-name</code> - Path to the command to spawn by the monitor instead of node, for external processes guarded by this monitor</li>
<li><code>server-process-args</code> - Arguments for spawned processes, for passing v8 options or other flags in case of external processes</li>
<li><code>server-worker-args</code> - Node arguments for workers, job and web processes, for passing v8 options</li>
<li><code>api-images-url</code> - URL where images are stored, for cases of central image server(s), must be full URL with optional path</li>
<li><code>api-images-s3</code> - S3 bucket name where to store and retrieve images</li>
<li><code>api-images-raw</code> - Return raw urls for the images, requires images-url to be configured. The path will reflect the actual 2 level structure and account id in the image name</li>
<li><code>api-images-s3-options</code> - S3 options to sign images urls, may have expires:, key:, secret: properties</li>
<li><code>api-images-ext</code> - Default image extension to use when saving images</li>
<li><code>api-images-mod</code> - Images scaling module, sharp or default bkjs-wand</li>
<li><code>api-files-raw</code> - Return raw urls for the files, requires files-url to be configured. The path will reflect the actual 2 level structure and account id in the file name</li>
<li><code>api-files-url</code> - URL where files are stored, for cases of central file server(s), must be full URL with optional path</li>
<li><code>api-files-s3</code> - S3 bucket name where to store files uploaded with the File API</li>
<li><code>api-max-request-queue</code> - Max number of requests in the processing queue, if exceeds this value server returns too busy error</li>
<li><code>api-no-access-log</code> - Disable access logging in both file or syslog</li>
<li><code>api-access-log-file</code> - File for access logging</li>
<li><code>api-access-log-level</code> - Syslog level priority, default is local5.info, 21 * 8 + 6</li>
<li><code>api-access-log-fields</code> - Additional fields from the request or account to put in the access log, prefix defines where the field is lcoated: q: - query, h: - headers, a: - account otherwise from the request, Example: -api-log-fields h:Referer,a:name,q:action</li>
<li><code>api-salt</code> - Salt to be used for scrambling credentials or other hashing activities</li>
<li><code>api-qs-options-(.+)</code> - Options to pass to qs.parse: depth, arrayLimit, allowDots, comma, plainObjects, allowPrototypes, parseArrays</li>
<li><code>api-no-static</code> - Disable static files from /web folder, no .js or .html files will be served by the server</li>
<li><code>api-static-options-(.+)</code> - Options to pass to serve-static module: maxAge, dotfiles, etag, redirect, fallthrough, extensions, index, lastModified</li>
<li><code>api-vhost-path-([^/]+)</code> - Define a virtual host regexp to be matched against the hostname header to serve static content from a different root, a vhost path must be inside the web directory, if the regexp starts with !, that means negative match, example: api-vhost-path-test_dir=test.com$</li>
<li><code>api-no-vhost-path</code> - Add to the list of URL paths that should be served for all virtual hosts</li>
<li><code>api-templating</code> - Templating engine to use, see consolidate.js for supported engines, the &#39;consolidate&#39; package must be installed to use this</li>
<li><code>api-no-session</code> - Disable cookie session support, all requests must be signed for Web clients</li>
<li><code>api-session-age</code> - Session age in milliseconds, for cookie based authentication</li>
<li><code>api-session-domain-(.+)</code> - Cookie domain by Host: header, if not matched session is bound to the exact host only, example: -api-session-domain-site.com=site.com$</li>
<li><code>api-session-same-site</code> - Session SameSite option, for cookie based authentication</li>
<li><code>api-session-cache</code> - Cache name for session control</li>
<li><code>api-query-token-secret</code> - Name of the property to be used for encrypting tokens for pagination or other sensitive data, any property from bk_user can be used, if empty no secret is used, if not a valid property then it is used as the secret</li>
<li><code>api-app-header-name</code> - Name for the app name/version query parameter or header, it is can be used to tell the server about the application version</li>
<li><code>api-version-header-name</code> - Name for the access version query parameter or header, this is the core protocol version that can be sent to specify which core functionality a client expects</li>
<li><code>api-no-cache-files</code> - Set cache-control=no-cache header for matching static files</li>
<li><code>api-tz-header-name</code> - Name for the timezone offset header a client can send for time sensitive requests, the backend decides how to treat this offset</li>
<li><code>api-signature-header-name</code> - Name for the access signature query parameter, header and session cookie</li>
<li><code>api-lang-header-name</code> - Name for the language query parameter, header and session cookie, primary language for a client</li>
<li><code>api-signature-age</code> - Max age for request signature in milliseconds, how old the API signature can be to be considered valid, the &#39;expires&#39; field in the signature must be less than current time plus this age, this is to support time drifts</li>
<li><code>api-no-access-token</code> - Disable access tokens support</li>
<li><code>api-access-time-interval</code> - Intervals to refresh last access time for accounts, only updates the cache if <code>bk_user</code> is configured to be cached</li>
<li><code>api-access-token-name</code> - Name for the access token query parameter or header</li>
<li><code>api-access-token-secret</code> - A secret to be used for access token signatures, additional enryption on top of the signature to use for API access without signing requests, it is required for access tokens to be used</li>
<li><code>api-access-token-age</code> - Access tokens age in milliseconds, for API requests with access tokens only</li>
<li><code>api-disable-session</code> - Disable access to API endpoints for Web sessions, must be signed properly</li>
<li><code>api-disable-session-acl</code> - Combine regexps from the specified acls for the check explained by <code>-api-disable-session</code> parameter</li>
<li><code>api-allow-authenticated</code> - Add URLs which can be accessed by any authenticated user account, can be partial urls or Regexp, it is checked before any other account types, if matched then no account specific paths will be checked anymore(any of the allow-account-...)</li>
<li><code>api-allow-acl-authenticated</code> - Combine regexps from the specified acls for the check explained by <code>-api-allow-authenticated</code> parameter</li>
<li><code>api-allow-admin</code> - Add URLs which can be accessed by admin accounts only, can be partial urls or Regexp</li>
<li><code>api-allow-acl-admin</code> - Combine regexps from the specified acls for the check explained by <code>-api-allow-admin</code> parameter</li>
<li><code>api-allow-account-([a-z0-9_]+)</code> - Add URLs which can be accessed by specific account type, can be partial urls or Regexp</li>
<li><code>api-allow-acl-([a-z0-9_]+)</code> - Combine regexps from the specified acls for allow checks for the specified account type</li>
<li><code>api-only-account-([a-z0-9_,]+)</code> - Add URLs which can be accessed by specific account type only, can be partial urls or Regexp</li>
<li><code>api-only-acl-([a-z0-9_,]+)</code> - Combine regexps from the specified acls allowed for the specified account type only</li>
<li><code>api-deny-authenticated</code> - Add URLs which CAN NOT be accessed by any authenticated user account, can be partial urls or Regexp, it is checked before any other account types, if matched then no account specific paths will be checked anymore(any of the deny-account-...)</li>
<li><code>api-deny-acl-authenticated</code> - Combine regexps from the specified acls for the check explained by <code>-api-deny-authenticated</code> parameter</li>
<li><code>api-deny-account-([a-z0-9_]+)</code> - Add URLs which CAN NOT be accessed by specific account type, can be partial urls or Regexp, this is checked before any allow parameters</li>
<li><code>api-deny-acl-([a-z0-9_]+)</code> - Combine regexps from the specified acls for deny checks for the specified account type</li>
<li><code>api-acl-([a-z0-9_]+)</code> - Add URLs to the named ACL which can be used in allow/deny rules per account</li>
<li><code>api-allow</code> - Regexp for URLs that dont need credentials, replaces the whole access list</li>
<li><code>api-allow-path</code> - Add to the list of allowed URL paths without authentication, adds to the <code>-api-allow</code> parameter</li>
<li><code>api-allow-acl</code> - Combine regexps from the specified acls for the check explained by <code>-api-allow</code> parameter</li>
<li><code>api-deny</code> - Regexp for URLs that will be denied access, replaces the whole access list</li>
<li><code>api-deny-path</code> - Add to the list of URL paths to be denied without authentication, adds to the <code>-api-deny</code> parameter</li>
<li><code>api-deny-acl</code> - Combine regexps from the specified acls for the check explained by <code>-api-deny</code> parameter</li>
<li><code>api-allow-anonymous</code> - Add to the list of allowed URL paths that can be served with or without valid account, the difference with <code>-api-allow-path</code> is that it will check for signature and an account but will continue if no login is provided, return error in case of wrong account or not account found</li>
<li><code>api-allow-acl-anonymous</code> - Combine regexps from the specified acls for the check explained by <code>-allow-anonymous</code> parameter</li>
<li><code>api-allow-empty</code> - Regexp for URLs that should return empty responses if not found, for example return nothing for non-existent javascript files or css files</li>
<li><code>api-ignore-allow</code> - Regexp for URLs that should be ignored by the allow rules, the processing will continue</li>
<li><code>api-ignore-allow-path</code> - Add to the list of URL paths which should be ignored by the allow rules, in order to keep allow/deny rules simple, for example to keep some js files from open to all: -allow-path .js -ignore-allow-path /secure/</li>
<li><code>api-ignore-allow-acl</code> - Combine regexps from the specified acls for the check explained by <code>-ignore-allow-path</code> parameter</li>
<li><code>api-allow-ip</code> - Add to the list of regexps for IPs that dont need credentials. It is checked before endpoint access list</li>
<li><code>api-deny-ip</code> - Add to the list of regexps for IPs that will be denied access. It is checked before endpoint access list.</li>
<li><code>api-path-errmsg-(.+)</code> - Error message to return for the specified path for authentication failures</li>
<li><code>api-acl-errmsg-([a-z0-9_]+)</code> - Error message to return for the specified acl for authentication failures</li>
<li><code>api-allow-ssl</code> - Add to the list of allowed locations using HTTPs only, plain HTTP requests to these urls will be refused</li>
<li><code>api-ignore-ssl</code> - Allow plain HTTP from matched IP addresss or locations</li>
<li><code>api-redirect-ssl</code> - Add to the list of the locations to be redirected to the same path but using HTTPS protocol, for proxy mode the proxy server will perform redirects</li>
<li><code>api-express-options</code> - Set Express config options during initialization,example: <code>-api-express-options { &quot;trust proxy&quot;: 1, &quot;strict routing&quot;: true }</code></li>
<li><code>api-mime-body</code> - Collect full request body in the req.body property for the given MIME type in addition to json and form posts, this is for custom body processing</li>
<li><code>api-mime-ignore</code> - Ignore the body for the following MIME content types, request body will not be parsed at all</li>
<li><code>api-mime-map-(.+)</code> - File extension to MIME content type mapping, this is used by static-serve, example: -api-mime-map-mobileconfig application/x-apple-aspen-config</li>
<li><code>api-ignore-content-type</code> - Ignore the content type for the following endpoint paths, keep the body unparsed</li>
<li><code>api-platform-match</code> - An JSON object with list of regexps to match user-agent header for platform detection, example: { &#39;ios|iphone|ipad&#39;: &#39;ios&#39;, &#39;android&#39;: &#39;android&#39; }</li>
<li><code>api-cors-origin</code> - Origin header for CORS requests</li>
<li><code>api-cors-allow</code> - Enable CORS requests if a request host/path matches the given regexp</li>
<li><code>api-server-header</code> - Custom Server: header to return for all requests</li>
<li><code>api-error-message</code> - Default error message to return in case of exceptions</li>
<li><code>api-restart</code> - On address in use error condition restart the specified servers, this assumes an external monitor like monit to handle restarts</li>
<li><code>api-allow-error-code</code> - Error codes in exceptions to return in the response to the user, if not matched the error-message will be returned</li>
<li><code>api-rlimits-(rate|max|interval|ttl|ip)-(.+)</code> - Rate limiter parameters for Token Bucket algorithm. <code>ttl</code> is to expire cache entries, <code>ip</code> is to limit by IP address as well, ex. -api-rlimits-ip-ip=10, -api-rlimits-rate-/path=1</li>
<li><code>api-rlimits-queue-(.+)</code> - Queue to use for the given path</li>
<li><code>api-rlimits-(interval|ttl)</code> - Default rate limiter parameters, default interval is 1s, <code>ttl</code> is to expire old cache entries</li>
<li><code>api-rlimits-total</code> - Total number of servers used in the default rate limiter behind a load balancer, rates will be divided by this number so each server handles only a portion of the total rate limit</li>
<li><code>api-rlimits-message</code> - Message to show when any limits reached</li>
<li><code>api-exit-on-error</code> - Exit on uncaught exception in the route handler</li>
<li><code>api-upload-limit</code> - Max size for uploads, bytes</li>
<li><code>api-limiter-queue</code> - Name of an ipc queue for API rate limiting</li>
<li><code>api-errlog-limiter-max</code> - How many error messages to put in the log before throttling kicks in</li>
<li><code>api-errlog-limiter-interval</code> - Interval for error log limiter, max errors per this interval</li>
<li><code>api-errlog-limiter-ignore</code> - Do not show errors that match the regexp</li>
<li><code>api-proxy-reverse</code> - A Web server where to proxy requests not macthed by the url patterns or host header, in the form: <a href="http://host%5B:port%5D">http://host[:port]</a></li>
<li><code>api-proxy-url-(.+)</code> - URL regexp to be passed to other web server running behind, each parameter defines an url regexp and the destination in the value in the form <a href="http://host%5B:port%5D">http://host[:port]</a>, example: -api-proxy-url-^/api <a href="http://127.0.0.1:8080">http://127.0.0.1:8080</a></li>
<li><code>api-proxy-host-(.+)</code> - Virtual host mapping, to match any Host: header, each parameter defines a host name and the destination in the value in the form <a href="http://host%5B:port%5D">http://host[:port]</a>, example: -api-proxy-host-<a href="http://www.myhost.com">www.myhost.com</a> <a href="http://127.0.0.1:8080">http://127.0.0.1:8080</a></li>
<li><code>api-routing-(.+)</code> - Locations to be re-routed to other path, this is done inside the server at the beginning, only the path is replaced, same format and placeholders as in redirect-url, use ! in front of regexp to remove particular redirect from the list, example: -api-routing-^/account/get /acount/read</li>
<li><code>api-ignore-routing</code> - Ignore locations from the routing</li>
<li><code>api-auth-routing-(.+)</code> - URL path to be re-routed to other path after the authentication is successful, this is done inside the server, only the path is replaced, same format and placeholders as in redirect-url, example: -api-routing-auth-^/account/get /acount/read</li>
<li><code>api-redirect-url</code> - Add to the list a JSON object with property name defining a location regexp to be matched early against in order to redirect using the value of the property, if the regexp starts with !, that means it must be removed from the list, variables can be used for substitution: @HOST@, @PATH@, @URL@, @BASE@, @DIR@, @QUERY@, status code can be prepended to the location, example: { &#39;^[^/]+/path/$&#39;: &#39;/path2/index.html&#39;, &#39;.+/$&#39;: &#39;301:@PATH@/index.html&#39; } </li>
<li><code>api-login-redirect-(.+)</code> - Define a location where to redirect if no login is provided, same format and placeholders as in redirect-url, example: api-login-redirect-^/admin/=/login.html</li>
<li><code>api-auth-status</code> - Default authenticated status, if no auth rules matched but valid signature this is the status returned</li>
<li><code>api-auth-message:</code> - Default authenticated message to be returned the default auth status</li>
<li><code>api-reset-acl</code> - Reset all ACL related rules and permissions</li>
<li><code>api-response-headers</code> - An JSON object with list of regexps to match against the location and set response headers defined as a ist of pairs name, value..., api-response-headers={ &quot;^/&quot;: [&quot;x-frame-options&quot;,&quot;sameorigin&quot;,&quot;x-xss-protection&quot;,&quot;1; mode=block&quot;] }</li>
<li><code>api-cleanup-rules-(.+)</code> - Rules for the cleanupResult per table, ex. api-cleanup-rules-bk_user=email:0,phone:1</li>
<li><code>api-request-cleanup</code> - List of fields to explicitely cleanup on request end</li>
<li><code>auth-table</code> - Table to use for user accounts</li>
<li><code>auth-err-(.+)</code> - Error messages for various cases</li>
<li><code>auth-admin-roles</code> - List of special super admin roles</li>
<li><code>auth-sigversion</code> - Signature version for secrets</li>
<li><code>auth-hash</code> - Hashing method to use by default: bcrypt, argon2, none</li>
<li><code>auth-bcrypt</code> - Number of iterations for bcrypt</li>
<li><code>auth-argon2</code> - Argon2 parameteres, ex: type:2,memoryCost:1,hashLength:32</li>
<li><code>auth-max-length</code> - Max login and name length</li>
<li><code>jobs-workers</code> - How many worker processes to launch to process the job queue, -1 disables jobs, 0 means launch as many as the CPUs available</li>
<li><code>jobs-worker-cpu-factor</code> - A number to multiply the number of CPUs available to make the total number of workers to launch, only used if <code>workers</code> is 0</li>
<li><code>jobs-worker-args</code> - Node arguments for workers, for passing v8 jobspec, see <code>process</code></li>
<li><code>jobs-worker-env</code> - Environment to be passed to the worker via fork, see <code>cluster.fork</code></li>
<li><code>jobs-worker-delay</code> - Delay in milliseconds for a worker before it will start accepting jobs, for cases when other dependencies may take some time to start</li>
<li><code>jobs-max-runtime</code> - Max number of seconds a job can run before being killed</li>
<li><code>jobs-max-lifetime</code> - Max number of seconds a worker can live, after that amount of time it will exit once all the jobs are finished, 0 means indefinitely</li>
<li><code>jobs-shutdown-timeout</code> - Max number of milliseconds to wait for the graceful shutdown sequence to finish, after this timeout the process just exits</li>
<li><code>jobs-worker-queue</code> - Queue(s) to subscribe for workers, multiple queues can be processes at the same time, i.e. more than one job can run from different queues</li>
<li><code>jobs-worker-options-(.+)</code> - Custom parameters by queue name, passed to <code>ipc.subscribeQueue</code> on worker start, useful with channels, ex: <code>-jobs-worker-options-nats#events {&quot;count&quot;:10}</code></li>
<li><code>jobs-cron-queue</code> - Default queue to use for cron jobs</li>
<li><code>jobs-global-queue</code> - Default queue for all jobs, the queueName is ignored</li>
<li><code>jobs-global-ignore</code> - Queue names which ignore the global setting, the queueName is used as usual</li>
<li><code>jobs-cron</code> - Allow cron jobs to be executed from the local etc/crontab file or via config parameter</li>
<li><code>jobs-schedule</code> - Cron jobs to be scheduled, the JSON must be in the same format as crontab file</li>
<li><code>jobs-unique-queue</code> - Default queue name to use for keeping track of unique jobs</li>
<li><code>jobs-unique-ignore</code> - Ignore all unique parameters if a job&#39;s uniqueKey matches</li>
<li><code>events-worker-queue-(.+)</code> - Queues to subscribe for workers, same queues can be used at the same time with different functions and channels, ex: -ipc-worker-queue-ticket ticket.processEvents</li>
<li><code>events-worker-options-(.+)</code> - Custom parameters by queue name, passed to <code>ipc.subscribeQueue</code> on worker start, useful with channels, ex: <code>-events-worker-options-ticket {&quot;count&quot;:3,&quot;raw&quot;:1}</code></li>
<li><code>events-max-runtime</code> - Max number of seconds an event processing can run before being killed</li>
<li><code>events-routing-(.+)</code> - Queue routing by event topic</li>
<li><code>events-properties</code> - List of properties to copy into an event envelope from the provided options</li>
</ul>
<h2 id="module-api">Module: API</h2>
<ul>
<li><p><code>api.init(options, callback)</code></p>
<p> Initialize API layer, this must be called before the <code>api</code> module can be used but it is called by the server module automatically so <code>api.init</code> is
rearely need to called directly, only for new server implementation or if using in the shell for testing.</p>
</li>
</ul>
<p> During the init sequence, this function calls <code>api.initMiddleware</code> and <code>api.initApplication</code> methods which by default are empty but can be redefined in the user aplications.</p>
<p> The bkjs.js uses its own request parser that places query parameters into <code>req.query</code> or <code>req.body</code> depending on the method.</p>
<p> For GET method, <code>req.query</code> contains all url-encoded parameters, for POST method <code>req.body</code> contains url-encoded parameters or parsed JSON payload or multipart payload.</p>
<p> The reason not to do this by default is that this may not be the alwayse wanted case and distinguishing data coming in the request or in the body may be desirable,
 also, this will needed only for Express handlers <code>.all</code>, when registering handler by method like <code>.get</code> or <code>.post</code> then the handler needs to deal with only either source of the request data.</p>

<ul>
<li><p><code>api.shutdown(callback)</code></p>
<p> Gracefully close all connections, call the callback after that</p>
</li>
</ul>

<ul>
<li><p><code>api.shutdownWeb(options, callback)</code></p>
<p> Gracefully close all database pools when the shutdown is initiated by a Web process</p>
</li>
</ul>

<ul>
<li><p><code>api.configureStatic()</code></p>
<p> Templating and static paths</p>
</li>
</ul>

<ul>
<li><p><code>api.configureAccessLog()</code></p>
<p> Setup access log stream</p>
</li>
</ul>

<ul>
<li><p><code>api.handleServerRequest(req, res)</code></p>
<p> Start Express middleware processing wrapped in the node domain</p>
</li>
</ul>

<ul>
<li><p><code>api.prepareRequest(req)</code></p>
<p> Prepare request options that the API routes will merge with, can be used by pre process hooks, initialize
required properties for subsequent use</p>
</li>
</ul>

<ul>
<li><p><code>api.prepareOptions(req)</code></p>
<p> Parse or re-parse special headers about app version, language and timezone, it is called early to parse headers first and then
right after the query parameters are available, query values have higher priority than headers.</p>
</li>
</ul>

<ul>
<li><p><code>api.startMetrics(req, res, next)</code></p>
<p> This is supposed to be called at the beginning of request processing to start metrics and install the handler which
will be called at the end to finalize the metrics and call the cleanup handlers</p>
</li>
</ul>

<ul>
<li><p><code>api.handleMetrics(req, elapsed)</code></p>
<p> Finish metrics collection about the current rquest</p>
</li>
</ul>

<ul>
<li><p><code>api.handleCleanup(req)</code></p>
<p> Call registered cleanup hooks and clear the request explicitly</p>
</li>
</ul>

<ul>
<li><p><code>api.checkQuery(req, res, next)</code></p>
<p> Parse incoming query parameters</p>
</li>
</ul>

<ul>
<li><p><code>api.checkBody(req, res, next)</code></p>
<p> Parse multipart forms for uploaded files</p>
</li>
</ul>

<ul>
<li><p><code>api.checkRouting(req, name, ignore)</code></p>
<p> Check if the current request must be re-routed to another endpoint</p>
</li>
</ul>

<ul>
<li><p><code>api.checkRedirectPlaceholders(req, pathname)</code></p>
<p> Replace redirect placeholders</p>
</li>
</ul>

<ul>
<li><p><code>api.checkRedirect(req, options)</code></p>
<p> Check a request for possible redirection condition based on the configuration, this can be SSL checks or
defined redirect rules. This is used by API servers and proxy servers for early redirections. It returns null
if no redirects or errors happend, otherwise an object with status that is expected by the <code>api.sendStatus</code> method.
The options is expected to contain the following cached request properties:</p>
</li>
</ul>
<ul>
<li>path - from req.path or the request pathname only</li>
<li>host - from req.hostname or the hostname part only</li>
<li>port - port from the host: header if specified</li>
<li>secure - if the protocol is https</li>
</ul>

<ul>
<li><p><code>api.checkRedirectRules(req, options, name)</code></p>
<p> Redirect rules, supports regexpobj and regexpmap parameters</p>
</li>
</ul>

<ul>
<li><p><code>api.checkRateLimits(req, options, callback)</code></p>
<p> Perform rate limiting by specified property, if not given no limiting is done.</p>
</li>
</ul>
<p> The following options properties can be used:</p>
<ul>
<li><p>type - predefined: <code>ip,  path, opath</code>, determines by which property to perform rate limiting, when using account properties
 the rate limiter should be called after the request signature has been parsed. Any other value is treated as
 custom type and used as is. If it is an array all items will be checked sequentially.
 <strong>This property is required.</strong></p>
<p> The predefined types checked for every request:</p>
<ul>
<li><p>ip - check every IP address</p>
</li>
<li><p>opath - same as path but uses original path before routing</p>
</li>
<li><p>path - limit number of requests for an API path, * can be used at the end to match only the beginning</p>
<p>  -api-rlimits-rate-ip=100
  -api-rlimits-rate-/api/path=2
  -api-rlimits-ip-/api/path=1
  -api-rlimits-rate-/api/path/*=1</p>
</li>
</ul>
</li>
<li><p>ip - to use the specified IP address</p>
</li>
<li><p>max - max capacity to be used by default</p>
</li>
<li><p>rate - fill rate to be used by default</p>
</li>
<li><p>interval - interval in ms within which the rate is measured, default 1000 ms</p>
</li>
<li><p>message - more descriptive text to be used in the error message for the type, if not specified a generic error message is used</p>
</li>
<li><p>total - apply this factor to the rate, it is used in case of multiple servers behind a loadbalancer, so for
 total 3 servers in the cluster the factor will be 3, i.e. each individual server checks for a third of the total request rate</p>
</li>
<li><p>queue - which queue to use instead of the default, some limits is more useful with global queues like Redis instead of the default</p>
</li>
</ul>
<p> The metrics are kept in the LRU cache in the master process by default.</p>
<p> Example:</p>
<pre><code>   api.checkRateLimits(req, { type: &quot;ip&quot;, rate: 100, interval: 60000 }, (err, info) =&gt; {
      if (err) return api.sendReply(err);
      ...
   });
</code></pre>

<ul>
<li><p><code>api.sendJSON(req, err, rows)</code></p>
<p> Send result back with possibly executing post-process callback, this is used by all API handlers to allow custom post processing in the apps.
If err is not null the error message is returned immediately.</p>
</li>
</ul>

<ul>
<li><p><code>api.sendFormatted(req, err, data, options)</code></p>
<p> Send result back formatting according to the options properties:</p>
<ul>
<li>format - json, csv, xml, JSON is default</li>
<li>separator - a separator to use for CSV and other formats</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>api.sendStatus(res, options)</code></p>
<p> Return reply to the client using the options object, it contains the following properties:</p>
</li>
</ul>
<ul>
<li>status - defines the respone status code</li>
<li>message  - property to be sent as status line and in the body</li>
<li>type - defines Content-Type header, the message will be sent in the body</li>
<li>url - for redirects when status is 301, 302...</li>
</ul>
<p> <strong>i18n Note:</strong></p>
<p> The API server attaches fake i18n functions <code>req.__</code> and <code>res.__</code> which are used automatically for the <code>message</code> property
 before sending the response.</p>
<p> With real i18n module these can/will be replaced performing actual translation without
 using <code>i18n.__</code> method for messages explicitely in the application code for <code>sendStatus</code> or <code>sendReply</code> methods.</p>

<ul>
<li><p><code>api.sendReply(res, status, text)</code></p>
<p> Send formatted JSON reply to an API client, if status is an instance of Error then error message with status 500 is sent back.</p>
</li>
</ul>
<p> If the status is an object it is sent as is.</p>
<p> All Error objects will return a generic error message without exposing the real error message, it will log all error exceptions in the logger
 subject to log throttling configuration.</p>

<ul>
<li><p><code>api.sendFile(req, file, redirect)</code></p>
<p> Send file back to the client, res is Express response object</p>
</li>
</ul>

<h2 id="module-app">Module: APP</h2>
<ul>
<li><p><code>app.configure(options, callback) </code></p>
<p> Called after all config files are loaded and command line args are parsed, home directory is set but before the db is initialized,
the primary purpose of this early call is to setup environment before connecting to the database. This is called regardless of the server
to be started and intended to initialize the common environment before the database and other subsystems are initialized.</p>
</li>
</ul>

<ul>
<li><p><code>app.configureModule(options, callback) </code></p>
<p> Called after the core.init has been initialized successfully, this can be redefined in the applications to add additional
init steps that all processes require to have. All database pools and other confugration is ready at this point. This hook is
called regardless of what kind of server is about to start, it is always called before starting a server or shell.</p>
</li>
</ul>

<ul>
<li><p><code>app.configureMiddleware(options, callback) </code></p>
<p> This handler is called during the Express server initialization just after the security middleware.</p>
</li>
</ul>
<p> NOTE: <code>api.app</code> refers to the Express instance.</p>

<ul>
<li><p><code>app.configureWeb(options, callback) </code></p>
<p> This handler is called after the Express server has been setup and all default API endpoints initialized but the Web server
is not ready for incoming requests yet. This handler can setup additional API endpoints, add/modify table descriptions.</p>
</li>
</ul>
<p> NOTE: <code>api.app</code> refers to the Express instance</p>

<ul>
<li><p><code>app.shutdownWeb(options, callback) </code></p>
<p> Perform shutdown sequence when a Web process is about to exit</p>
</li>
</ul>
<p> NOTE: <code>api.app</code> refers to the Express instance</p>

<ul>
<li><p><code>app.configureMaster(options, callback) </code></p>
<p> This handler is called during the master server startup, this is the process that monitors the worker jobs and performs jobs scheduling</p>
</li>
</ul>

<ul>
<li><p><code>app.configureServer(options, callback) </code></p>
<p> This handler is called during the Web server startup, this is the master process that creates Web workers for handling Web requests, this process
interacts with the Web workers via IPC sockets between processes and relaunches them if any Web worker dies.</p>
</li>
</ul>

<ul>
<li><p><code>app.configureWorker(options, callback) </code></p>
<p> This handler is called on job worker instance startup after the tables are intialized and it is ready to process the job</p>
</li>
</ul>

<ul>
<li><p><code>app.shutdownWorker(options, callback) </code></p>
<p> Perform last minute operations inside a worker process before exit, the callback must be called eventually which will exit the process.
This method can be overrided to implement custom worker shutdown procedure in order to finish pending tasks like network calls.</p>
</li>
</ul>

<ul>
<li><p><code>app.configureMonitor(options, callback) </code></p>
<p> This callback is called when the monitor process is ready, there is no any other code is supposed to run inside the monitor, but
in case it is needed, this is the hook to be used.</p>
</li>
</ul>

<ul>
<li><p><code>app.configureShell(options, callback) </code></p>
<p> This callback is called by the shell process to setup additional command or to execute a command which is not
supported by the standard shell. Setting options.done to 1 will stop the shell, this is a signal that command has already
been processed.</p>
</li>
</ul>

<h2 id="module-auth">Module: AUTH</h2>
<ul>
<li><p><code>mod.prepareSecret(query, options, callback)</code></p>
<p> If specified in the options, prepare credentials to be stored in the db, if no error occurred return null, otherwise an error object</p>
<ul>
<li>scramble is used to encrypt the secret with login as HMAC_SHA256 so the db never stores cleartext credentials</li>
<li>hash - use bcrypt or argon2 explicitely, otherwise use the config</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>mod.checkSecret(user, password, callback)</code></p>
<p> Verify an existing user record with given password,</p>
<ul>
<li>user - if a string it is a hashed secret from an existing user record, otherwise must be an user object</li>
<li>password - plain text password or other secret passed to be verified</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>mod.get(query, options, callback)</code></p>
<p> Returns an account record by login or id, to make use of a cache add to the config <code>db-cache-keys-bk_user-id=id</code></p>
</li>
</ul>

<ul>
<li><p><code>mod.add(query, options, callback)</code></p>
<p> Registers a new account, returns new record in the callback, when <code>options.isInternal</code> is true then allow to set all properties
otherwise internal properties will not be added</p>
</li>
</ul>

<ul>
<li><p><code>mod.update(query, options, callback)</code></p>
<p> Updates an existing account by login or id, if <code>options.isInternal</code> is true then allow to update all properties, returns a new record in the callback</p>
</li>
</ul>

<ul>
<li><p><code>mod.del(query, options, callback)</code></p>
<p> Deletes an existing account by login or id, no admin checks, returns the old record in the callback</p>
</li>
</ul>

<h2 id="module-aws">Module: AWS</h2>
<ul>
<li><p><code>aws.configure(options, callback)</code></p>
<p> Initialization of metadata</p>
</li>
</ul>

<ul>
<li><p><code>aws.configureServer(options, callback)</code></p>
<p> Execute on Web server startup</p>
</li>
</ul>

<ul>
<li><p><code>aws.configureMaster(options, callback)</code></p>
<p> Execute on master server startup</p>
</li>
</ul>

<ul>
<li><p><code>aws.readCredentials(profile, callback)</code></p>
<p> Read key and secret from the AWS SDK credentials file, if no profile is given in the config or command line only the default peofile
will be loaded.</p>
</li>
</ul>

<ul>
<li><p><code>aws.getInstanceMeta(path, callback)</code></p>
<p> Retrieve instance meta data</p>
</li>
</ul>

<ul>
<li><p><code>aws.getInstanceCredentials(path, callback)</code></p>
<p> Retrieve instance credentials using EC2 instance profile and setup for AWS access</p>
</li>
</ul>

<ul>
<li><p><code>aws.getInstanceInfo(options, callback)</code></p>
<p> Retrieve instance launch index from the meta data if running on AWS instance</p>
</li>
</ul>

<ul>
<li><p><code>aws.parseXMLResponse(err, params, options, callback)</code></p>
<p> Parse AWS response and try to extract error code and message, convert XML into an object.</p>
</li>
</ul>

<ul>
<li><p><code>aws.querySign(region, service, host, method, path, body, headers, credentials, options)</code></p>
<p> Build version 4 signature headers</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryPrepare(action, version, obj, options)</code></p>
<p> Return a request object ready to be sent to AWS, properly formatted</p>
</li>
</ul>

<ul>
<li><p><code>aws.querySigner()</code></p>
<p> It is called in the context of a http request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryAWS(region, service, proto, host, path, obj, options, callback)</code></p>
<p> Make AWS request, return parsed response as Javascript object or null in case of error</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryEndpoint(service, version, action, obj, options, callback)</code></p>
<p> AWS generic query interface</p>
</li>
</ul>

<ul>
<li><p><code>aws.getServiceRegion(service, region)</code></p>
<p> Check for supported regions per service, return the first one if the given region is not supported</p>
</li>
</ul>

<ul>
<li><p><code>aws.copyCredentials(obj, options)</code></p>
<p> Copy all credentials properties from the options into the obj</p>
</li>
</ul>

<ul>
<li><p><code>aws.querySQS(action, obj, options, callback)</code></p>
<p> AWS SQS API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryIAM(action, obj, options, callback)</code></p>
<p> AWS AIM API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.querySTS(action, obj, options, callback)</code></p>
<p> AWS STS API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.querySES(action, obj, options, callback)</code></p>
<p> AWS SES API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryCFN(action, obj, options, callback)</code></p>
<p> AWS CFN API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryElastiCache(action, obj, options, callback)</code></p>
<p> AWS Elastic Cache API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryAS(action, obj, options, callback)</code></p>
<p> AWS Autoscaling API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryRoute53(method, path, data, options, callback)</code></p>
<p> Make a request to Route53 service</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryRekognition(action, obj, options, callback)</code></p>
<p> Make a request to the Rekognition service</p>
</li>
</ul>

<ul>
<li><p><code>aws.querySSM(action, obj, options, callback)</code></p>
<p> AWS SSM API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryACM(action, obj, options, callback)</code></p>
<p> AWS ACM API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryComprehend(action, obj, options, callback)</code></p>
<p> AWS Comprehend API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.queryTranscribe(action, obj, options, callback)</code></p>
<p> AWS Transcribe API request</p>
</li>
</ul>

<ul>
<li><p><code>aws.getTagValue(obj, key)</code></p>
<p> Returns a tag value by key, default key is Name</p>
</li>
</ul>

<ul>
<li><p><code>aws.stsAssumeRole(options, callback)</code></p>
<p> Assume a role and return new credentials that can be used in other API calls</p>
</li>
</ul>

<ul>
<li><p><code>aws.sqsReceiveMessage(url, options, callback)</code></p>
<p> Receive message(s) from the SQS queue, the callback will receive a list with messages if no error.
The following options can be specified:</p>
<ul>
<li>count - how many messages to receive</li>
<li>timeout - how long to wait, in milliseconds, this is for Long Poll</li>
<li>visibilityTimeout - the duration (in milliseconds) that the received messages are hidden from subsequent retrieve requests</li>
<li>attempt - request attempt id for FIFO queues
after being retrieved by a ReceiveMessage request.</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.sqsSendMessage(url, body, options, callback)</code></p>
<p> Send a message to the SQS queue.
The options can specify the following:</p>
<ul>
<li>delay - how long to delay this message in milliseconds</li>
<li>group - a group id for FIFO queues</li>
<li>unique - deduplication id for FIFO queues</li>
<li>attrs - an object with additional message attributes to send, use only string, numbers or binary values,
all other types will be converted into strings</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.sesSendEmail(to, subject, body, options, callback)</code></p>
<p> Send an email via SES
The following options supported:</p>
<ul>
<li>from - an email to use in the From: header</li>
<li>cc - list of email to use in CC: header</li>
<li>bcc - list of emails to use in Bcc: header</li>
<li>replyTo - list of emails to ue in ReplyTo: header</li>
<li>returnPath - email where to send bounces</li>
<li>charset - charset to use, default is UTF-8</li>
<li>html - if set the body is sent as MIME HTML</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.sesSendRawEmail(body, options, callback)</code></p>
<p> Send raw email
The following options accepted:</p>
<ul>
<li>to - list of email addresses to use in RCPT TO</li>
<li>from - an email to use in from header</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>aws.route53Change(names, options, callback)</code></p>
<p> Create or update a host in the Route53 database.</p>
</li>
</ul>
<ul>
<li><code>names</code> is a host name to be set with the current IP address or a list with objects in the format<pre><code>[ { name: &quot;..&quot;, value: &quot;1.1.1.1&quot;, type: &quot;A&quot;, ttl: 300 } ...]
</code></pre>
</li>
</ul>
<p> The <code>options</code> may contain the following:</p>
<ul>
<li>type - default record type, A</li>
<li>ttl - default TTL, 300 seconds</li>
<li>op - an operation, default is UPSERT</li>
</ul>

<ul>
<li><p><code>aws.detectLabels(name, options, callback)</code></p>
<p> Detect image featires using AWS Rekognition service, the <code>name</code> can be a Buffer, a local file or an url to the S3 bucket. In the latter case
the url can be just apath to the file inside a bucket if <code>options.bucket</code> is specified, otherwise it must be a public S3 url with the bucket name
to be the first part of the host name. For CDN/CloudFront cases use the <code>option.bucket</code> option.</p>
</li>
</ul>

<h2 id="module-core">Module: CORE</h2>
<ul>
<li><p><code>core.init(options, callback)</code></p>
<p> Main initialization, must be called prior to perform any actions.</p>
</li>
</ul>
<p> If options are given they may contain the following properties:</p>
<ul>
<li>noDb - if true do not initialize database</li>
<li>noConfigure - do not run all configure methods</li>
<li>noDns - do not retrieve config from DNS</li>
<li>noWatch - do not watch and reload config files</li>
<li>noModules - do not load modules</li>
<li>noLocales - do not load locales</li>
<li>preloadModules - list of modules to load first</li>
</ul>

<ul>
<li><p><code>core.run(options, callback)</code></p>
<p> Run any backend function after environment has been initialized, this is to be used in shell scripts,
core.init will parse all command line arguments, the simplest case to run from /data directory and it will use
default environment or pass -home dir so the script will reuse same config and paths as the server
context can be specified for the callback, if no then it run in the core context</p>
</li>
</ul>
<ul>
<li>require(&#39;backendjs&#39;).run(function() {}) is one example where this call is used as a shortcut for ad-hoc scripting</li>
</ul>

<ul>
<li><p><code>core.exit(code, msg)</code></p>
<p> Exit the process with possible message to be displayed and status code</p>
</li>
</ul>

<ul>
<li><p><code>core.setHome(home)</code></p>
<p> Switch to new home directory, exit if we cannot, this is important for relative paths to work if used,
no need to do this in worker because we already switched to home directory in the master and all child processes
inherit current directory
Important note: If run with combined server or as a daemon then this MUST be an absolute path, otherwise calling
it in the spawned web master will fail due to the fact that we already set the home and relative path will not work after that.</p>
</li>
</ul>

<ul>
<li><p><code>core.parseConfig(data, pass, file)</code></p>
<p> Parse config lines for the file or other place</p>
</li>
</ul>

<ul>
<li><p><code>core.loadConfig(file, callback)</code></p>
<p> Parse the config file, configFile can point to a file or can be skipped and the default file will be loaded</p>
</li>
</ul>

<ul>
<li><p><code>core.reloadConfig(callback)</code></p>
<p> Reload all config files</p>
</li>
</ul>

<ul>
<li><p><code>core.loadDnsConfig(options, callback)</code></p>
<p> Load configuration from the DNS TXT records</p>
</li>
</ul>

<ul>
<li><p><code>core.loadLocales(options, callback)</code></p>
<p> Load configured locales</p>
</li>
</ul>

<ul>
<li><p><code>core.runMethods(name, params, options, callback)</code></p>
<p> Run a method for every module, a method must conform to the following signature: <code>function(options, callback)</code> and
call the callback when finished. The callback second argument will be the parameters passed to each method, the options if provided can
specify the conditions or parameters which wil be used by the `runMethods`` only.</p>
</li>
</ul>
<p> The following properties can be specified in the options or params:</p>
<ul>
<li>allow - regexp with allowed modules, in options only</li>
<li>allowModules - a regexp of the modules names to be called only</li>
<li>stopOnError - on first error stop and return, otherwise all errors are ignored and all modules are processed</li>
<li>stopFilter - a function to be called after each pass to check if the processing must be stopped, it must return true to stop</li>
<li>logger_error - logger level, if not specified an error with status 200 will be reported with log level &#39;info&#39; and other errors with level &#39;error&#39;</li>
<li>logger_inspect - an object with inspect options to pverride current inspect parameters</li>
<li>logger_allow - a list of properties allowed in the log on error, this is to prevent logging too much or sensitive data</li>
<li>parallel - if true run methods for all modules in parallel using lib.forEach</li>
<li>concurrency - if a number greater than 1 run that many methods in parallel using lib.forEachLimit</li>
<li>sync - if true treat methods as simple functions without callbacks, methods MUST NOT call the second callback argument but simply return</li>
<li>direct - if true call all methods directly othwerwise via setImmediate</li>
</ul>

<ul>
<li><p><code>core.addModule(...args)</code></p>
<p> Adds reference to the objects in the core for further access, specify module name, module reference pairs.
This is used the the core itcore to register all internal modules and makes it available in the shell and in the <code>core.modules</code> object.</p>
</li>
</ul>
<p> Also this is used when creating modular backend application by separating the logic into different modules, by registering such
 modules with the core it makes the module a first class citizen in the backendjs core and exposes all the callbacks and methods.</p>
<p> For example, the module below will register API routes and some methods</p>
<pre><code>   const bkjs = require(&quot;backendjs&quot;);
   const mymod = { name: &quot;mymod&quot; }
   exports.module = mymod;
   core.addModule(mymod);

   mymod.configureWeb = function(options, callback) {
      bkjs.api.app.all(&quot;/mymod&quot;, function(req, res) {
           res.json({});
      });
   }
</code></pre>
<p> In the main app.js just load it and the rest will be done automatically, i.e. routes will be created ...</p>
<pre><code>   const mymod = require(&quot;./mymod.js&quot;);
</code></pre>
<p> Running the shell will make the object <code>mymod</code> available</p>
<pre><code>   ./app.sh -shell
   &gt; mymod
     { name: &quot;mymod&quot; }
</code></pre>

<ul>
<li><p><code>core.loadModules(dir, options, callback)</code></p>
<p> Dynamically load services from the specified directory.</p>
</li>
</ul>
<p> The modules are loaded using <code>require</code> as a normal nodejs module but in addition if the module exports
 <code>init</code> method it is called immediately with options passed as an argument. This is a synchronous function so it is supposed to be
 called on startup, not dynamically during a request processing. Only top level .js files are loaded, not subdirectories. <code>core.addModule</code> is called
 automatically.</p>
<p> Each module is put in the global <code>core.modules`` object by name, the name can be a property </code>name` or the module base file name.</p>
<p> Modules can be sorted by a priority, if .priority property is defined in the module it will be used to sort the modules, the higher priority the
 closer to the top the module will be. The position of a module in the <code>core.modules</code> will define the order <code>runMethods</code> will call.</p>
<p> <strong>Caution must be taken for module naming, it is possible to override any default bkjs module which will result in unexpected behaviour</strong></p>
<p>  Example, to load all modules from the local relative directory</p>
<pre><code>   core.loadModules(&quot;modules&quot;)
</code></pre>

<ul>
<li><p><code>core.httpGet(uri, params, callback)</code></p>
<p> Make a HTTP request, see <code>httpGet</code> module for more details.</p>
</li>
</ul>

<ul>
<li><p><code>core.sendRequest(options, callback)</code></p>
<p> Make a HTTP request using <code>httpGet</code> with ability to sign requests.</p>
</li>
</ul>
<p> The POST request is made, if data is an object, it is converted into string.</p>
<p> Returns params as in <code>httpGet</code> with .json property assigned with an object from parsed JSON response.</p>
<p> <em>When used with API endpoints, the <code>backend-host</code> parameter must be set in the config or command line to the base URL of the backend,
 like <a href="http://localhost:8000">http://localhost:8000</a>, this is when <code>uri</code> is relative URL. Absolute URLs do not need this parameter.</em></p>
<p> Special parameters for options:</p>
<ul>
<li>url - url if options is first argument</li>
<li>login - login to use for access credentials instead of global credentials</li>
<li>secret - secret to use for access instead of global credentials</li>
<li>checksum - calculate checksum from the data</li>
<li>obj - return just the result object, not the whole params</li>
</ul>

<h2 id="module-db">Module: DB</h2>
<ul>
<li><p><code>Database tables</code></p>
<pre><code>      // Configuration store, same parameters as in the commandline or config file, can be placed in separate config groups
      // to be used by different backends or workers
      bk_config: {
          name: { primary: 1 },            // name of the parameter
          type: { primary: 1 },            // config type or tag
          value: { type: &quot;text&quot; },         // the value
          status: { value: &quot;ok&quot; },         // ok - availaible
          ttl: { type: &quot;int&quot; },            // refresh interval in seconds since last read
          mtime: { type: &quot;now&quot; }
      },

      // General purpose properties, can be used to store arbitrary values
      bk_property: {
          name: { primary: 1 },
          value: {},
          count: { type: &quot;counter&quot; },      // general purpose counter value
          ttl: { type: &quot;int&quot; },            // time to live, seconds since last update
          mtime: { type: &quot;now&quot; }
      },
</code></pre>
</li>
</ul>

<ul>
<li><p><code>createPool:(opts) </code></p>
<p> None database driver</p>
</li>
</ul>

<ul>
<li><p><code>db.init(options, callback)</code></p>
<p> Initialize all database pools. the options may containt the following properties:</p>
<ul>
<li>createTables - if true then create new tables or upgrade tables with new columns</li>
<li>localTables - if true only enable local, default and config pools</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.initConfig(options, callback)</code></p>
<p> Load configuration from the config database, must be configured with <code>db-config-type</code> pointing to the database pool where bk_config table contains
configuration parameters.</p>
</li>
</ul>
<p> The priority of the paramaters is fixed and goes from the most broad to the most specific, most specific always wins, this allows
 for very flexible configuration policies defined by the app or place where instances running and separated by the run mode.</p>
<p> The following list of properties will be queried from the config database and the sorting order is very important, the last values
 will override values received for the earlier properties, for example, if two properties defined in the <code>bk_config</code> table with the
 types <code>myapp</code> and <code>prod-myapp</code>, then the last value will be used only.</p>
<p> The major elements are the following:</p>
<ul>
<li>the run mode specified in the command line <code>-run-mode: production</code></li>
<li>the application name from the package.json: <code>myapp</code></li>
<li>the process role: <code>-worker</code></li>
<li>the instance tag, AWS name tag or other name: <code>-nat</code></li>
</ul>
<p> The modifiers which are appended to each major attributes:</p>
<ul>
<li>the network where the instance is running, first 2 octets from the current IP address: <code>-192.168</code></li>
<li>the region where the instance is running, AWS region or other: <code>us-east-1</code></li>
</ul>
<p> The top level list is the following:</p>
<ul>
<li>runMode</li>
<li>appName</li>
<li>runMode-appName</li>
<li>runMode-role</li>
<li>runMode-tag</li>
<li>runMode-tag-role</li>
<li>runMode-appName-role</li>
<li>runMode-appName-tag</li>
</ul>
<p> All modifiers are appended for every item in the list like <code>runMode-network</code>, <code>runMode-appName-tag-region</code>,...</p>
<p> The options takes the following properties:</p>
<ul>
<li>force - if true then force to refresh and reopen all db pools</li>
<li>delta - if true then pull only records updated since the last config pull using the max mtime from received records.</li>
<li>table - a table where to read the config parameters, default is bk_config</li>
</ul>
<p> <strong>NOTE: The config parameters from the DB always take precedence even over config.local.</strong></p>
<p> On return, the callback second argument will receive all parameters received form the database as a list: -name value ...</p>

<ul>
<li><p><code>db.getConfig(options, callback)</code></p>
<p> Return all config records for the given instance, the result will be sorted most relevant at the top</p>
</li>
</ul>

<ul>
<li><p><code>db.refreshConfig(options, callback)</code></p>
<p> Refresh parameters which are configured with a TTL</p>
</li>
</ul>

<ul>
<li><p><code>db.dropTables(tables, options, callback)</code></p>
<p> Delete all specified tables from the specific pool or all active pools if <code>options.pool</code> is empty, <code>tables</code> can be a list of tables or an
object with table definitions</p>
</li>
</ul>

<ul>
<li><p><code>db.query(req, options, callback)</code></p>
<p> Execute query using native database driver, the query is passed directly to the driver.</p>
</li>
</ul>
<ul>
<li><p>req - an object with the following properties:</p>
<ul>
<li>text - SQL statement or other query in the format of the native driver, can be a list of statements</li>
<li>values - parameter values for SQL bindings or other driver specific data</li>
<li>op - operations to be performed, used by non-SQL drivers</li>
<li>obj - actual object with data for non-SQL drivers</li>
<li>table - table name for the operation</li>
</ul>
</li>
<li><p>options may have the following properties:</p>
<ul>
<li>pool - name of the database pool where to execute this query.
The difference with the high level functions that take a table name as their firt argument, this function must use pool
explicitely if it is different from the default. Other functions can resolve
the pool by table name if some tables are assigned to any specific pool by configuration parameters <code>db-pool-tables</code>.</li>
<li>unique - perform sorting the result and eliminate any duplicate rows by the column name specified in the <code>unique</code> property</li>
<li>filterrows - function to filter rows not to be included in the result, returns a new result set, args are: function(req, rows)</li>
<li>processrows - function to process rows in the result, returns a new result, args are: function(req, rows), this result will be put in cache
if requested so this may be used for preparing cached results, it must return an array</li>
<li>processasync - function to process result rows via async callback, return a new result in the callback, the function is: function(req, rows, callback),
the callback is function(err, rows)</li>
<li>syncMode - skip columns preprocessing and dynamic values for pool sync and backup restore</li>
<li>quiet - report errors in debug level</li>
<li>first - return the first row from the result</li>
<li>logger_db - log results at the end with this level or debug by default</li>
<li>logger_error - a log level to report about the errors, default is &#39;error&#39;, if an object it can specify different log levels by err.code, * is default level for not matched codes</li>
<li>ignore_error - clear errors occurred as it never happen, do not report in the log, if an array then only matched codes will be cleared</li>
<li>noprocessrows - if true then skip post processing result rows, return the data as is, this will result in returning combined columns as it is</li>
<li>noconvertrows - if true skip converting the data from the database format into Javascript data types, it uses column definitions</li>
<li>nopreparerow - if true skip row preparation and columns processing, the req.obj is passed as is, useful for syncing between pools
for the table to convert values returned from the db into the the format defined by the column</li>
<li>cached - if true perform cache invalidation for the operations that resulted in modification of the table record(s)</li>
<li>total - if true then it is supposed to return only one record with property <code>count</code>, skip all post processing and convertion</li>
<li>info_obj - to return the record just processed in the info object as <code>obj</code> property, it will include all generated and updated columns</li>
<li>result_obj - to return the query record as result including all post processing and new generated columns, this is not what <code>returning</code> property does, it only
returns the query record with new columns from memory</li>
<li>keep_req - on return do not clear out the request object, by default all properties are deleted to free up memory</li>
<li>keep_obj - only preserve op, obj, table properties in the reqest after return</li>
</ul>
</li>
<li><p>callback(err, rows, info) where</p>
<ul>
<li>info is an object with information about the last query: inserted_oid,affected_rows,next_token,consumed_capacity</li>
<li>rows is always returned as a list, even in case of error it is an empty list</li>
</ul>
</li>
</ul>
<p>  Example with SQL driver</p>
<pre><code>      db.query({ text: &quot;SELECT a.id,c.type FROM bk_user a,bk_icon c WHERE a.id=c.id and a.id=?&quot;, values: [&#39;123&#39;] }, { pool: &#39;pg&#39; }, function(err, rows, info) {
      });
</code></pre>

<ul>
<li><p><code>db.queryProcessSync(pool, req, row)</code></p>
<p> Post process hook to be used for replicating records to another pool, this is supposed to be used as this:</p>
<pre><code>  db.setProcessRow(&quot;post&quot;, &quot;*&quot;, (req, row) =&gt; { db.queryProcessSync(&quot;elasticsearch&quot;, req, row) });
</code></pre>
</li>
</ul>
<p> The conditions when to use it is up to the application logic.</p>
<p> It does not deal with the destination pool to be overloaded, all errors will be ignored, this is for simple and light load only</p>
<p> The destination poll must have tables to be synced configured:</p>
<pre><code>  db-elasticsearch-pool-tables=table1,table2
</code></pre>

<ul>
<li><p><code>db.get(table, query, options, callback)</code></p>
<p> Retrieve one record from the database by primary key, returns found record or null if not found
Options can use the following special properties:</p>
<ul>
<li>select - a list of columns or expressions to return, default is to return all columns</li>
<li>ops - operators to use for comparison for properties, see <code>db.select</code></li>
<li>cached - if specified it runs getCached version</li>
<li>nocache - disable caching even if configured for the table</li>
</ul>
</li>
</ul>
<p> NOTE: On return the <code>info.cached</code> will be set to 1 if the record was retrieved from cache or was put in the cache.</p>
<p> Example</p>
<pre><code>      db.get(&quot;bk_user&quot;, { login: &#39;12345&#39; }, function(err, row) {
         if (row) console.log(row.name);
      });
</code></pre>

<ul>
<li><p><code>db.select(table, query, options, callback)</code></p>
<p> Select objects from the database that match supplied conditions.</p>
</li>
</ul>
<ul>
<li>query - can be an object with properties for the condition, all matching records will be returned,
also can be a list where each item is an object with primary key condition. Only records specified in the list must be returned.</li>
<li>options can use the following special properties:<ul>
<li>ops - operators to use for comparison for properties, an object with column name and operator. The following operators are available:
 <code>&gt;, gt, &lt;, lt, =, !=, &lt;&gt;, &gt;=, ge, &lt;=, le, in, all_in, between, regexp, iregexp, begins_with, not_begins_with, like%, ilike%, contains, not_contains</code></li>
<li>opsMap - operator mapping between supplied operators and actual operators supported by the db</li>
<li>typesMap - type mapping between supplied and actual column types, an object</li>
<li>select - a list of columns or expressions to return or all columns if not specified, only existing columns will be returned</li>
<li>select_all - a list of columns or expressions to return, passed as is to the underlying driver</li>
<li>start - start records with this primary key, this is the next_token passed by the previous query</li>
<li>count - how many records to return</li>
<li>first - a convenient option to return the first record from the result or null (similar to <code>db.get</code> method)</li>
<li>join - how to join condition expressions, default is AND</li>
<li>joinOps - operators to use to combine several expressions in case when an array of values is given, supports `and|or|AND|OR``</li>
<li>sort - sort by this column. if null then no sorting must be done at all, records will be returned in the order they are kept in the DB.
 <em>NOTE: For DynamoDB this may affect the results if columns requsted are not projected in the index, with sort
  <code>select</code> property might be used to get all required properties. For Elasticsearch if sort is null then scrolling scan will be used,
  if no <code>timeout</code> or <code>scroll</code> are given the default is 1m.</em></li>
<li>sort_timeout - for pagination how long to keep internal state in millisecons, depends on the DB, for example for Elasticsearch it corresponds
 to the scroll param and defaults to 60000 (1m)</li>
<li>desc - if sorting, do in descending order</li>
<li>page - starting page number for pagination, uses count to find actual record to start, for SQL databases mostly</li>
<li>unique - specified the column name to be used in determining unique records, if for some reasons there are multiple records in the location
 table for the same id only one instance will be returned</li>
<li>cacheKey - exlicit key for caching, return from the cache or from the DB and then cache it with this key, works the same as <code>get</code></li>
<li>cacheKeyName - a name of one of the cache keys to use, it must be defined by a <code>db-cache-keys-table-name</code> parameter</li>
<li>nocache - do not use cache even if cache key is given</li>
<li>aliases - an object with mapping between artificial name to real column name, useful in $or/$and conditions with same column but different values</li>
<li>custom_columns - an array of pairs to define global or artificial columns, the format is: [ RegExp, type, ...], useful with aliases</li>
</ul>
</li>
</ul>
<p> On return, the callback can check third argument which is an object with some predefined properties along with driver specific state returned by the query:</p>
<ul>
<li>affected_rows - how many records this operation affected, for add/put/update</li>
<li>inserted_oid - last created auto generated id</li>
<li>next_token - next primary key or offset for pagination by passing it as .start property in the options, if null it means there are no more pages availabe for this query</li>
</ul>
<p> Example: get by primary key, refer above for default table definitions</p>
<pre><code>    db.select(&quot;bk_message&quot;, { id: &#39;123&#39; }, { count: 2 }, function(err, rows) {

    });
</code></pre>
<p> Example: get all icons with type greater or equal to 2</p>
<pre><code>    db.select(&quot;bk_icon&quot;, { id: &#39;123&#39;, type: &#39;2&#39; }, { select: &#39;id,type&#39;, ops: { type: &#39;ge&#39; } }, function(err, rows) {

    });
</code></pre>
<p> Example: get unread msgs sorted by time, recent first</p>
<pre><code>    db.select(&quot;bk_message&quot;, { id: &#39;123&#39;, status: &#39;N:&#39; }, { sort: &quot;status&quot;, desc: 1, ops: { status: &quot;begins_with&quot; } }, function(err, rows) {

    });
</code></pre>
<p> Example: allow all accounts icons to be visible</p>
<pre><code>    db.select(&quot;bk_user&quot;, {}, function(err, rows) {
        rows.forEach(function(row) {
            row.acl_allow = &#39;auth&#39;;
            db.update(&quot;bk_icon&quot;, row);
        });
    });
</code></pre>
<p> Example: scan accounts with custom filter, not by primary key: by exact zipcode</p>
<pre><code>    db.select(&quot;bk_user&quot;, { zipcode: &#39;20000&#39; }, function(err, rows) {

    });
</code></pre>
<p> Example: select accounts by type for the last day</p>
<pre><code>    db.select(&quot;bk_user&quot;, { type: &#39;admin&#39;, mtime: Date.now()-86400000 }, { ops: { type: &quot;contains&quot;, mtime: &quot;gt&quot; } }, function(err, rows) {

    });
</code></pre>

<ul>
<li><p><code>db.search(table, query, options, callback)</code></p>
<p> Perform full text search on the given table, the database implementation may ignore table name completely
in case of global text index.</p>
</li>
</ul>
<p> Query in general is a text string with the format that is supported by the underlying driver,
 the db module <em>DOES NOT PARSE</em> the query at all if the driver supports full text search, otherwise it behaves like <code>select</code>.</p>
<p> Options make take the same properties as in the <code>select</code> method.</p>
<p> A special query property <code>q</code> may be used for generic search in all fields.</p>
<p> Without full text search support in the driver this may return nothing or an error.</p>
<p>  Example
            db.search(&quot;bk_user&quot;, { type: &quot;admin&quot;, q: &quot;john*&quot; }, { pool: &quot;elasticsearch&quot; }, lib.log);
            db.search(&quot;bk_user&quot;, &quot;john*&quot;, { pool: &quot;elasticsearch&quot; }, lib.log);</p>

<ul>
<li><p><code>db.add(table, obj, options, callback)</code></p>
<p> Insert new object into the database</p>
</li>
</ul>
<ul>
<li>obj - an JavaScript object with properties for the record, primary key properties must be supplied</li>
<li>options may contain the following properties:<ul>
<li>no_columns - do not check for actual columns defined in the pool tables and add all properties from the obj, only will work for NoSQL dbs,
by default all properties in the obj not described in the table definition for the given table will be ignored.</li>
<li>skip_columns - ignore properties by name listed in the this array</li>
<li>mtime - if set, mtime column will be added automatically with the current timestamp, if mtime is a
string then it is used as a name of the column instead of default mtime name</li>
<li>skip_null - if set, all null values will be skipped, otherwise will be written into the DB as NULLs</li>
</ul>
</li>
</ul>
<p> On return the <code>obj</code> will contain all new columns generated before adding the record</p>
<p> Note: SQL, DynamoDB, MongoDB, Redis drivers are fully atomic but other drivers may be subject to race conditions</p>
<p> Example</p>
<pre><code>   db.add(&quot;bk_user&quot;, { id: &#39;123&#39;, login: &#39;admin&#39;, name: &#39;test&#39; }, function(err, rows, info) {
   });
</code></pre>

<ul>
<li><p><code>db.incr(table, obj, options, callback)</code></p>
<p> Counter operation, increase or decrease column values, similar to update but all specified columns except primary
key will be incremented, use negative value to decrease the value.</p>
</li>
</ul>
<p> If no <code>options.updateOps</code> object specified or no &#39;incr&#39; operations are provided then
 all columns with type &#39;counter&#39; will be used for the action <code>incr</code></p>
<p> <em>Note: The record must exist already for SQL databases, for DynamoDB and Cassandra a new record will be created
 if does not exist yet.</em> To disable upsert pass <code>noupsert</code> in the options.</p>
<p> Example</p>
<pre><code>   db.incr(&quot;bk_counter&quot;, { id: &#39;123&#39;, like0: 1, invite0: 1 }, function(err, rows, info) {
   });
</code></pre>

<ul>
<li><p><code>db.put(table, obj, options, callback)</code></p>
<p> Add/update an object in the database, if object already exists it will be replaced with all new properties from the obj</p>
</li>
</ul>
<ul>
<li>obj - an object with record properties, primary key properties must be specified</li>
<li>options - same properties as for <code>db.add</code> method</li>
</ul>
<p> Example</p>
<pre><code>   db.put(&quot;bk_user&quot;, { id: &#39;123&#39;, login: &#39;test&#39;, name: &#39;test&#39; }, function(err, rows, info) {
   });
</code></pre>

<ul>
<li><p><code>db.update(table, obj, options, callback)</code></p>
<p> Update existing object in the database.</p>
</li>
</ul>
<ul>
<li>obj - is an actual record to be updated, primary key properties must be specified</li>
<li>options - same properties as for <code>db.add</code> method with the following additional properties:<ul>
<li>ops - object for comparison operators for primary key, default is equal operator</li>
<li>opsMap - operator mapping into supported by the database</li>
<li>typesMap - type mapping for properties to be used in the condition</li>
<li>aliases - an object to map column aliases in the query in case the same column is used ultiple times</li>
<li>expected - an object with the condition for the update, it is used in addition to the primary keys condition from the <code>obj</code>,
 a property named $or or $and will be treated as a sub-expression if it is an object.</li>
<li>expectedJoin - how to join expected expressions: OR, AND, default is AND</li>
<li>upsert - create a new record if it does not exist</li>
<li>syncMode - skip columns preprocessing and dynamic values for pool sync and backup restore</li>
<li>updateOps - an object with column names and operations to be performed on the named column<ul>
<li>incr - increment by given value</li>
<li>add - add an item to the list</li>
<li>del - remove an item from the list</li>
<li>set - to update as it is, for reseting counters forexample</li>
<li>concat - concatenate given value, for strings if the database supports it</li>
<li>append - append to the list of values, only for lists if the database supports it</li>
<li>prepend - insert at the beginning of the list, depends on the database</li>
<li>not_exists - only update if not exists or null</li>
</ul>
</li>
<li>typesOps - an object that defines updateOps operation by column type, for example <code>typesOps: { list: &quot;add&quot; }</code> will
 make sure all lists will have updateOps set as add if not specified explicitly</li>
</ul>
</li>
</ul>
<p> Note: not all database drivers support atomic update with conditions, all drivers for SQL, DynamoDB, MongoDB, Redis fully atomic, but other drivers
 perform get before put and so subject to race conditions</p>
<p> Example</p>
<pre><code>      db.update(&quot;bk_user&quot;, { login: &#39;test&#39;, id: &#39;123&#39; }, (err, rows, info) =&gt; {
          console.log(&#39;updated:&#39;, info.affected_rows);
      });

      db.update(&quot;bk_user&quot;, { login: &#39;test&#39;, id: &#39;123&#39;, first_name: &#39;Mr&#39; }, { pool: pg&#39; }, (err, rows, info) =&gt; {
          console.log(&#39;updated:&#39;, info.affected_rows);
      });

      db.update(&quot;bk_user&quot;, { login: &#39;test&#39;, first_name: &#39;John&#39; }, { expected: { first_name: &quot;Carl&quot; } }, (err, rows, info) =&gt; {
          console.log(&#39;updated:&#39;, info.affected_rows);
      });

      db.update(&quot;bk_user&quot;, { login: &#39;test&#39;, first_name: &#39;John&#39; }, { expected: { &quot;$or&quot;: { first_name: &quot;Carl&quot;, g1: null }, aliases: { g1: &quot;first_name&quot; } }, (err, rows, info) =&gt; {
          console.log(&#39;updated:&#39;, info.affected_rows);
      });
</code></pre>

<ul>
<li><p><code>db.updateAll(table, query, obj, options, callback)</code></p>
<p> Update all records that match given condition in the <code>query</code>, one by one, the input is the same as for <code>db.select</code> and every record
returned will be updated using <code>db.update</code> call by the primary key, so make sure options.select include the primary key for every row found by the select.</p>
</li>
</ul>
<p> All properties from the <code>obj</code> will be set in every matched record.</p>
<p> The callback will receive on completion the err and all rows found and updated. This is mostly for non-SQL databases and for very large range it may take a long time
 to finish due to sequential update every record one by one.
 Special properties that can be in the options for this call:</p>
<ul>
<li>updateOptions - options to be passed to the db.update if needed, this is useful so select and update options will not be mixed up</li>
<li>updateCollect - if true return all updated rows in the callback otherwise just the number of updated rows</li>
<li>factorCapacity - write capacity factor for update operations, default is 0.25</li>
<li>op - by default it uses db.update but the <code>op</code> can be set to <code>put</code> or <code>add</code></li>
<li>updateProcess - a function callback that will be called for each row before updating it, this is for some transformations of the record properties
 in case of complex columns that may contain concatenated values as in the case of using DynamoDB. The callback will be called
 as <code>options.updateProcess(row, options)</code>. If it returns non-empty value the update will stop and return it as the error.</li>
<li>updateFilter - a function that must return something to the callback in order to skip the current record. <code>options.updateFilter(row, options, (skip) =&gt; {})</code></li>
</ul>
<p>  If no <code>options.select</code> is specified only the primary keys will be returned or collected</p>
<p> Example, update birthday format if not null</p>
<pre><code>      db.updateAll(&quot;bk_user&quot;,
                  { birthday: 1 },
                  { mtime: Date.now() },
                  { ops: { birthday: &quot;not null&quot; },
                    updateProcess: function(r, o) {
                       r.birthday = lib.strftime(new Date(r.birthday, &quot;%Y-%m-D&quot;));
                    },
                    updateFilter: function(r, o, cb) {
                       cb(r.status == &#39;ok&#39;);
                    } },
      function(err, count) {
         console.log(count, &quot;rows updated&quot;);
      });
</code></pre>

<ul>
<li><p><code>db.del(table, obj, options, callback)</code></p>
<p> Delete an object in the database, no error if the object does not exist</p>
</li>
</ul>
<ul>
<li>obj - an object with primary key properties only, other properties will be ignored</li>
<li>options - same properties as for <code>db.update</code> method</li>
</ul>
<p> Example</p>
<pre><code>   db.del(&quot;bk_user&quot;, { login: &#39;123&#39; }, function(err, rows, info) {
       console.log(&#39;updated:&#39;, info.affected_rows);
   });
</code></pre>

<ul>
<li><p><code>db.delAll(table, query, options, callback)</code></p>
<p> Delete all records that match given condition, one by one, the input is the same as for <code>db.select</code> and every record
returned will be deleted using <code>db.del</code> call. The callback will receive on completion the err and all rows found and deleted.
Special properties that can be in the options for this call:</p>
<ul>
<li>ops - query operations to retrieve records to be deleted</li>
<li>count - how many matching records to delete</li>
<li>delScan - if true force to use db.scan instead of native <code>delAll</code> for the given pool</li>
<li>delOptions - options to be passed to the db.del if needed, this is useful so select and del options will not be mixed up</li>
<li>delCollect - if true return all deleted rows in the callback, oherwise just the number of rows deleted</li>
<li>factorCapacity - write capqcity factor for delete operations, default is 0.35</li>
<li>concurrency - how many delete requests to execute at the same time by using lib.forEachLimit.</li>
<li>ignore_error - continue deleting records even after an error</li>
<li>delProcess - a function callback that will be called for each row before deleting it, this is for some transformations of the record properties
in case of complex columns that may contain concatenated values as in the case of using DynamoDB. The callback will be called
as <code>options.delProcess(row, options, info)</code>. If it returns non-empty value the scan will stop and return it as the error.</li>
<li>delFilter - a function that must return something to the callback in order to skip the current record. <code>options.delFilter(row, options, (skip) =&gt; {})</code></li>
<li>batch - delete using bulk operations, all functions must accept an array of rows instead</li>
</ul>
<p>If no <code>options.select</code> is specified only the primary keys will be returned or collected</p>
</li>
</ul>
<p> If <code>db-skip-drop</code> matches the table name and there is no query provided it will exit with error</p>

<ul>
<li><p><code>db.list(table, query, options, callback)</code></p>
<p> Convenient helper to retrieve all records by primary key, the obj must be a list with key property or a string with list of primary key column
Example</p>
<pre><code>db.list(&quot;bk_user&quot;, [&quot;id1&quot;, &quot;id2&quot;], function(err, rows) { console.log(err, rows) });
db.list(&quot;bk_user&quot;, &quot;id1,id2&quot;, function(err, rows) { console.log(err, rows) });
</code></pre>
</li>
</ul>

<ul>
<li><p><code>db.batch(list, options, callback)</code></p>
<p> Perform a batch of operations at the same time, all operations for the same table will be run
together one by one but different tables will be updated in parallel.</p>
</li>
</ul>
<ul>
<li><code>list</code> an array of objects to put/delete from the database in the format:<ul>
<li>op - is one of add, incr, put, update, del</li>
<li>table - which table to use</li>
<li>obj - an object with data</li>
<li>options - params for the operation, optional</li>
</ul>
</li>
<li>options can have the follwoing:<ul>
<li>concurrency - number of how many operations to run at the same time, 1 means sequential</li>
<li>no_errors - will stop on first error, because operations will be run in parallel some operations still may be performed</li>
<li>factorCapacity - a capacity factor to apply to the write capacity if present, by default it is used write capacity at 100%</li>
</ul>
</li>
</ul>
<p> On return the second arg to the callback is a list of records with errors, same input record with added property <code>errstatus</code> and <code>errmsg</code></p>
<p>  Example:</p>
<pre><code>      var ops = [ { op: &quot;add&quot;, table: &quot;bk_counter&quot;, obj: { id:1, like:1 } },
                  { op: &quot;add&quot;, table: &quot;bk_user&quot;, obj: { login: &quot;test&quot;, id:1, name:&quot;test&quot; }]
      db.batch(ops, { factorCapacity: 0.5 }, lib.log);
</code></pre>

<ul>
<li><p><code>db.bulk(list, options, callback)</code></p>
<p> Bulk operations, it will be noop if the driver does not support it.
The input format is the same as for the <code>db.batch</code> method.</p>
</li>
</ul>
<p> On return the second arg to the callback is a list of records with errors, same input record with added property <code>errstatus</code> and <code>errmsg</code></p>
<p> NOTE: DynamoDB only supports add/put/del only and 25 at a time, if more specified it will send multiple batches</p>
<p> Example</p>
<pre><code>      var ops = [ { op: &quot;add&quot;, table: &quot;bk_counter&quot;, obj: { id:1, like:1 } },
                  { op: &quot;del&quot;, table: &quot;bk_user&quot;, obj: { login: &quot;test1&quot; } },
                  { op: &quot;incr&quot;, table: &quot;bk_counter&quot;, obj: { id:2, like:1 } },
                  { op: &quot;add&quot;, table: &quot;bk_user&quot;, obj: { login: &quot;test2&quot;, id:2, name:&quot;test2&quot; } }]
      db.bulk(ops, { pool: &quot;elasticsearch&quot; }, lib.log);
</code></pre>

<ul>
<li><p><code>db.transaction(list, options, callback)</code></p>
<p> Same as the <code>db.bulk</code> but in transaction mode, all operations must succeed or fail. Not every driver can support it,
in DynamoDB case only 10 operations can be done at the same time, if the list is larger then it will be sequentially run with batches of 25 records.</p>
</li>
</ul>
<p> In case of error the second arg will contain the records of the failed batch</p>

<ul>
<li><p><code>db.scan(table, query, options, rowCallback, endCallback)</code></p>
<p> Convenient helper for scanning a table for some processing, rows are retrieved in batches and passed to the callback until there are no more
records matching given criteria. The obj is the same as passed to the <code>db.select</code> method which defined a condition which records to get.
The rowCallback must be present and is called for every row or batch retrieved and second parameter which is the function to be called
once the processing is complete. At the end, the callback will be called just with 1 argument, err, this indicates end of scan operation.
Basically, db.scan is the same as db.select but can be used to retrieve large number of records in batches and allows async processing of such records.
To hint a driver that scanning is in progress the <code>options.scanning</code> will be set to true.</p>
</li>
</ul>
<p> Parameters:</p>
<ul>
<li>table - table to scan</li>
<li>query - an object with query conditions, same as in <code>db.select</code></li>
<li>options - same as in <code>db.select</code>, with the following additions:<ul>
<li>count - size of every batch, default is 100</li>
<li>limit - total number of records to scan</li>
<li>start - the primary key to start the scan from</li>
<li>search - use search instead of select, for ElasticSearch,...</li>
<li>batch - if true rowCallback will be called with all rows from the batch, not every row individually, batch size is defined by the count property</li>
<li>concurrency - how many rows to process at the same time, if not given process sequentially</li>
<li>noscan - if 1 no scan will be performed if no primary keys are specified</li>
<li>emptyscan - if 0 no empty scan will be performed when no table columns in the query to be used as a filter</li>
<li>fullscan - if 1 force to scan full table without using any primary key conditons, use all query properties for all records (DynamoDB)</li>
<li>useCapacity - triggers to use specific capacity, default is <code>read</code></li>
<li>factorCapacity - a factor to apply for the read capacity limit and triggers the capacity check usage, default is <code>0.9</code></li>
<li>tableCapacity - use a different table for capacity throttling instead of the <code>table</code>, useful for cases when the row callback performs
 writes into that other table and capacity is different</li>
<li>capacity - a full capacity object to pass to select calls</li>
</ul>
</li>
<li>rowCallback - process records when called like this `callback(rows, next, info)</li>
<li>endCallback - end of scan when called like this: `callback(err)</li>
</ul>
<p>  Example:</p>
<pre><code>      db.scan(&quot;bk_user&quot;, {}, { count: 10, pool: &quot;dynamodb&quot; }, function(row, next) {
          // Copy all accounts from one db into another
          db.add(&quot;bk_user&quot;, row, { pool: &quot;pg&quot; }, next);
      }, function(err) { });
</code></pre>

<ul>
<li><p><code>db.copy(table, query, options, callback)</code></p>
<p> Copy records from one table to another between different DB pools or regions</p>
</li>
</ul>
<p> Parameters:</p>
<ul>
<li>table - name of the table to copy</li>
<li>query - a query condition for the table</li>
<li>options properties<ul>
<li>sort - index to use for query</li>
<li>minCapacity - capacity minimum for read/writes, it will override actual DB capacity</li>
<li>factorCapacity - factor the actual capacity for reads/writes</li>
<li>stopOnError - stop the copy on first DB error, otherwise ignore errors</li>
<li>region - other region where to copy</li>
<li>pool - other DB pool</li>
<li>file - dump the data into a file as JSON</li>
<li>preprocess - a function(table, row, options) to be called before the update, if it returns true the record will be skipped</li>
<li>posprocess - a function(table, row, options, next) to be called after the record is copied, for recursive or joined cases</li>
<li>reget - if set the actual record will read using db.get, for cases when db.scan returns only partial record as in DynamoDB cases with indexes</li>
<li>incremental - if set, try to read the latest record in the other table and continue from there, uses <code>sort</code> index in desc order</li>
<li>batch - a number of records to copy at once using the bulk operation</li>
<li>syncMode - if set enabled the update mode in which all values are preserved and not pre-processed, default is 1</li>
<li>updateOptions - pass options to update/bulk operations</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>db.join(table, rows, options, callback)</code></p>
<p> Join the given list of records with the records from other table by primary key.
The properties from the joined table will be merged with the original rows preserving the existing properties</p>
</li>
</ul>
<ul>
<li>options.keys defines custom primary key to use instead of table&#39;s primary key</li>
<li>options.keysMap - an object that defines which property should be used for a key in the given rows, this is
for cases when actual primary keys in the table are different from the rows properties.</li>
<li>options.columnsMap - save properties with a different name using this mapping object</li>
<li>options.existing is 1 then return only joined records.</li>
<li>options.override - joined table properties will replace the original table existing properties</li>
<li>options.attach - specifies a property name which will be used to attach joined record to the original record, no merging will occur, for
 non-existing records an empty object will be attached</li>
<li>options.incr can be a list of property names that need to be summed up with each other, not overriden</li>
<li>options.nomerge - do not merge lists, just return new rows as is</li>
</ul>
<p> A special case when table is empty <code>db.join</code> just returns same rows to the callback, this is
 for convenience of doing joins on some conditions and trigger it by setting the table name or skip the join completely.</p>
<p> Example:</p>
<pre><code>      db.join(&quot;bk_user&quot;, [{id:&quot;123&quot;,key1:1},{id:&quot;234&quot;,key1:2}], lib.log)
      db.join(&quot;bk_user&quot;, [{aid:&quot;123&quot;,key1:1},{aid:&quot;234&quot;,key1:2}], { keysMap: { id: &quot;aid&quot; }}, lib.log)
      db.join(&quot;bk_user&quot;, [{id:&quot;123&quot;,state:&quot;NY&quot;},{id:&quot;234&quot;,state:&quot;VA&quot;}], { columnsMap: { state: &quot;astate&quot; }}, lib.log)
</code></pre>

<ul>
<li><p><code>db.create(table, columns, options, callback)</code></p>
<p> Create a table using column definitions represented as a list of objects. Each column definition may
contain the following properties:</p>
</li>
</ul>
<ul>
<li><code>name</code> - column name</li>
<li><code>type</code> - column type: int, bigint, real, string, now, counter or other supported type</li>
<li><code>primary</code> - column is part of the primary key</li>
<li><code>unique</code> - column is part of an unique key</li>
<li><code>index</code> - column is part of an index, the value is a number for the column position in the index</li>
<li><code>indexN</code> - additonal inxdexes where N is 1..5</li>
<li><code>value</code> - default value for the column</li>
<li><code>len</code> - column length</li>
<li><code>maxlength</code> - limit the max column value, the value will be truncated before saving into the DB</li>
<li><code>pub</code> - columns is public, <em>this is very important property because it allows anybody to see it when used in the default API functions, i.e. anybody with valid
 credentials can retrieve all public columns from all other tables, and if one of the other tables is account table this may expose some personal information,
 so by default only a few columns are marked as public in the <code>bk_user</code> table</em></li>
<li><code>pub_admin</code> - a generic read permission requires <code>options.isAdmin</code> when used with <code>api.cleanResult</code></li>
<li><code>pub_staff</code> - a generic read permission requires <code>options.isStaff</code> when used with <code>api.cleanResult</code></li>
<li><code>pub_types</code> - a role or a list of roles which further restrict access to a public column to only users with specified roles</li>
<li><code>priv_types</code> - a role or a list of roles which excplicitely deny access to a column for users with specified roles</li>
<li><code>priv</code> - an opposite for the pub property, if defined this property should never be returned to the client by the API handlers</li>
<li><code>auth</code> - this property will be set in <code>req.options.account</code> for access permissions checks when only options are available</li>
<li><code>internal</code> - if set then this property can only be updated by admin/root or with <code>isInternal`` property, implemented by the </code>auth` module only</li>
<li><code>hidden</code> - completely ignored by all update operations but could be used by the public columns cleaning procedure, if it is computed and not stored in the db
 it can contain pub property to be returned to the client</li>
<li><code>readonly</code> - only add/put operations will use the value, incr/update will not affect the value</li>
<li><code>writeonly</code> - only incr/update can change this value, add/put will ignore it</li>
<li><code>noresult</code> - delete this property from the result, mostly for joined artificial columns which used for indexes only</li>
<li><code>random</code> - add a random number between 0 and this value, useful with type: &quot;now&quot;</li>
<li><code>lower</code> - make string value lowercase</li>
<li><code>upper</code> - make string value uppercase</li>
<li><code>strip</code> - if a regexp perform replace on the column value before saving</li>
<li><code>trim</code> - strim string value of whitespace</li>
<li><code>cap</code> - capitalize into a title with lib.toTitle</li>
<li><code>word</code> - if a number only save nth word from the value, split by <code>separator</code></li>
<li><code>clock</code> - for <code>now</code> type use high resolution clock in nanoseconds</li>
<li><code>epoch</code> - for <code>now</code> type save as seconds since the Epoch, not milliseconds</li>
<li><code>multiplier</code> - for numeric columns apply this multipliers before saving</li>
<li><code>incrememnt</code> - for numeric columns add this value before saving</li>
<li><code>decimal</code> - for numeric columns convert into fixed number using this number of decimals</li>
<li><code>prefix</code> - prefix to be prepended for autogenerated columns: <code>uuid</code>, <code>suud</code>, <code>tuud</code></li>
<li><code>separator</code> - to be used as a separator in join or split depending on the column properties</li>
<li><code>list</code> - splits the column value into an array, optional <code>separator</code> property can be used, default separator is <code>,|</code></li>
<li><code>autoincr</code> - for counter tables, mark the column to be auto-incremented by the connection API if the connection type has the same name as the column name</li>
<li><code>join</code> - a list with property names that must be joined together before performing a db operation, it will use the given record to produce new property,
  this will work both ways, to the db and when reading a record from the db it will split joined property and assign individual
  properties the value from the joined value. See <code>db.joinColumns</code> for more options.</li>
<li><code>unjoin</code> - split the join column into respective columns on retrieval</li>
<li><code>keepjoined</code> - keep the joined column value, if not specified the joined column is deleted after unjoined</li>
<li><code>notempty</code> - do not allow empty columns, if not provided it is filled with the default value</li>
<li><code>skip_empty</code> - ignore the column if the value is empty, i.e. null or empty string</li>
<li><code>fail_ifempty</code> - returtn an error if there is no  value for the column, this is checked during record preprocessing</li>
<li><code>values</code> - an array with allowed values, ignore the column if not present</li>
<li><code>values_map</code> - an array of pairs to be checked for exact match and be replaced with the next item, [&quot;&quot;, null, &quot;&quot;, undefined, &quot;null&quot;, &quot;&quot;]</li>
</ul>
<p> <em>Some properties may be defined multiple times with number suffixes like: <code>unique1, unique2, index1, index2</code> to create more than one index for the table, same
 properties define a composite key in the order of definition or sorted by the property value, for example: <code>{ a: { index:2 }, b: { index:1 } }</code> will create index (b,a)
 because of the <code>index:</code> property value being not the same. If all index properties are set to 1 then a composite index will use the order of the properties.</em></p>
<p> <em>Special column types</em>:</p>
<ul>
<li><code>uuid</code> - autogenerate the column value with UUID, optional <code>prefix</code> property will be prepended, <code>{ type: &quot;uuid&quot;, prefix: &quot;u_&quot; }</code></li>
<li><code>now</code> - defines a column to be automatically filled with the current timestamp, <code>{ type: &quot;now&quot; }</code></li>
<li><code>counter</code> - defines a columns that will be automatically incremented by the <code>db.incr</code> command, on creation it is set with 0</li>
<li><code>uid</code> - defines a columns to be automatically filled with the current user id, this assumes that account object is passed in the options from the API level</li>
<li><code>uname</code> - defines a columns to be automatically filled with the current user name, this assumes that account object is passed in the options from the API level</li>
<li><code>ttl</code> - mark the column to be auto expired, can be set directly to time in the future or use one of: <code>days</code>, <code>hours</code>, <code>minutes</code> as a interval in the future</li>
</ul>
<p> NOTE: Index creation is not required and all index properties can be omitted, it can be done more effectively using native tools for any specific database,
 this format is for simple and common use cases without using any other tools but it does not cover all possible variations for every database. But all indexes and
 primary keys created outside of the backend application will be detected properly by <code>db.cacheColumns</code> and by each pool <code>cacheIndexes</code> methods.</p>
<p> Each database pool also can support native options that are passed directly to the driver in the options, these properties are
 defined in the object with the same name as the db driver, all properties are combined, for example to define provisioned throughput for the DynamoDB index:</p>
<pre><code>      db.create(&quot;test_table&quot;, { id: { primary: 1, type: &quot;int&quot;, index: 1, dynamodb: { readCapacity: 50, writeCapacity: 50 } },
                                type: { primary: 1, pub: 1, projections: 1 },
                                name: { index: 1, pub: 1 } }
                              });
</code></pre>
<p> Create DynamoDB table with global secondary index, the first index property if not the same as primary key hash defines global index, if it is the same then local,
 or if the second key column contains <code>global</code> property then it is a global index as well, below we create global secondary index on property &#39;name&#39; only,
 in the example above it was local secondary index for id and name. Also a local secondary index is created on <code>id,title</code>.</p>
<p> DynamoDB projection is defined by a <code>projections</code> property, can be a number/boolean or an array with index numbers:</p>
<pre><code>      db.create(&quot;test_table&quot;, { id: { primary: 1, type: &quot;int&quot;, index1: 1 },
                                type: { primary: 1, projections: [0] },
                                name: { index: 1, projections: 1 },
                                title: { index1: 1, projections: [1] } },
                                descr: { index: 1, projections: [0, 1] },
                              });
</code></pre>
<p>  When using real DynamoDB creating a table may take some time, for such cases if <code>options.waitTimeout</code> is not specified it defaults to 1min,
  so the callback is called as soon as the table is active or after the timeout whichever comes first.</p>
<p> Pass MongoDB options directly:
        db.create(&quot;test_table&quot;, { id: { primary: 1, type: &quot;int&quot;, mongodb: { w: 1, capped: true, max: 100, size: 100 } },
                                  type: { primary: 1, pub: 1 },
                                  name: { index: 1, pub: 1, mongodb: { sparse: true, min: 2, max: 5 } }
                                });</p>

<ul>
<li><p><code>db.upgrade(table, columns, options, callback)</code></p>
<p> Upgrade a table with missing columns from the definition list, if after the upgrade new columns must be re-read from the database
then <code>info.affected_rows</code> must be non zero.</p>
</li>
</ul>

<ul>
<li><p><code>db.drop(table, options, callback)</code></p>
<p> Drop a table</p>
</li>
</ul>

<ul>
<li><p><code>db.sql(text, values, options, callback)</code></p>
<p> Execute arbitrary SQL-like statement if the pool supports it, values must be an Array with query parameters or can be omitted.</p>
</li>
</ul>
<p> Example:</p>
<pre><code>   db.sql(&quot;SELECT * FROM bk_property WHERE value=? LIMIT 1&quot;, [1], { pool: &quot;sqlite&quot;, count: 10 }, lib.log)
   db.sql(&quot;SELECT * FROM bk_property&quot;, { pool: &quot;dynamodb&quot; }, lib.log)
   db.sql(&quot;SELECT * FROM bk_property&quot;, { pool: &quot;dynamodb&quot;, count: 10 }, lib.log)
</code></pre>

<h2 id="module-events">Module: EVENTS</h2>
<ul>
<li><p><code>mod.shutdownWorker(options, callback)</code></p>
<p> Perform graceful worker shutdown, to be used for workers restart</p>
</li>
</ul>

<ul>
<li><p><code>mod.checkTimes()</code></p>
<p> Check how long we run a job and force kill if exceeded, check if total life time is exceeded.</p>
</li>
</ul>
<p> If exit is required the <code>shundownWorker</code> methods will receive options with <code>shutdownReason</code> property
 set and the name-sake property will contained the value exceeded.</p>

<h2 id="module-http_get">Module: HTTP_GET</h2>
<h2 id="module-index">Module: INDEX</h2>
<h2 id="module-ipc">Module: IPC</h2>
<ul>
<li><p><code>Ipc.prototype.handleServerMessages(worker, msg)</code></p>
<p> To be used in messages processing that came from the clients or other way</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.sendReplPort(role, worker)</code></p>
<p> Send REPL port to a worker if needed</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.newMsg(op, msg, options)</code></p>
<p> Returns an IPC message object, <code>msg</code> must be an object if given.</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.emitMsg(op, msg, options)</code></p>
<p> Wrapper around EventEmitter <code>emit</code> call to send unified IPC messages in the same format</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.sendMsg(op, msg, options, callback)</code></p>
<p> Send a message to the master process via IPC messages, callback is used for commands that return value back</p>
</li>
</ul>
<ul>
<li>the <code>timeout</code> property can be used to specify a timeout for how long to wait the reply, if not given the default is used</li>
<li>the rest of the properties are optional and depend on the operation.</li>
</ul>
<p> If called inside the server, it process the message directly, reply is passed in the callback if given.</p>
<p> Examples:</p>
<pre><code>    ipc.sendMsg(&quot;op1&quot;, { data: &quot;data&quot; }, { timeout: 100 })
    ipc.sendMsg(&quot;op1&quot;, { name: &quot;name&quot;, value: &quot;data&quot; }, function(data) { console.log(data); })
    ipc.sendMsg(&quot;op1&quot;, { 1: 1, 2: 2 }, { timeout: 100 })
    ipc.sendMsg(&quot;op1&quot;, { 1: 1, 2: 2 }, function(data) { console.log(data); })
    ipc.newMsg({ __op: &quot;op1&quot;, name: &quot;test&quot; })
</code></pre>

<ul>
<li><p><code>Ipc.prototype.initServer()</code></p>
<p> This function is called by a master server process to setup IPC channels and support for cache and messaging</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.initWorker()</code></p>
<p> This function is called by a worker process to setup IPC channels and support for cache and messaging</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.createClient(url, options)</code></p>
<p> Return a new client for the given host or null if not supported</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.getQueue(options)</code></p>
<p> Return a cache or queue client by name if specified in the options or use default client which always exists,
use <code>queueName</code> to specify a specific queue. If it is an array it will rotate items sequentially.</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.initClients()</code></p>
<p> Initialize a client for cache or queue purposes, previous client will be closed.</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.checkClients(prefix)</code></p>
<p> Initialize missing or new clients, existing clients stay the same</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.stats(options, callback)</code></p>
<p> Returns the cache statistics, the format depends on the cache type used, for queues it returns a property &#39;queueCount&#39; with currently
visible messages in the queue, &#39;queueRunning&#39; with currently in-flight messages</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.clear(pattern, options, callback)</code></p>
<p> Clear all or only items that match the given pattern</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.get(key, options, callback)</code></p>
<p> Retrieve an item from the cache by key.</p>
</li>
</ul>
<ul>
<li><code>options.set</code> is given and no value exists in the cache it will be set as the initial value, still
nothing will be returned to signify that a new value assigned.</li>
<li><code>options.mapName</code> defines a map from which the key will be retrieved if the cache supports maps, to get the whole map
the key must be set to *</li>
<li><code>options.listName</code> defines a map from which to get items, if a key is given it will return 1 if it belongs to the list,
if no key is provided it will return an array with 2 elements:  [a random key, the length of the list], to get the whole list specify * as the key. Specifying
<code>del</code> in the options will delete returned items from the list.</li>
<li><code>options.ttl</code> can be used with lists with <code>del</code> and empty key, in such case all popped up keys will be saved in
the cache with specified time to live, when being popped up every key is checked if it has been served already, i.e.
it exists in the cache and not expired yet, such keys are ignored and only never seen keys are returned</li>
<li><code>options.datatype</code> specifies that the returned value must be converted into the specified type using <code>lib.toValue</code></li>
</ul>
<p> If the <code>key</code> is an array then it returns an array with values for each key, for non existent keys an empty
 string will be returned. For maps only if the <code>key</code> is * it will return the whole object, otherwise only value(s)
 are returned.</p>
<p> Example</p>
<pre><code>ipc.get([&quot;my:key1&quot;, &quot;my:key2&quot;], function(err, data) { console.log(data) });
ipc.get(&quot;my:key&quot;, function(err, data) { console.log(data) });
ipc.get(&quot;my:counter&quot;, { set: 10 }, function(err, data) { console.log(data) });
ipc.get(&quot;*&quot;, { mapName: &quot;my:map&quot; }, function(err, data) { console.log(data) });
ipc.get(&quot;key1&quot;, { mapName: &quot;my:map&quot; }, function(err, data) { console.log(data) });
ipc.get([&quot;key1&quot;, &quot;key2&quot;], { mapName: &quot;my:map&quot; }, function(err, data) { console.log(data) });
ipc.get([&quot;key1&quot;, &quot;key2&quot;], { listName: &quot;my:list&quot; }, function(err, data) { console.log(data) });
ipc.get(&quot;&quot;, { listName: &quot;my:list&quot;, del: 1 }, function(err, data) { console.log(data) });
ipc.get(&quot;&quot;, { listName: &quot;my:list&quot;, del: 1, ttl: 30000 }, function(err, data) { console.log(data) });
</code></pre>

<ul>
<li><p><code>Ipc.prototype.del(key, options, callback)</code></p>
<p> Delete an item by key(s),  if <code>key</code> is an array all keys will be deleted at once atomically if supported</p>
</li>
</ul>
<ul>
<li><code>options.mapName</code> defines a map from which the counter will be deleted if the cache supports maps, to delete the whole map
the key must be set to *</li>
<li><code>options.listName</code> defines a list from which an item should be removed</li>
</ul>
<p> Example:</p>
<pre><code>    ipc.del(&quot;my:key&quot;)
    ipc.del(&quot;key1&quot;, { mapName: &quot;my:map&quot; })
    ipc.del(&quot;*&quot;, { mapName: &quot;my:map&quot; })
    ipc.del(&quot;1&quot;, { listName: &quot;my:list&quot; })
</code></pre>

<ul>
<li><p><code>Ipc.prototype.put(key, val, options, callback)</code></p>
<p> Replace or put a new item in the cache.</p>
</li>
</ul>
<ul>
<li><code>options.ttl</code> can be passed in milliseconds if the driver supports it</li>
<li><code>options.mapName</code> defines a map where the counter will be stored if the cache supports maps, to store the whole map in one
operation the <code>key</code> must be set to * and the <code>val</code> must be an object</li>
<li><code>options.setmax</code> if not empty tell the driver to set this new number only if there is no existing
value or it is less that the new number, only works for numeric values</li>
<li><code>options.listName</code> defines a list where to add items, <code>val</code> can be a value or an array of values, <code>key</code> is ignored in this case</li>
</ul>
<p> Example:</p>
<pre><code>   ipc.put(&quot;my:key&quot;, 2)
   ipc.put(&quot;my:key&quot;, 1, { setmax: 1 })
   ipc.put(&quot;key1&quot;, 1, { mapName: &quot;my:map&quot; })
   ipc.put(&quot;*&quot;, { key1: 1, key2: 2 }, { mapName: &quot;my:map&quot; })
   ipc.put(&quot;&quot;, [1,2,3], { listName: &quot;my:list&quot; })
</code></pre>

<ul>
<li><p><code>Ipc.prototype.incr(key, val, options, callback)</code></p>
<p> Increase/decrease a counter in the cache by <code>val</code>, non existent items are treated as 0, if a callback is given an
error and the new value will be returned.</p>
</li>
</ul>
<ul>
<li><code>options.ttl</code> in milliseconds can be used if the driver supports it</li>
<li><code>options.mapName</code> defines a map where the counter will be stored if the cache supports maps</li>
</ul>
<p> Example:</p>
<pre><code>    ipc.incr(&quot;my:key&quot;, 1)
    ipc.incr(&quot;key1&quot;, 1, { mapName: &quot;my:map&quot; })
</code></pre>

<ul>
<li><p><code>Ipc.prototype.subscribe(channel, options, callback)</code></p>
<p> Subscribe to receive messages from the given channel, the callback will be called only on new message received.</p>
<ul>
<li><code>options.queueName</code> defines the queue, if not specified then it is sent to the default queue</li>
</ul>
<p>Example:</p>
<pre><code>    ipc.subscribe(&quot;alerts&quot;, function(msg) {
        req.res.json(data);
    }, req);
</code></pre>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.unsubscribe(channel, options, callback)</code></p>
<p> Close a subscription for the given channel, no more messages will be delivered.</p>
<ul>
<li><code>options.queueName</code> defines the queue, if not specified then it is sent to the default queue</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.publish(channel, msg, options, callback)</code></p>
<p> Publish an event to the channel to be delivered to all subscribers. If the <code>msg</code> is not a string it will be stringified.</p>
<ul>
<li><code>options.queueName</code> defines the queue, if not specified then it is sent to the default queue</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.broadcast(channel, msg, options, callback)</code></p>
<p> Send a message to a channel, this is high level routine that uses the corresponding queue, it uses eventually ipc.publish.
If no client or queue is provided in the options it uses default <code>systemQueue</code>.</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.subscribeQueue(options, callback)</code></p>
<p> Listen for messages from the given queue, the callback will be called only on new message received.</p>
<ul>
<li><code>options.queueName</code> defines the queue, if not specified then it is sent to the default queue</li>
</ul>
</li>
</ul>
<p> The callback accepts 2 arguments, a message and optional next callback, if it is provided it must be called at the end to confirm or reject the message processing.
 Only errors with code&gt;=500 will result in rejection, not all drivers support the next callback if the underlying queue does not support message acknowledgement.</p>
<p> Depending on the implementation, this can work as fan-out, delivering messages to all subscribed to the same channel or
 can implement job queue model where only one subscriber receives a message.
 For some cases like Redis this is the same as <code>subscribe</code>.</p>
<p> For cases when the <code>next</code> callback is provided this means the queue implementation requires an acknowledgement of successful processing,
 returning an error with <code>err.status &gt;= 500</code> will keep the message in the queue to be processed later. Special code <code>600</code> means to keep the job
 in the queue and report as warning in the log.</p>
<p>  Example:</p>
<pre><code>      ipc.listen({ queueName: &quot;jobs&quot; }, function(msg, next) {
          req.res.json(data);
          if (next) next();
      }, req);
</code></pre>

<ul>
<li><p><code>Ipc.prototype.unsubscribeQueue(options, callback)</code></p>
<p> Stop listening for message, if no callback is provided all listeners for the key will be unsubscribed, otherwise only the specified listener.</p>
<ul>
<li><code>options.queueName</code> defines the queue, if not specified then it is sent to the default queue</li>
</ul>
</li>
</ul>
<p> The callback will not be called.</p>
<p> It keeps a count how many subscribe/unsubscribe calls been made and stops any internal listeners once nobody is
 subscribed. This is specific to a queue which relies on polling.</p>

<ul>
<li><p><code>Ipc.prototype.publishQueue(msg, options, callback)</code></p>
<p> Submit a message to the queue, if the <code>msg</code> is not a string it will be stringified.</p>
<ul>
<li><code>options.queueName</code> defines the queue, if not specified then it is sent to the default queue</li>
<li><code>options.stime</code> defines when the message should be processed, it will be held in the queue until the time comes</li>
<li><code>options.etime</code> defines when the message expires, i.e. will be dropped if not executed before this time.</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.monitorQueue(options)</code></p>
<p> Queue specific monitor services that must be run in the master process, this is intended to perform
queue cleanup or dealing with stuck messages (Redis)</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.unpublishQueue(msg, options, callback)</code></p>
<p> Queue specific message deletion from the queue in case of abnormal shutdown or job running too long in order not to re-run it after the restart, this
is for queues which require manual message deletion ofter execution(SQS). Each queue client must maintain the mapping or other means to identify messages,
the options is the message passed to the listener</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.limiter(options, callback)</code></p>
<p> Check for rate limit using the default or specific queue, by default TokenBucket using local LRU cache is
used unless a queue client provides its own implementation.</p>
</li>
</ul>
<p> The options must have the following properties:</p>
<ul>
<li>name - unique id, can be IP address, account id, etc...</li>
<li>max - the maximum burst capacity</li>
<li>rate - the rate to refill tokens</li>
<li>interval - interval for the bucket refills, default 1000 ms</li>
<li>ttl - auto expire after specified ms since last use</li>
<li>reset - if true reset the token bucket if not consumed or the total reached this value if it is a number greater than 1</li>
</ul>
<p> The callback takes 2 arguments:</p>
<ul>
<li><code>delay</code> is a number of milliseconds till the bucket can be used again if not consumed, i.e. 0 means consumed.</li>
<li><code>info</code> is an object with info about the state of the token bucket after the operation with properties: delay, count, total, elapsed</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.checkLimiter(options, callback)</code></p>
<p> Keep checking the limiter until it is clear to proceed with the operation, if there is no available tokens in the bucket
it will wait and try again until the bucket is filled.</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.localLimiter(msg)</code></p>
<p> Uses msg.name as a key returns the same message with consumed set to 1 or 0</p>
</li>
</ul>

<ul>
<li><p><code>Ipc.prototype.lock(name, options, callback)</code></p>
<p> Implementation of a lock with optional ttl, only one instance can lock it, can be for some period of time and will expire after timeout.
A lock must be uniquely named and the ttl period is specified by <code>options.ttl</code> in milliseconds.</p>
</li>
</ul>
<p> This is intended to be used for background job processing or something similar when
 only one instance is needed to run. At the end of the processing <code>ipc.unlock</code> must be called to enable another instance immediately,
 otherwise it will be available after the ttl only.</p>
<p> if <code>options.timeout</code> is given the function will keep trying to lock for the <code>timeout</code> milliseconds.</p>
<p> if <code>options.set</code> is given it will unconditionally set the lock for the specified ttl, this is for cases when
 the lock must be active for longer because of the long running task</p>
<p> The callback must be passed which will take an error and a boolean value, if true is returned it means the timer has been locked by the caller,
 otherwise it is already locked by other instance. In case of an error the lock is not supposed to be locked by the caller.</p>
<p> Example:</p>
<pre><code>      ipc.lock(&quot;my-lock&quot;, { ttl: 60000, timeout: 30000 }, function(err, locked) {
           if (locked) {
               ...
               ipc.unlock(&quot;my-lock&quot;);
           }
      });
</code></pre>

<ul>
<li><p><code>Ipc.prototype.unlock(name, options, callback)</code></p>
<p> Unconditionally unlock the lock, any client can unlock any lock.</p>
</li>
</ul>

<h2 id="module-jobs">Module: JOBS</h2>
<ul>
<li><p><code>mod.configureMaster(options, callback)</code></p>
<p> Initialize jobs processing in the master process</p>
</li>
</ul>

<ul>
<li><p><code>mod.configureWorker(options, callback)</code></p>
<p> Initialize a worker to be ready for jobs to execute, in instance mode setup timers to exit on no activity.</p>
</li>
</ul>

<ul>
<li><p><code>mod.shutdownWorker(options, callback)</code></p>
<p> Perform graceful worker shutdown, to be used for workers restart</p>
</li>
</ul>

<ul>
<li><p><code>mod.exitWorker(options)</code></p>
<p> Perform graceful worker shutdown and then exit the process</p>
</li>
</ul>

<ul>
<li><p><code>mod.initServer(options, callback)</code></p>
<p> Initialize a master that will manage jobs workers</p>
</li>
</ul>

<ul>
<li><p><code>mod.initWorker(options, callback)</code></p>
<p> Initialize a worker for processing jobs</p>
</li>
</ul>

<ul>
<li><p><code>mod.isCancelled(name, tag)</code></p>
<p> Returns true if a task with given name must be cancelled, this flag is set from the jobs master and
stoppable tasks must check it from time to time to terminate gracefully</p>
</li>
</ul>

<ul>
<li><p><code>mod.cancelTask(name, options)</code></p>
<p> Send cancellation request to a worker or all workers, this has to be called from the jobs master.
<code>options.workers</code> can be a single worker id or a list of worker ids, if not given the request will be sent to all workers for the current process cluster.
<code>options.tag</code> is an opaque data that will be used to verifying which task should be cancelled, without it all tasks with given name will be cancelled.</p>
</li>
</ul>

<ul>
<li><p><code>mod.getMaxRuntime()</code></p>
<p> Find the max runtime allowed in seconds</p>
</li>
</ul>

<ul>
<li><p><code>mod.checkTimes()</code></p>
<p> Check how long we run a job and force kill if exceeded, check if total life time is exceeded.</p>
</li>
</ul>
<p> If exit is required the <code>shundownWorker</code> methods will receive options with <code>shutdownReason</code> property
 set and the name-sake property will contained the value exceeded.</p>

<ul>
<li><p><code>mod._badJob(jobspec)</code></p>
<p> Make sure the job is valid and has all required fields, returns a normalized job object or an error, the jobspec
must be in the following formats:</p>
<pre><code>  &quot;module.method&quot;
  { job: &quot;module.method&quot; }
  { job: { &quot;module.method&quot;: {}, .... } }
  { job: [ &quot;module.method&quot;, { &quot;module.method&quot;: {} ... } ...] }
</code></pre>
</li>
</ul>
<p> any task in string format &quot;module.method&quot; will be converted into { &quot;module.method: {} } automatically</p>

<ul>
<li><p><code>mod.checkOptions(jobspec, options)</code></p>
<p> Apply special job properties from the options</p>
</li>
</ul>

<ul>
<li><p><code>mod.submitJob(jobspec, options, callback)</code></p>
<p> Submit a job for execution, it will be saved in a queue and will be picked up later and executed.
The queue and the way how it will be executed depends on the configured queue. See <code>isJob</code> for
the format of the job objects.</p>
</li>
</ul>
<p> <code>jobspec.uniqueTtl</code> if greater than zero it defines number of milliseconds for this job to stay in the queue or run,
 it creates a global lock using the job object as the hash key, no other job can be run until the ttl expires or the job
 finished, non unique jobs will be kept in the queue and repeated later according to the <code>visibilityTimeout</code> setting.</p>
<p> <code>jobspec.uniqueKey</code> can define an alternative unique key for this job for cases when different jobs must be run sequentially</p>
<p> <code>jobspec.uniqueKeep</code> if true then keep the unique lock after the jobs finished, otherwise it is cleared</p>
<p> <code>jobspec.uniqueDrop</code> if true will make non-unique jobs to be silently dropped instead of keeping them in the queue</p>
<p> <code>jobspec.logger</code> defines the logger level which will be used to log when the job is finished, default is debug</p>
<p> <code>jobspec.maxRuntime</code> defines max number of seconds this job can run, if not specified then the queue default is used</p>
<p> <code>jobspec.uniqueTag</code> defines additional tag to be used for job cancelling, for cases when multiple jobs are running with the same method</p>
<p> <code>jobspec.uniqueOnce</code> if true than the visibility timeout is not kept alive while the job is running</p>
<p> <code>jobspec.noWait</code> will run the job and delete it from the queue immediately, not at the end, for one-off jobs</p>
<p> <code>jobspec.noWaitTimeout</code> number of seconds before deleting the job for one-off jobs but taking into account the uniqueKey and visibility timeout giving time
  to check for uniqueness and exit, can be used regardless of the noWait flag</p>
<p> <code>jobspec.noVisibility</code> will always delete messages after processing, ignore 600 errors as well</p>
<p> <code>jobspec.visibilityTimeout</code> custom timeout for how long to keep this job invisible, overrides the default timeout</p>
<p> <code>jobspec.retryVisibilityTimeout</code> custom timeout for how long to keep this job invisible in case of errors &gt;= 500 which results in keeping tasks in the queue for retry</p>
<p> <code>jobspec.retryVisibilityStatus</code> apply retryVisibilityTimeout only if this code is specified, this can be a list with multiple codes</p>
<p> <code>jobspec.stopOnError</code> will stop tasks processing on first error, otherwise all errors will be just logged. Errors with status &gt;= 600 will
  stop the job regardless of this flag</p>
<p> <code>jobspec.startTime</code> and/or <code>jobspec.endTime</code> will define the time period during whihc this job is allowed to run, if
  outside the period it will be dropped</p>
<p> <code>options.delay</code> is only supported by SQS currently, it delays the job execution for the specified amount of ms</p>
<p> <code>options.dedup_ttl</code> - if set it defines number of ms to keep track of duplicate messages, it tries to preserver only-once behaviour. To make
  some queue to automatically use dedup mode it can be set in the queue options: <code>-ipc-queue[-NAME]-options-dedup_ttl 86400000</code>.
  Note: <code>uniqueTtl</code> settings take precedence and if present dedup is ignored.</p>
<p> Special queue name: <code>jobs.selfQueue</code> is reserved to run the job immediately inside the current process,
 it will call the <code>runJob</code> directly, this is useful in cases when already inside a worker and instead of submitting a new job
 just run it directly. Any queue can be configured to run in <code>selfQueue</code> by setting <code>-ipc-queue[-NAME]-options-self-queue 1</code>.</p>

<ul>
<li><p><code>mod.runJob(jobspec, options, callback)</code></p>
<p> Run all tasks in the job object</p>
</li>
</ul>

<ul>
<li><p><code>mod._runJob(jobspec, options, callback)</code></p>
<p> Sequentially execute all tasks in the list, run all subtasks in parallel</p>
</li>
</ul>

<ul>
<li><p><code>mod.runTask(name, jobspec, options, callback)</code></p>
<p> Execute a task by name, the <code>options</code> will be passed to the function as the first argument, calls the callback on finish or error</p>
</li>
</ul>

<ul>
<li><p><code>mod._finishTask(err, name, jobspec, options, callback)</code></p>
<p> Complete task execution, cleanup and update the status</p>
</li>
</ul>

<ul>
<li><p><code>mod.scheduleCronjob(jobspec)</code></p>
<p> Create a new cron job, for remote jobs additional property args can be used in the object to define
arguments for the instance backend process, properties must start with -</p>
</li>
</ul>
<p> Example:</p>
<pre><code>      { &quot;cron&quot;: &quot;0 */10 * * * *&quot;, &quot;job&quot;: &quot;server.processQueue&quot; },
      { &quot;cron&quot;: &quot;0 */30 * * * *&quot;, &quot;job&quot;: { &quot;server.processQueue&quot;: { name: &quot;queue1&quot; } } },
      { &quot;cron&quot;: &quot;0 5 * * * *&quot;, &quot;job&quot;: [ { &quot;scraper.run&quot;: { &quot;url&quot;: &quot;host1&quot; } }, { &quot;scraper.run&quot;: { &quot;url&quot;: &quot;host2&quot; } } ] }
</code></pre>

<ul>
<li><p><code>mod.scheduleCronjobs(type, list)</code></p>
<p> Schedule a list of cron jobs, types is used to cleanup previous jobs for the same type for cases when
a new list needs to replace the existing jobs. Empty list does nothing, to reset the jobs for the particular type and
empty invalid jobs must be passed, like: <code>[ {} ]</code></p>
</li>
</ul>
<p> Returns number of cron jobs actually scheduled.</p>

<ul>
<li><p><code>mod.loadCronjobs()</code></p>
<p> Load crontab from JSON file as list of job specs:</p>
</li>
</ul>
<ul>
<li>cron - cron time interval spec: &#39;second&#39; &#39;minute&#39; &#39;hour&#39; &#39;dayOfMonth&#39; &#39;month&#39; &#39;dayOfWeek&#39;</li>
<li>job - a string as obj.method or an object with job name as property name and the value is an object with
 additional jobspec for the job passed as first argument, a job callback always takes jobspec and callback as 2 arguments</li>
<li>disabled - disable the job but keep in the cron file, it will be ignored</li>
<li>queueName - name of the queue where to submit this job, if not given it uses cron-queue</li>
<li>uniqueTtl - defines that this job must be the only one in the queue for the number of milliseconds specified, after that
 time another job with the same arguments can be submitted.</li>
</ul>
<p> Example:</p>
<pre><code>      [ { cron: &quot;0 0 * * * *&quot;, job: &quot;scraper.run&quot; }, ..]
</code></pre>

<ul>
<li><p><code>mod.parseCronjobs(type, data)</code></p>
<p> Parse a JSON data with cron jobs and schedule for the given type, this can be used to handle configuration properties</p>
</li>
</ul>

<h2 id="module-lib">Module: LIB</h2>
<ul>
<li><p><code>lib.tryCall(callback)</code></p>
<p> Run a callback if a valid function, all arguments after the callback will be passed as is</p>
</li>
</ul>

<ul>
<li><p><code>lib.tryCatch(callback)</code></p>
<p> Run a callback inside try..catch block, all arguments after the callback will be passed as is, in case of error
all arguments will be printed in the log</p>
</li>
</ul>

<ul>
<li><p><code>lib.log()</code></p>
<p> Print all arguments into the console, for debugging purposes, if the first arg is an error only print the error</p>
</li>
</ul>

<ul>
<li><p><code>lib.__()</code></p>
<p> Simple i18n translation method compatible with other popular modules, supports the following usage:</p>
</li>
</ul>
<ul>
<li>__(name)</li>
<li>__(fmt, arg,...)</li>
<li>__({ phrase: &quot;&quot;, locale: &quot;&quot; }, arg...</li>
</ul>

<ul>
<li><p><code>lib.getArg(name, dflt)</code></p>
<p> Return commandline argument value by name</p>
</li>
</ul>

<ul>
<li><p><code>lib.getArgInt(name, dflt)</code></p>
<p> Return commandline argument value as a number</p>
</li>
</ul>

<ul>
<li><p><code>lib.isArg(name)</code></p>
<p> Returns true of given arg(s) are present in the command line, name can be a string or an array of strings.</p>
</li>
</ul>

<ul>
<li><p><code>lib.deferCallback(parent, msg, callback, timeout)</code></p>
<p> Register the callback to be run later for the given message, the message may have the <code>__id</code> property which will be used for keeping track of the responses or it will be generated.
The <code>parent</code> can be any object and is used to register the timer and keep reference to it.</p>
</li>
</ul>
<p> A timeout is created for this message, if <code>runCallback</code> for this message will not be called in time the timeout handler will call the callback
 anyway with the original message.</p>
<p> The callback passed will be called with only one argument which is the message, what is inside the message this function does not care. If
 any errors must be passed, use the message object for it, no other arguments are expected.</p>

<ul>
<li><p><code>lib.onDeferCallback(msg)</code></p>
<p> To be called on timeout or when explicitely called by the <code>runCallback</code>, it is called in the context of the message.</p>
</li>
</ul>

<ul>
<li><p><code>lib.runCallback(parent, msg)</code></p>
<p> Run delayed callback for the message previously registered with the <code>deferCallback</code> method.
The message must have <code>id</code> property which is used to find the corresponding callback, if the msg is a JSON string it will be converted into the object.</p>
</li>
</ul>
<p> Same parent object must be used for <code>deferCallback</code> and this method.</p>

<ul>
<li><p><code>lib.deferInterval(parent, interval, name, callback)</code></p>
<p> Assign or clear an interval timer, keep the reference in the given parent object</p>
</li>
</ul>

<ul>
<li><p><code>lib.sortByVersion(list, name)</code></p>
<p> Sort a list be version in descending order, an item can be a string or an object with
a property to sort by, in such case <code>name</code> must be specified which property to use for sorting.
The name format is assumed to be: <code>XXXXX-N.N.N</code></p>
</li>
</ul>

<ul>
<li><p><code>lib.dropPrivileges(uid, gid)</code></p>
<p> Drop root privileges and switch to a regular user</p>
</li>
</ul>

<ul>
<li><p><code>lib.ip2int(ip)</code></p>
<p> Convert an IP address into integer</p>
</li>
</ul>

<ul>
<li><p><code>lib.int2ip(int)</code></p>
<p> Convert an integer into IP address</p>
</li>
</ul>

<ul>
<li><p><code>lib.inCidr(ip, cidr)</code></p>
<p> Return true if the given IP address is within the given CIDR block</p>
</li>
</ul>

<ul>
<li><p><code>lib.cidrRange(cidr)</code></p>
<p> Return first and last IP addresses for the CIDR block</p>
</li>
</ul>

<ul>
<li><p><code>lib.shuffle(list)</code></p>
<p> Randomize the list items in place</p>
</li>
</ul>

<ul>
<li><p><code>lib.domainName(host)</code></p>
<p> Extract domain from the host name, takes all host parts except the first one</p>
</li>
</ul>

<ul>
<li><p><code>lib.newError(msg, status, code)</code></p>
<p> Return a new Error object, msg can be a string or an object with message, code, status properties.
The default error status is 400 if not specified.</p>
</li>
</ul>

<ul>
<li><p><code>lib.traceError(err)</code></p>
<p> Returns the error stack or the error itself, to be used in error messages</p>
</li>
</ul>

<ul>
<li><p><code>lib.loadLocale(file, callback)</code></p>
<p> Load a file with locale translations into memory</p>
</li>
</ul>

<ul>
<li><p><code>lib.typeName(v)</code></p>
<p> Return object type, try to detect any distinguished type</p>
</li>
</ul>

<ul>
<li><p><code>lib.isObject(v)</code></p>
<p> Returns true of the argument is a generic object, not a null, Buffer, Date, RegExp or Array</p>
</li>
</ul>

<ul>
<li><p><code>lib.isNumber(val)</code></p>
<p> Return true if the value is a number</p>
</li>
</ul>

<ul>
<li><p><code>lib.isPrefix(val, prefix)</code></p>
<p> Return true if the value is prefixed</p>
</li>
</ul>

<ul>
<li><p><code>lib.isUuid(val, prefix)</code></p>
<p> Returns true if the value represents an UUID</p>
</li>
</ul>

<ul>
<li><p><code>lib.isTuuid(str)</code></p>
<p> Returns true if the value represent tuuid</p>
</li>
</ul>

<ul>
<li><p><code>lib.isUnicode(str)</code></p>
<p> Returns true of a string contains Unicode characters</p>
</li>
</ul>

<ul>
<li><p><code>lib.isPositive(val)</code></p>
<p> Returns true if a number is positive, i.e. greater than zero</p>
</li>
</ul>

<ul>
<li><p><code>lib.isArray(val, dflt)</code></p>
<p> Returns the array if the value is non empty array or dflt value if given or undefined</p>
</li>
</ul>

<ul>
<li><p><code>lib.isEmpty(val)</code></p>
<p> Return true of the given value considered empty</p>
</li>
</ul>

<ul>
<li><p><code>lib.isNumeric(val)</code></p>
<p> Returns true if the value is a number or string representing a number</p>
</li>
</ul>

<ul>
<li><p><code>lib.isNumericType(type)</code></p>
<p> Returns true if the given type belongs to the numeric family of data types</p>
</li>
</ul>

<ul>
<li><p><code>lib.isDate(d)</code></p>
<p> Returns true if the given date is valid</p>
</li>
</ul>

<ul>
<li><p><code>lib.isFlag(list, name)</code></p>
<p> Returns true if <code>name</code> exists in the array <code>list</code>, search is case sensitive. if <code>name</code> is an array it will return true if
any element in the array exists in the <code>list</code>.</p>
</li>
</ul>

<ul>
<li><p><code>lib.validNum(...args)</code></p>
<p> Returns first valid number from the list of arguments or 0</p>
</li>
</ul>

<ul>
<li><p><code>lib.validPositive(...args)</code></p>
<p> Returns first valid positive number from the list of arguments or 0</p>
</li>
</ul>

<ul>
<li><p><code>lib.validBool(...args)</code></p>
<p> Returns first valid boolean from the list of arguments or false</p>
</li>
</ul>

<h2 id="module-logger">Module: LOGGER</h2>
<ul>
<li><p><code>logger</code></p>
<p> Simple logger utility for debugging</p>
</li>
</ul>

<ul>
<li><p><code>logger.registerLevel(level, callback, options)</code></p>
<p> Register a custom level handler, must be invoked via <code>logger.logger</code> only, if no handler registered for given level
the whole message will be logger as an error. The custom hadnler is called in the context of the module which means
the options are available inside the handler.</p>
</li>
</ul>
<p> The following properties are supported automatically:</p>
<ul>
<li>format - if 1 then all arguments will be formatted into one line as for the regular levels and passed
 the handler as one argument, this is to support different transport and preserve the same standard logging format</li>
</ul>

<ul>
<li><p><code>logger.setSyslog(facility, tag)</code></p>
<p> Set or close syslog mode</p>
</li>
</ul>

<ul>
<li><p><code>logger.setFile(file, options)</code></p>
<p> Redirect logging into file</p>
</li>
</ul>

<ul>
<li><p><code>logger.setLevel(level)</code></p>
<p> Set the output level, it can be a number or one of the supported level names</p>
</li>
</ul>

<ul>
<li><p><code>logger.setDebugFilter(str)</code></p>
<p> Enable debugging level for this label, if used with the same debugging level it will be printed regardless of the global level,
a label is first argument to the <code>logger.debug</code> methods, it is used as is, usually the fist argument is
the current function name with comma, like <code>logger.debug(&quot;select:&quot;, name, args)</code></p>
</li>
</ul>

<ul>
<li><p><code>logger.errorWithOptions(err, options)</code></p>
<p> Prints the given error and the rest of the arguments, the logger level to be used is determined for the given error by code,
uses <code>options</code> or <code>options.logger_error</code> as the level if a string,</p>
</li>
</ul>
<ul>
<li>if <code>options.logger_error</code> is an object, extract the level by <code>err.code</code> or use <code>*</code> as the default level for not matched codes,
the default is to use the <code>error</code> level.</li>
<li>In case the level is notice or info the error will only show status/code/message properties in order not to print stack trace</li>
<li>Merge <code>options.logger_inspect</code> if present with the current inspect options to log the rest of arguments.</li>
</ul>

<ul>
<li><p><code>logger.setInspectOptions(options)</code></p>
<p> Merge with existing inspect options temporarily, calling without options will reset to previous values</p>
</li>
</ul>

<ul>
<li><p><code>logger.trace()</code></p>
<p> Print stack backtrace as error</p>
</li>
</ul>

<ul>
<li><p><code>logger.logger(level, ...args)</code></p>
<p> A generic logger method, safe, first arg is supposed to be a logging level, if not valid the error level is used</p>
</li>
</ul>

<ul>
<li><p><code>logger.write(str)</code></p>
<p> Stream emulation</p>
</li>
</ul>

<h2 id="module-metrics">Module: METRICS</h2>
<ul>
<li><p><code>TokenBucket.prototype.configure(rate, max, interval, total)</code></p>
<p> Initialize existing token with numbers for rate calculations</p>
</li>
</ul>

<ul>
<li><p><code>TokenBucket.prototype.toJSON()</code></p>
<p> Return a JSON object to be serialized/saved</p>
</li>
</ul>

<ul>
<li><p><code>TokenBucket.prototype.toString()</code></p>
<p> Return a string to be serialized/saved</p>
</li>
</ul>

<ul>
<li><p><code>TokenBucket.prototype.toArray()</code></p>
<p> Return an array object to be serialized/saved</p>
</li>
</ul>

<ul>
<li><p><code>TokenBucket.prototype.equal(rate, max, interval)</code></p>
<p> Return true if this bucket uses the same rates in arguments</p>
</li>
</ul>

<ul>
<li><p><code>TokenBucket.prototype.consume(tokens)</code></p>
<p> Consume N tokens from the bucket, if no capacity, the tokens are not pulled from the bucket.</p>
</li>
</ul>
<p> Refill the bucket by tracking elapsed time from the last time we touched it.</p>
<pre><code>  min(totalTokens, current + (fillRate * elapsedTime))
</code></pre>

<ul>
<li><p><code>TokenBucket.prototype.delay(tokens)</code></p>
<p> Returns number of milliseconds to wait till number of tokens can be available again</p>
</li>
</ul>

<h2 id="module-msg">Module: MSG</h2>
<ul>
<li><p><code>Msg.prototype.init(options, callback)</code></p>
<p> Initialize supported notification services, it supports jobs arguments convention so can be used in the jobs that
need to send push notifications in the worker process.</p>
</li>
</ul>

<ul>
<li><p><code>Msg.prototype.shutdown(options, callback)</code></p>
<p> Shutdown notification services, wait till all pending messages are sent before calling the callback</p>
</li>
</ul>

<ul>
<li><p><code>Msg.prototype.send(device, options, callback)</code></p>
<p> Deliver a notification for the given device token(s).</p>
</li>
</ul>
<p> The <code>device</code> is where to send the message to, can be multiple ids separated by , or |.</p>
<p> Options with the following properties:</p>
<ul>
<li>service_id - list of services to use for delivery only: default, sns, apn, gcm</li>
<li>account_id - an account id associated with this token, for debugging and invalid token management</li>
<li>app_id - send to the devices for the given app only, if none matched send to the default device tokens only</li>
<li>msg - text message to send</li>
<li>badge - badge number to show if supported by the service</li>
<li>sound - set to 1 if a sound should be produced on message receive</li>
<li>type - set type of the message, service specific</li>
<li>category - action category for APN</li>
<li>id - send id with the notification, this is application specific data, sent as is</li>
<li>name - notification group name, can be used for grouping multiple messages under this name</li>
<li>url - a launch url for the app, it show associated screen on launch if supported</li>
</ul>

<ul>
<li><p><code>Msg.prototype.parseDevice(device)</code></p>
<p> Parse device URN and returns an object with all parts into separate properties. A device URN can be in the following format:
  [service://]device_token[@app]</p>
<ul>
<li>service is optional, supported types: <code>apn</code>, <code>gcm</code>, <code>sns</code>, the <code>default</code> service uses APN delivery</li>
<li>app is optional and can define an application id which is used by APN for routing to the devices with corresponding APS certificate.</li>
</ul>
</li>
</ul>

<ul>
<li><p><code>Msg.prototype.getClient(dev)</code></p>
<p> Return a client module that supports the given device</p>
</li>
</ul>

<ul>
<li><p><code>Msg.prototype.getConfig(name)</code></p>
<p> Return a list of all config cert/key parameters for the given name.
Each item in the list is an object with the following properties: key, secret, app</p>
</li>
</ul>

<ul>
<li><p><code>Msg.prototype.getAgent(mod, dev)</code></p>
<p> Return an agent for the given module for the given device</p>
</li>
</ul>

<ul>
<li><p><code>Msg.prototype.getTeam(app)</code></p>
<p> Return a team for the given app</p>
</li>
</ul>

<h2 id="module-pool">Module: POOL</h2>
<ul>
<li><p><code>Pool.prototype.init(options)</code></p>
<p> Initialize pool properties, this can be run anytime even on the active pool to override some properties</p>
</li>
</ul>

<ul>
<li><p><code>Pool.prototype.acquire(callback)</code></p>
<p> Return next available resource item, if not available immediately wait for defined amount of time before calling the
callback with an error. The callback second argument is active resource item.</p>
</li>
</ul>

<ul>
<li><p><code>Pool.prototype.destroy(item, callback)</code></p>
<p> Destroy the resource item calling the provided close callback</p>
</li>
</ul>

<ul>
<li><p><code>Pool.prototype.release(item)</code></p>
<p> Return the resource item back to the list of available resources.</p>
</li>
</ul>

<ul>
<li><p><code>Pool.prototype.destroyAll()</code></p>
<p> Close all active items</p>
</li>
</ul>

<ul>
<li><p><code>Pool.prototype.stats()</code></p>
<p> Return an object with stats</p>
</li>
</ul>

<ul>
<li><p><code>Pool.prototype.shutdown(callback, maxtime)</code></p>
<p> Close all connections and shutdown the pool, no more items will be open and the pool cannot be used without re-initialization,
if callback is provided then wait until all items are released and call it, optional maxtime can be used to retsrict how long to wait for
all items to be released, when expired the callback will be called</p>
</li>
</ul>

<ul>
<li><p><code>Pool.prototype._call(name, callback)</code></p>
<p> Call registered method and catch exceptions, pass it to the callback if given</p>
</li>
</ul>

<ul>
<li><p><code>Pool.prototype._timer()</code></p>
<p> Timer to ensure pool integrity</p>
</li>
</ul>

<h2 id="module-server">Module: SERVER</h2>
<ul>
<li><p><code>server.start()</code></p>
<p> Start the server process, call the callback to perform some initialization before launchng any server, just after core.init</p>
</li>
</ul>

<ul>
<li><p><code>server.startMonitor(options)</code></p>
<p> Start process monitor, running as root</p>
</li>
</ul>

<ul>
<li><p><code>server.startShell(options)</code></p>
<p> Start REPL shell or execute any subcommand if specified in the command line.
A subcommand may return special string to indicate how to treat the flow:</p>
</li>
</ul>
<ul>
<li>stop - stop processing commands and create REPL</li>
<li>continue - do not exit and continue processing other commands or end with REPL</li>
<li>all other values will result in returning from the run assuming the command will decide what to do, exit or continue running, no REPL is created</li>
</ul>

<ul>
<li><p><code>server.startMaster(options)</code></p>
<p> Setup worker environment</p>
</li>
</ul>

<ul>
<li><p><code>server.startWebServer(options)</code></p>
<p> Create Express server, setup worker environment, call supplied callback to set initial environment</p>
</li>
</ul>

<ul>
<li><p><code>server.startWebMaster()</code></p>
<p> Spawn web server from the master as a separate master with web workers, it is used when web and master processes are running on the same server</p>
</li>
</ul>

<ul>
<li><p><code>server.handleChildProcess(child, type, method)</code></p>
<p> Setup exit listener on the child process and restart it</p>
</li>
</ul>

<ul>
<li><p><code>server.startProcess()</code></p>
<p> Restart the main process with the same arguments and setup as a monitor for the spawn child</p>
</li>
</ul>

<ul>
<li><p><code>server.startWatcher()</code></p>
<p> Watch source files for modifications and restart</p>
</li>
</ul>

<ul>
<li><p><code>server.startDaemon()</code></p>
<p> Create daemon from the current process, restart node with -daemon removed in the background</p>
</li>
</ul>

<ul>
<li><p><code>server.onProcessExit()</code></p>
<p> Kill all child processes on exit</p>
</li>
</ul>

<ul>
<li><p><code>server.onProcessTerminate()</code></p>
<p> Terminates the server process, it is called on SIGTERM signal but can be called manually for graceful shitdown,
it runs <code>shutdown[Role]</code> methods before exiting</p>
</li>
</ul>

<ul>
<li><p><code>server.shutdown(options, callback)</code></p>
<p> Shutdown the system immediately, mostly to be used in the remote jobs as the last task</p>
</li>
</ul>

<ul>
<li><p><code>server.shutdownServer(options, callback)</code></p>
<p> Graceful shutdown if the api server needs restart</p>
</li>
</ul>

<ul>
<li><p><code>server.spawnProcess(args, skip, opts)</code></p>
<p> Start new process reusing global process arguments, args will be added and args in the skip list will be removed</p>
</li>
</ul>

<ul>
<li><p><code>server.writePidfile()</code></p>
<p> Create a pid file for the current process</p>
</li>
</ul>

<h2 id="module-bk_data">Module: BK_DATA</h2>
<ul>
<li><p><code>mod</code></p>
<p> Account management</p>
</li>
</ul>

<ul>
<li><p><code>mod.configureWeb(options, callback)</code></p>
<p> Create API endpoints and routes</p>
</li>
</ul>

<ul>
<li><p><code>mod.configureDataAPI()</code></p>
<p> API for full access to all tables</p>
</li>
</ul>

<h2 id="module-bk_system">Module: BK_SYSTEM</h2>
<ul>
<li><p><code>system</code></p>
<p> System management</p>
</li>
</ul>

<ul>
<li><p><code>system.init(options)</code></p>
<p> Initialize the module</p>
</li>
</ul>

<ul>
<li><p><code>system.configureWeb(options, callback)</code></p>
<p> Create API endpoints and routes</p>
</li>
</ul>

<ul>
<li><p><code>system.configureSystemAPI()</code></p>
<p> API for internal provisioning and configuration</p>
</li>
</ul>

<h2 id="module-bk_user">Module: BK_USER</h2>
<ul>
<li><p><code>mod.configureWeb(options, callback)</code></p>
<p> Create API endpoints and routes</p>
</li>
</ul>

<ul>
<li><p><code>mod.configureAccountsAPI()</code></p>
<p> Account management</p>
</li>
</ul>

<ul>
<li><p><code>mod.getAccount(req, options, callback)</code></p>
<p> Returns current account, used in /account/get API call, req.account will be filled with the properties from the db</p>
</li>
</ul>

<ul>
<li><p><code>mod.notifyAccount(options, callback)</code></p>
<p> Send Push notification to the account. The delivery is not guaranteed, if the message was queued for delivery, no errors will be returned.</p>
</li>
</ul>
<p> The options may contain the following:</p>
<ul>
<li>account - the whole account record where to send the notification</li>
<li>account_id - the id of the account where to send, the record will be retrieved</li>
<li>account_login - the login of the account where to send, the record will be retrieved</li>
<li>device_id - the device to send the message directly</li>
<li>msg - message text to send</li>
</ul>
<p> In addition the device_id can be saved in the format service://id where the service is one of the supported delivery services, this way the notification
 system will pick the right delivery service depending on the device id, the default service is apple.</p>
<p>  Example:</p>
<pre><code>   bk_user.notifyAccount({ account_id: &quot;123&quot;, msg: &quot;test&quot;, badge: 1, sound: 1 } })
</code></pre>

<ul>
<li><p><code>mod.addAccount(req, options, callback)</code></p>
<p> Register new account, may be used an API call, but the req does not have to be an Express request, it just
need to have query and options objects.</p>
</li>
</ul>

<ul>
<li><p><code>mod.updateAccount(req, options, callback)</code></p>
<p> Update existing account, used in /account/update API call</p>
</li>
</ul>

<ul>
<li><p><code>mod.deleteAccount(req, callback)</code></p>
<p> Delete account specified by the obj. Used in <code>/account/del</code> API call.
The options may contain <code>keep</code> array with tables to be kept, for example
delete an account but keep all messages and location: keep:[&quot;bk_user&quot;,&quot;bk_location&quot;]</p>
</li>
</ul>
<p> This methods is suitable for background jobs</p>

</div></body>
